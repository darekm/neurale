{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartek/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/XifengGuo/DCEC/blob/master/DCEC.py\n",
    "from time import time\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "from ConvAE import CAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    " \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCEC(object):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 filters=[32, 64, 128, 10],\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0):\n",
    "\n",
    "        super(DCEC, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "        self.pretrained = False\n",
    "        self.y_pred = []\n",
    "        self.delata_label = 0 \n",
    "        self.save_dir='temp';\n",
    "\n",
    "        self.cae = CAE(input_shape, filters)\n",
    "        hidden = self.cae.get_layer(name='embedding').output\n",
    "        self.encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "        # Define DCEC model\n",
    "        print('nn',self.n_clusters)\n",
    "        clustering_layer = ClusteringLayer(n_clusters=self.n_clusters, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=self.cae.input,\n",
    "                           outputs=[clustering_layer, self.cae.output])\n",
    "\n",
    "    def pretrain(self, x, batch_size=256, epochs=50, optimizer='adam'):\n",
    "        print('...Pretraining...')\n",
    "        self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "        from keras.callbacks import CSVLogger\n",
    "        csv_logger = CSVLogger(self.save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "        t0 = time()\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.cae.save(self.save_dir + '/pretrain_cae_model.h5')\n",
    "        print('Pretrained weights are saved to %s/pretrain_cae_model.h5' % self.save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "    \n",
    "    def printMetrics(self,aName,yy,_loss):\n",
    "        if y is not None:\n",
    "                    acc = np.round(metrics.acc(y, yy), 5)\n",
    "                    nmi = np.round(metrics.nmi(y, yy), 5)\n",
    "                    ari = np.round(metrics.ari(y, yy), 5)\n",
    "                    loss = np.round(_loss, 7)\n",
    "                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=_loss[0], Lc=_loss[1], Lr=_loss[2])\n",
    "                    #logwriter.writerow(logdict)\n",
    "                    # print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss,'  delta=',delta_label)\n",
    "\n",
    "                    print(aName,'acc = %.4f, nmi = %.4f, ari = %.4f' % (acc,nmi,ari),';  loss=',_loss)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, loss=['kld', 'mse'], loss_weights=[1, 1], optimizer='adam'):\n",
    "        self.model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "    def dopretrain(self,x, cae_weights=None,batch_size=256,epochs=50):\n",
    "        # Step 1: pretrain if necessary\n",
    "        if not self.pretrained and cae_weights is None:\n",
    "            #Nepoch=50\n",
    "            print('...pretraining CAE using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=',epochs)\n",
    "            self.pretrain(x, batch_size, epochs=epochs)\n",
    "            self.cae.save_weights( 'pretrain_cae.h5')\n",
    "      \n",
    "            self.pretrained = True\n",
    "        elif cae_weights is not None:\n",
    "            self.cae.load_weights('pretrain_cae.h5')\n",
    "            print('cae_weights is loaded successfully.')\n",
    "  \n",
    "    def fit(self, x, y=None, batch_size=256, maxiter=2e3, tol=1e-2,\n",
    "            update_interval=140, save_dir='temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = int(x.shape[0] / batch_size * 10)\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "   \n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        #t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "        loss = [0, 0, 0]\n",
    " \n",
    "        self.printMetrics(\"kMeans\",self.y_pred,loss)    \n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "        logfile = open(self.save_dir + '/dcec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                self.printMetrics('Iter '+ str(ite),self.y_pred,loss)\n",
    "             \n",
    "                # check stop criterion\n",
    "                #delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    #logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if (index + 1) * batch_size > x.shape[0]:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size::],\n",
    "                                                 y=[p[index * batch_size::], x[index * batch_size::]])\n",
    "                index = 0\n",
    "            else:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size:(index + 1) * batch_size],\n",
    "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    x[index * batch_size:(index + 1) * batch_size]])\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                # save DCEC model checkpoints\n",
    "                print('saving model to:', save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/dcec_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/dcec_model_final.h5')\n",
    "        #t3 = time()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "Ndataset='mnist-test'\n",
    "Nclusters=10\n",
    "Nsave_dir='temp'\n",
    "Ngamma=0.1\n",
    "Ntol=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusteringll = ClusteringLayer(10, name='clustering')(hidden)\n",
    "from keras.datasets import mnist\n",
    "(x_t, y_t), (x_s, y_s) = mnist.load_data()\n",
    "xtt = x_t.reshape(-1, 784).astype('int32')\n",
    "\n",
    "#np.savetxt(fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\", X=xtt)\n",
    "\n",
    "#x_t.savetxt('mnisttxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: (70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_mnist, load_usps\n",
    "if Ndataset == 'mnist':\n",
    "    x, y = load_mnist()\n",
    "elif Ndataset == 'usps':\n",
    "    x, y = load_usps('data/usps')\n",
    "elif Ndataset == 'mnist-test':\n",
    "    x, y = load_mnist()\n",
    "    x, y = x[60000:], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 10)                11530     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1152)              12672     \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)    (None, 7, 7, 64)          73792     \n",
      "_________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)    (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)    (None, 28, 28, 1)         801       \n",
      "=================================================================\n",
      "Total params: 275,979\n",
      "Trainable params: 275,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "nn 10\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1_input (InputLayer)        (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 14, 14, 32)   832         conv1_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 7, 7, 64)     51264       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 3, 3, 128)    73856       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 1152)         0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Dense)               (None, 10)           11530       flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1152)         12672       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 3, 3, 128)    0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)       (None, 7, 7, 64)     73792       reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)       (None, 14, 14, 32)   51232       deconv3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "clustering (ClusteringLayer)    (None, 10)           100         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)       (None, 28, 28, 1)    801         deconv2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 276,079\n",
      "Trainable params: 276,079\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prepare the DCEC model\n",
    "dcec = DCEC(input_shape=x.shape[1:], filters=[32, 64, 128, 10], n_clusters=Nclusters)\n",
    "plot_model(dcec.model, to_file=Nsave_dir + '/dcec_model.png', show_shapes=True)\n",
    "dcec.model.summary()\n",
    "dcec.save_dir=Nsave_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "Ntol=0.001\n",
    "Ngamma=0.2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cae_weights is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "dcec.compile(loss=['kld', 'mse'], loss_weights=[Ngamma, 1], optimizer=optimizer)\n",
    "t0 = time() \n",
    "#dcec.dopretrain(x,cae_weights=None,batch_size=256,epochs=30)\n",
    "dcec.dopretrain(x,cae_weights=1,batch_size=256,epochs=30)\n",
    "t1 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 390\n",
      "Initializing cluster centers with k-means.\n",
      "kMeans acc = 0.7122, nmi = 0.6546, ari = 0.5789 ;  loss= [0, 0, 0]\n",
      "Iter 0 acc = 0.7122, nmi = 0.6546, ari = 0.5789 ;  loss= [0, 0, 0]\n",
      "saving model to: temp/dcec_model_0.h5\n",
      "Iter 140 acc = 0.7276, nmi = 0.6820, ari = 0.6069 ;  loss= [0.023341028, 0.046185143, 0.014103999]\n",
      "Iter 280 acc = 0.7439, nmi = 0.7126, ari = 0.6377 ;  loss= [0.02920323, 0.067217834, 0.015759664]\n",
      "saving model to: temp/dcec_model_390.h5\n",
      "Iter 420 acc = 0.7560, nmi = 0.7347, ari = 0.6579 ;  loss= [0.047640286, 0.12704691, 0.022230903]\n",
      "Iter 560 acc = 0.7619, nmi = 0.7464, ari = 0.6691 ;  loss= [0.03747976, 0.093056336, 0.018868491]\n",
      "Iter 700 acc = 0.7674, nmi = 0.7556, ari = 0.6797 ;  loss= [0.048040316, 0.12480928, 0.023078458]\n",
      "saving model to: temp/dcec_model_780.h5\n",
      "Iter 840 acc = 0.7675, nmi = 0.7568, ari = 0.6797 ;  loss= [0.040314056, 0.083826326, 0.023548793]\n",
      "Iter 980 acc = 0.7719, nmi = 0.7647, ari = 0.6882 ;  loss= [0.0456482, 0.115723655, 0.022503467]\n",
      "Iter 1120 acc = 0.7733, nmi = 0.7664, ari = 0.6903 ;  loss= [0.0286436, 0.059928313, 0.01665794]\n",
      "saving model to: temp/dcec_model_1170.h5\n",
      "Iter 1260 acc = 0.7754, nmi = 0.7704, ari = 0.6941 ;  loss= [0.043086343, 0.10613032, 0.02186028]\n",
      "Iter 1400 acc = 0.7766, nmi = 0.7693, ari = 0.6944 ;  loss= [0.027317084, 0.053455167, 0.01662605]\n",
      "Iter 1540 acc = 0.7788, nmi = 0.7737, ari = 0.6981 ;  loss= [0.04187894, 0.1027877, 0.021321397]\n",
      "saving model to: temp/dcec_model_1560.h5\n",
      "Iter 1680 acc = 0.7792, nmi = 0.7729, ari = 0.6984 ;  loss= [0.025116976, 0.04258665, 0.016599646]\n",
      "Iter 1820 acc = 0.7792, nmi = 0.7747, ari = 0.6989 ;  loss= [0.04047636, 0.09829817, 0.020816727]\n",
      "saving model to: temp/dcec_model_1950.h5\n",
      "Iter 1960 acc = 0.7799, nmi = 0.7758, ari = 0.7004 ;  loss= [0.020120006, 0.037661877, 0.012587631]\n",
      "saving model to: temp/dcec_model_final.h5\n",
      "acc = 0.7799, nmi = 0.7758, ari = 0.7004\n"
     ]
    }
   ],
   "source": [
    "t10 = time()\n",
    "dcec.fit(x, y=y, tol=Ntol, maxiter=2e3,\n",
    "             update_interval=140)\n",
    "y_pred = dcec.y_pred\n",
    "t11 = time()\n",
    "\n",
    "print('acc = %.4f, nmi = %.4f, ari = %.4f' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred), metrics.ari(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname=\"ypred.csv\",fmt=\"%d\", delimiter=\",\", X=y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrain time:   0.2062227725982666\n",
      "Clustering time: 247.71899437904358\n"
     ]
    }
   ],
   "source": [
    "print('Pretrain time:  ', t1 - t0)\n",
    "print('Clustering time:', t11 - t10)\n",
    "#print('Total time:     ', t3 - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
