{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/XifengGuo/DCEC/blob/master/DCEC.py\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from sklearn.cluster import KMeans\n",
    "from datasets import load_mnist, load_usps, load_mrec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "from ConvAEmnist import CAE\n",
    "yHid=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(aName,yy,_loss):\n",
    "        if yHid is not None:\n",
    "                    acc = np.round(metrics.acc(yHid, yy), 5)\n",
    "                    nmi = np.round(metrics.nmi(yHid, yy), 5)\n",
    "                    ari = np.round(metrics.ari(yHid, yy), 5)\n",
    "                    loss = np.round(_loss, 7)\n",
    "                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=_loss[0], Lc=_loss[1], Lr=_loss[2])\n",
    "                    #logwriter.writerow(logdict)\n",
    "                    # print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss,'  delta=',delta_label)\n",
    "\n",
    "                    print(aName,'acc = %.4f, nmi = %.4f, ari = %.4f' % (acc,nmi,ari),';  loss=',_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    " \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight('clusters',(self.n_clusters, input_dim), initializer='glorot_uniform')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCEC(object):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 filters=[32, 64, 128, 10],\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0):\n",
    "\n",
    "        super(DCEC, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "        self.pretrained = False\n",
    "        self.y_pred = []\n",
    "        self.delata_label = 0 \n",
    "        self.save_dir='temp'\n",
    "        self.yHid=None\n",
    "\n",
    "        self.cae = CAE(input_shape, filters)\n",
    "        hidden = self.cae.get_layer(name='embedding').output\n",
    "        self.encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "        # Define DCEC model\n",
    "        print('nn',self.n_clusters)\n",
    "        clustering_layer = ClusteringLayer(n_clusters=self.n_clusters, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=self.cae.input,\n",
    "                           outputs=[clustering_layer, self.cae.output])\n",
    "\n",
    "    def pretrain(self, x, batch_size=256, epochs=50, optimizer='adam'):\n",
    "        print('...Pretraining...')\n",
    "        self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "        from tensorflow.keras.callbacks import CSVLogger\n",
    "        csv_logger = CSVLogger(self.save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=2, callbacks=[csv_logger])\n",
    "        nq, _ = self.model.predict(x, verbose=0)\n",
    "        npp= nq.argmax(1)\n",
    "        printMetrics('Iter a',npp,0)\n",
    "             \n",
    "        t0 = time()\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        nq, _ = self.model.predict(x, verbose=0)\n",
    "        npp= nq.argmax(1)\n",
    "        printMetrics('Iter a',npp,0)\n",
    "        self.cae.save(self.save_dir + '/pretrain_cae_model.h5')\n",
    "        print('Pretrained weights are saved to %s/pretrain_cae_model.h5' % self.save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, loss=['kld', 'mse'], loss_weights=[1, 1], optimizer='adam'):\n",
    "        self.model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "    def dopretrain(self,x, cae_weights=None,batch_size=256,epochs=50):\n",
    "        # Step 1: pretrain if necessary\n",
    "        if not self.pretrained or (cae_weights is None):\n",
    "            #Nepoch=50\n",
    "            print('...pretraining CAE using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=',epochs)\n",
    "            self.pretrain(x, batch_size, epochs=epochs)\n",
    "            self.cae.save_weights( 'pretrain_cae.h5')\n",
    "      \n",
    "            self.pretrained = True\n",
    "        elif cae_weights is not None:\n",
    "            self.cae.load_weights('pretrain_cae.h5')\n",
    "            print('cae_weights is loaded successfully.')\n",
    "  \n",
    "    def fit(self, x, batch_size=256, maxiter=2e3, tol=1e-2,\n",
    "            update_interval=140, save_dir='temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval=update_interval*2 \n",
    "        #save_interval = int(x.shape[0] / batch_size * 10)\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "   \n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        #t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "        loss = [0, 0, 0]\n",
    " \n",
    "        printMetrics(\"kMeans\",self.y_pred,loss)    \n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "        logfile = open(self.save_dir + '/dcec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "                ii=index * batch_size\n",
    "                #print('x',x[ii])\n",
    "                print('p',p[ii])\n",
    "                print('Q',q[ii])\n",
    "                self.q_pred=q;\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                #self.zcluster=\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                printMetrics('Iter '+ str(ite),self.y_pred,loss)\n",
    "             \n",
    "                # check stop criterion\n",
    "                #delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    #logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if (index + 1) * batch_size > x.shape[0]:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size::],\n",
    "                                                 y=[p[index * batch_size::], x[index * batch_size::]])\n",
    "                index = 0\n",
    "            else:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size:(index + 1) * batch_size],\n",
    "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    x[index * batch_size:(index + 1) * batch_size]])\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                # save DCEC model checkpoints\n",
    "                print('saving model to:', save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/dcec_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/dcec_model_final.h5')\n",
    "        #t3 = time()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "Ndataset='mnist-test'\n",
    "Nclusters=10\n",
    "Nsave_dir='temp'\n",
    "Ngamma=0.1\n",
    "Ntol=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd=[1,2,4]\n",
    "np.savetxt(fname=\"saved.csv\", delimiter=\",\", X=ddd)\n",
    "\n",
    "#ddd.savetxt('mnisttxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusteringll = ClusteringLayer(10, name='clustering')(hidden)\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "#(x_t, y_t), (x_s, y_s) = mnist.load_data()\n",
    "#xtt = x_t.reshape(-1, 784).astype('int32')\n",
    "\n",
    "#np.savetxt(fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\", X=xtt)\n",
    "\n",
    "#x_t.savetxt('mnisttxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: (70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_mnist, load_usps, load_mrec\n",
    "if Ndataset == 'mnist':\n",
    "    x, yHid = load_mnist()\n",
    "elif Ndataset == 'usps':\n",
    "    x, yHid = load_usps('data/usps')\n",
    "elif Ndataset == 'mrec':\n",
    "    x, YY = load_mrec()\n",
    "elif Ndataset == 'mnist-test':\n",
    "    x, yHid = load_mnist()\n",
    "    x, yHid = x[60000:], yHid[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 14, 14, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 7, 7, 64)          51264     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 10)                11530     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1152)              12672     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)    (None, 7, 7, 64)          73792     \n",
      "_________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)    (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)    (None, 28, 28, 1)         801       \n",
      "=================================================================\n",
      "Total params: 275,979\n",
      "Trainable params: 275,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "nn 10\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1_input (InputLayer)        [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 14, 14, 32)   832         conv1_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 7, 7, 64)     51264       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 3, 3, 128)    73856       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 1152)         0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Dense)               (None, 10)           11530       flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1152)         12672       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 3, 3, 128)    0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)       (None, 7, 7, 64)     73792       reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)       (None, 14, 14, 32)   51232       deconv3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "clustering (ClusteringLayer)    (None, 10)           100         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)       (None, 28, 28, 1)    801         deconv2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 276,079\n",
      "Trainable params: 276,079\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prepare the DCEC model\n",
    "dcec = DCEC(input_shape=x.shape[1:], filters=[32, 64, 128, 10], n_clusters=Nclusters)\n",
    "#plot_model(dcec.model, to_file=Nsave_dir + '/dcec_model.png', show_shapes=True)\n",
    "dcec.model.summary()\n",
    "dcec.save_dir=Nsave_dir\n",
    "dcec.yHid=yHid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "Ntol=0.0001\n",
    "Ngamma=0.1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09576062  0.31623173 -0.02340065 ...  0.00540897 -0.06257829\n",
      "   0.07884236]\n",
      " [-0.10482959 -0.03164749  0.01005473 ... -0.4290024   1.2822636\n",
      "  -0.01309455]\n",
      " [ 0.0533722   0.69349027 -0.06957897 ...  0.14484234 -0.27014762\n",
      "   0.08944123]\n",
      " ...\n",
      " [ 0.37646866  0.09096929 -1.4917985  ...  0.14286338 -0.03764597\n",
      "   0.7034949 ]\n",
      " [ 0.11029965  0.01935705 -0.0040554  ...  0.05618998 -0.15114951\n",
      "   0.15795447]\n",
      " [-0.18174101 -0.08479969  0.32634103 ... -0.14315802  1.921741\n",
      "  -0.42669982]]\n"
     ]
    }
   ],
   "source": [
    "qq=dcec.encoder.predict(x)\n",
    "pp = dcec.target_distribution(qq)  # update the auxiliary target distribution p\n",
    "print(pp)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...pretraining CAE using default hyper-parameters:\n",
      "   optimizer='adam';   epochs= 10\n",
      "...Pretraining...\n",
      "Epoch 1/2\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 0.0790\n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 3s 313us/sample - loss: 0.0607\n",
      "Iter a acc = 0.2322, nmi = 0.2265, ari = 0.0316 ;  loss= 0\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 3s 313us/sample - loss: 0.0452\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 3s 312us/sample - loss: 0.0345\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 3s 312us/sample - loss: 0.0291\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 3s 313us/sample - loss: 0.0257\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 3s 313us/sample - loss: 0.0231\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 3s 312us/sample - loss: 0.0216\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 3s 312us/sample - loss: 0.0204\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 3s 313us/sample - loss: 0.0196\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 3s 313us/sample - loss: 0.0188\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 3s 313us/sample - loss: 0.0183\n",
      "Pretraining time:  31.26853632926941\n",
      "Iter a acc = 0.3452, nmi = 0.2727, ari = 0.1290 ;  loss= 0\n",
      "Pretrained weights are saved to temp/pretrain_cae_model.h5\n"
     ]
    }
   ],
   "source": [
    "dcec.compile(loss=['kld', 'mse'], loss_weights=[Ngamma, 1], optimizer=optimizer)\n",
    "t0 = time() \n",
    "dcec.dopretrain(x,cae_weights=None,batch_size=256,epochs=10)\n",
    "#dcec.dopretrain(x,cae_weights=1,batch_size=256,epochs=30)\n",
    "t1 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update interval 140\n",
      "Save interval 280\n",
      "Initializing cluster centers with k-means.\n",
      "kMeans acc = 0.6002, nmi = 0.5750, ari = 0.4810 ;  loss= [0, 0, 0]\n",
      "p [0.00455987 0.01443567 0.00996763 0.01706933 0.00528897 0.9182013\n",
      " 0.01248278 0.00562948 0.00410865 0.00825639]\n",
      "Q [0.0367986  0.06953757 0.05390224 0.07309902 0.03736122 0.54095054\n",
      " 0.06356531 0.0371798  0.03209889 0.05550686]\n",
      "Iter 0 acc = 0.6002, nmi = 0.5750, ari = 0.4810 ;  loss= [0, 0, 0]\n",
      "saving model to: temp/dcec_model_0.h5\n",
      "p [0.03930619 0.00825358 0.01557406 0.03685729 0.82053626 0.00760787\n",
      " 0.02260883 0.01670157 0.02301905 0.00953533]\n",
      "Q [0.09276541 0.0479304  0.05984508 0.09271634 0.4102676  0.04376347\n",
      " 0.07485679 0.05833763 0.06574073 0.05377654]\n",
      "Iter 140 acc = 0.6164, nmi = 0.6002, ari = 0.5059 ;  loss= [0.024405394, 0.06862287, 0.017543107]\n",
      "p [1.8210593e-04 8.6325326e-04 2.4908027e-04 8.0788013e-04 1.5980525e-04\n",
      " 9.9673593e-01 4.1716930e-04 1.3107101e-04 1.4707533e-04 3.0665201e-04]\n",
      "Q [0.0103443  0.02731973 0.0134798  0.02362429 0.00989504 0.86325425\n",
      " 0.01776713 0.00916587 0.00879998 0.01634957]\n",
      "Iter 280 acc = 0.6327, nmi = 0.6317, ari = 0.5327 ;  loss= [0.024335027, 0.078283355, 0.01650669]\n",
      "saving model to: temp/dcec_model_280.h5\n",
      "p [4.9220300e-03 6.3853909e-04 2.4413809e-03 2.3589681e-03 9.8079270e-01\n",
      " 7.3422154e-04 2.3117959e-03 1.3027358e-03 3.2643666e-03 1.2332257e-03]\n",
      "Q [0.04644644 0.02135905 0.03837953 0.03624986 0.70905626 0.02072916\n",
      " 0.03777796 0.02596578 0.03573454 0.02830148]\n",
      "Iter 420 acc = 0.6440, nmi = 0.6509, ari = 0.5491 ;  loss= [0.03597069, 0.15545726, 0.020424966]\n",
      "p [1.0308151e-04 5.0183100e-04 1.0740221e-04 4.2536238e-04 7.4021460e-05\n",
      " 9.9830276e-01 1.9270611e-04 6.1885039e-05 8.2766121e-05 1.4820599e-04]\n",
      "Q [0.00760547 0.02203106 0.00962632 0.01777041 0.00718349 0.8983994\n",
      " 0.01289913 0.00667443 0.00652275 0.01128753]\n",
      "Iter 560 acc = 0.6509, nmi = 0.6621, ari = 0.5595 ;  loss= [0.026941385, 0.11017616, 0.015923768]\n",
      "saving model to: temp/dcec_model_560.h5\n",
      "p [1.0959979e-03 1.3236675e-04 6.6973211e-04 4.4843974e-04 9.9567914e-01\n",
      " 1.6496115e-04 5.0672278e-04 2.6060006e-04 7.4370945e-04 2.9838210e-04]\n",
      "Q [0.02405959 0.01139612 0.02405411 0.01821204 0.84148073 0.01143409\n",
      " 0.02107463 0.01377171 0.01895736 0.01555966]\n",
      "Iter 700 acc = 0.6546, nmi = 0.6712, ari = 0.5670 ;  loss= [0.037954144, 0.16937369, 0.021016773]\n",
      "p [6.2934312e-05 3.0615448e-04 5.5930883e-05 2.5975262e-04 3.9878891e-05\n",
      " 9.9900889e-01 1.0266999e-04 3.2598968e-05 4.8752168e-05 8.2498547e-05]\n",
      "Q [0.00582951 0.01771505 0.00721933 0.01421558 0.00546808 0.9213793\n",
      " 0.00982093 0.00501338 0.0049217  0.00841705]\n",
      "Iter 840 acc = 0.6580, nmi = 0.6785, ari = 0.5741 ;  loss= [0.024647724, 0.09633674, 0.0150140505]\n",
      "saving model to: temp/dcec_model_840.h5\n",
      "p [7.2341232e-04 8.0764039e-05 4.3555727e-04 2.7648310e-04 9.9722165e-01\n",
      " 1.0191779e-04 3.1369092e-04 1.5936245e-04 4.9601321e-04 1.9112778e-04]\n",
      "Q [0.01947417 0.00914923 0.02022417 0.01473078 0.8707056  0.00926947\n",
      " 0.0173419  0.01112481 0.01537617 0.01260359]\n",
      "Iter 980 acc = 0.6603, nmi = 0.6842, ari = 0.5786 ;  loss= [0.035651363, 0.1573481, 0.019916555]\n",
      "saving model to: temp/dcec_model_final.h5\n",
      "end: acc = 0.6603, nmi = 0.6842, ari = 0.5786 ;  loss= [0]\n"
     ]
    }
   ],
   "source": [
    "t10 = time()\n",
    "dcec.fit(x,  tol=Ntol, maxiter=1e3,\n",
    "             update_interval=140)\n",
    "y_pred = dcec.y_pred\n",
    "q_pred=dcec.q_pred\n",
    "t11 = time()\n",
    "\n",
    "printMetrics('end:',y_pred,[0])\n",
    "#print('acc = %.4f, nmi = %.4f, ari = %.4f' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred), metrics.ari(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname=\"ypred.csv\",fmt=\"%d\", delimiter=\",\", X=y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mse( inputs, targets):\n",
    "    error = inputs - targets\n",
    "    return tf.reduce_mean(tf.square(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_kld(inputs,pred):\n",
    "    _nn=tf.keras.losses.KLD( inputs,pred)\n",
    "    #return tf.keras.losses.KLDivergence(inputs,pred)\n",
    "    #loss = k([.4, .9, .2], [.5, .8, .12])\n",
    "    #return _nn\n",
    "    return tf.reduce_mean(_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00577047 0.01677052 0.00722646 ... 0.00497918 0.00474525 0.00816023]\n",
      " [0.07880014 0.02936345 0.08832586 ... 0.03858086 0.06850583 0.03879023]\n",
      " [0.00689968 0.00585611 0.00557377 ... 0.00259356 0.00387477 0.9426914 ]\n",
      " ...\n",
      " [0.01067292 0.01162967 0.006573   ... 0.00594893 0.00642999 0.01326605]\n",
      " [0.02206011 0.01855258 0.01248496 ... 0.01189885 0.01241228 0.02219081]\n",
      " [0.83956116 0.01277704 0.01377647 ... 0.01773834 0.04129395 0.01309343]]\n"
     ]
    }
   ],
   "source": [
    "yp=y_pred.astype(K.floatx())\n",
    "print(q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f701c50d6834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKLDivergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.09\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loss: -0.043\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "k = tf.keras.losses.KLDivergence()\n",
    "loss = k([.4, .9, .2], [.4, .09, .21])\n",
    "print('Loss: ', loss.numpy())  # Loss: -0.043\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zp=dcec.encoder.predict(x)\n",
    "print(zp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, q2 = dcec.model.predict(x, verbose=1)\n",
    "print(q2[0].shape)\n",
    "print(loss_mse(x,q2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q.shape)\n",
    "print(p[0])\n",
    "print(loss_kld(p,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_kld([[1.,2.,3000],[2,3,14.1]],[[11.,20.,30],[2,3,0.000400]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dcec.target_distribution(q_pred)  # update the auxiliary target distribution p\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_kld(p,q_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "_nn=tf.keras.losses.KLD(zp, p)\n",
    "print(_nn)\n",
    "print( tf.reduce_mean(_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25,3))\n",
    "\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YY['labeld']=y_pred\n",
    "YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "np.savetxt(fname=\"mreresult.csv\",fmt=\"%d\\t%d\\t%f\", delimiter=\"\\t\",header=\"time\\tnn\\tlabeld\", X=YY,comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pretrain time:  ', t1 - t0)\n",
    "print('Clustering time:', t11 - t10)\n",
    "#print('Total time:     ', t3 - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
