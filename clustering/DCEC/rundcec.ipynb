{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/XifengGuo/DCEC/blob/master/DCEC.py\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from sklearn.cluster import KMeans\n",
    "from datasets import load_mnist, load_usps, load_mrec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "from ConvAE import CAE\n",
    "yHid=None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMetrics(aName,yy,_loss):\n",
    "        if yHid is not None:\n",
    "                    acc = np.round(metrics.acc(yHid, yy), 5)\n",
    "                    nmi = np.round(metrics.nmi(yHid, yy), 5)\n",
    "                    ari = np.round(metrics.ari(yHid, yy), 5)\n",
    "                    loss = np.round(_loss, 7)\n",
    "                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=_loss[0], Lc=_loss[1], Lr=_loss[2])\n",
    "                    #logwriter.writerow(logdict)\n",
    "                    # print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss,'  delta=',delta_label)\n",
    "\n",
    "                    print(aName,'acc = %.4f, nmi = %.4f, ari = %.4f' % (acc,nmi,ari),';  loss=',_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    " \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight('clusters',(self.n_clusters, input_dim), initializer='glorot_uniform')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCEC(object):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 filters=[32, 64, 128, 10],\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0):\n",
    "\n",
    "        super(DCEC, self).__init__()\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "        self.pretrained = False\n",
    "        self.y_pred = []\n",
    "        self.delata_label = 0 \n",
    "        self.save_dir='temp'\n",
    "        self.yHid=None\n",
    "\n",
    "        self.cae = CAE(input_shape, filters)\n",
    "        hidden = self.cae.get_layer(name='embedding').output\n",
    "        self.encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "        # Define DCEC model\n",
    "        print('nn',self.n_clusters)\n",
    "        clustering_layer = ClusteringLayer(n_clusters=self.n_clusters, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=self.cae.input,\n",
    "                           outputs=[clustering_layer, self.cae.output])\n",
    "\n",
    "    def pretrain(self, x, batch_size=256, epochs=50, optimizer='adam'):\n",
    "        print('...Pretraining...')\n",
    "        self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "        from tensorflow.keras.callbacks import CSVLogger\n",
    "        csv_logger = CSVLogger(self.save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "        t0 = time()\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.cae.save(self.save_dir + '/pretrain_cae_model.h5')\n",
    "        print('Pretrained weights are saved to %s/pretrain_cae_model.h5' % self.save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights_path):\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def extract_feature(self, x):  # extract features from before clustering layer\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        q, _ = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, loss=['kld', 'mse'], loss_weights=[1, 1], optimizer='adam'):\n",
    "        self.model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer)\n",
    "\n",
    "    def dopretrain(self,x, cae_weights=None,batch_size=256,epochs=50):\n",
    "        # Step 1: pretrain if necessary\n",
    "        if not self.pretrained or (cae_weights is None):\n",
    "            #Nepoch=50\n",
    "            print('...pretraining CAE using default hyper-parameters:')\n",
    "            print('   optimizer=\\'adam\\';   epochs=',epochs)\n",
    "            self.pretrain(x, batch_size, epochs=epochs)\n",
    "            self.cae.save_weights( 'pretrain_cae.h5')\n",
    "      \n",
    "            self.pretrained = True\n",
    "        elif cae_weights is not None:\n",
    "            self.cae.load_weights('pretrain_cae.h5')\n",
    "            print('cae_weights is loaded successfully.')\n",
    "  \n",
    "    def fit(self, x, batch_size=256, maxiter=2e3, tol=1e-2,\n",
    "            update_interval=140, save_dir='temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval=update_interval*2 \n",
    "        #save_interval = int(x.shape[0] / batch_size * 10)\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "   \n",
    "        # Step 2: initialize cluster centers using k-means\n",
    "        #t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        self.y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(self.y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "        loss = [0, 0, 0]\n",
    " \n",
    "        printMetrics(\"kMeans\",self.y_pred,loss)    \n",
    "        # Step 3: deep clustering\n",
    "        # logging file\n",
    "        import csv, os\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "        logfile = open(self.save_dir + '/dcec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'L', 'Lc', 'Lr'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        index = 0\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q, _ = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                self.y_pred = q.argmax(1)\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                printMetrics('Iter '+ str(ite),self.y_pred,loss)\n",
    "             \n",
    "                # check stop criterion\n",
    "                #delta_label = np.sum(self.y_pred != y_pred_last).astype(np.float32) / self.y_pred.shape[0]\n",
    "                y_pred_last = np.copy(self.y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    #logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            if (index + 1) * batch_size > x.shape[0]:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size::],\n",
    "                                                 y=[p[index * batch_size::], x[index * batch_size::]])\n",
    "                index = 0\n",
    "            else:\n",
    "                loss = self.model.train_on_batch(x=x[index * batch_size:(index + 1) * batch_size],\n",
    "                                                 y=[p[index * batch_size:(index + 1) * batch_size],\n",
    "                                                    x[index * batch_size:(index + 1) * batch_size]])\n",
    "                index += 1\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                # save DCEC model checkpoints\n",
    "                print('saving model to:', save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/dcec_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/dcec_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/dcec_model_final.h5')\n",
    "        #t3 = time()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "Ndataset='mrec'\n",
    "Nclusters=10\n",
    "Nsave_dir='temp'\n",
    "Ngamma=0.1\n",
    "Ntol=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd=[1,2,4]\n",
    "np.savetxt(fname=\"saved.csv\", delimiter=\",\", X=ddd)\n",
    "\n",
    "#ddd.savetxt('mnisttxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusteringll = ClusteringLayer(10, name='clustering')(hidden)\n",
    "#from tensorflow.keras.datasets import mnist\n",
    "#(x_t, y_t), (x_s, y_s) = mnist.load_data()\n",
    "#xtt = x_t.reshape(-1, 784).astype('int32')\n",
    "\n",
    "#np.savetxt(fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\", X=xtt)\n",
    "\n",
    "#x_t.savetxt('mnisttxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MM (4032, 258)\n",
      "MREC: (4031, 8, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_mnist, load_usps, load_mrec\n",
    "if Ndataset == 'mnist':\n",
    "    x, yHid = load_mnist()\n",
    "elif Ndataset == 'usps':\n",
    "    x, yHid = load_usps('data/usps')\n",
    "elif Ndataset == 'mrec':\n",
    "    x, YY = load_mrec()\n",
    "elif Ndataset == 'mnist-test':\n",
    "    x, yHid = load_mnist()\n",
    "    x, yHid = x[60000:], yHid[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 8, 16, 32)         832       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 4, 4, 64)          51264     \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 2, 2, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Dense)            (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               5632      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)    (None, 4, 4, 128)         147584    \n",
      "_________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)    (None, 8, 16, 64)         73792     \n",
      "_________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)    (None, 8, 32, 1)          1601      \n",
      "=================================================================\n",
      "Total params: 359,691\n",
      "Trainable params: 359,691\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "nn 10\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "conv1_input (InputLayer)        [(None, 8, 32, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 8, 16, 32)    832         conv1_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv2D)                  (None, 4, 4, 64)     51264       conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv2D)                  (None, 2, 2, 128)    73856       conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Dense)               (None, 10)           5130        flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          5632        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 2, 2, 128)    0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "deconv3 (Conv2DTranspose)       (None, 4, 4, 128)    147584      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "deconv2 (Conv2DTranspose)       (None, 8, 16, 64)    73792       deconv3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "clustering (ClusteringLayer)    (None, 10)           100         embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deconv1 (Conv2DTranspose)       (None, 8, 32, 1)     1601        deconv2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 359,791\n",
      "Trainable params: 359,791\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prepare the DCEC model\n",
    "dcec = DCEC(input_shape=x.shape[1:], filters=[32, 64, 128, 10], n_clusters=Nclusters)\n",
    "#plot_model(dcec.model, to_file=Nsave_dir + '/dcec_model.png', show_shapes=True)\n",
    "dcec.model.summary()\n",
    "dcec.save_dir=Nsave_dir\n",
    "dcec.yHid=yHid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = 'adam'\n",
    "Ntol=0.001\n",
    "Ngamma=0.2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...pretraining CAE using default hyper-parameters:\n",
      "   optimizer='adam';   epochs= 90\n",
      "...Pretraining...\n",
      "Epoch 1/90\n",
      "4031/4031 [==============================] - 1s 207us/sample - loss: 0.2637\n",
      "Epoch 2/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0789\n",
      "Epoch 3/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 0.0583\n",
      "Epoch 4/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0538\n",
      "Epoch 5/90\n",
      "4031/4031 [==============================] - 1s 167us/sample - loss: 0.0527\n",
      "Epoch 6/90\n",
      "4031/4031 [==============================] - 1s 167us/sample - loss: 0.0521\n",
      "Epoch 7/90\n",
      "4031/4031 [==============================] - 1s 167us/sample - loss: 0.0506\n",
      "Epoch 8/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 0.0396\n",
      "Epoch 9/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 0.0256\n",
      "Epoch 10/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 0.0213\n",
      "Epoch 11/90\n",
      "4031/4031 [==============================] - 1s 166us/sample - loss: 0.0163\n",
      "Epoch 12/90\n",
      "4031/4031 [==============================] - 1s 167us/sample - loss: 0.0064\n",
      "Epoch 13/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 0.0033\n",
      "Epoch 14/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0025\n",
      "Epoch 15/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 0.0021\n",
      "Epoch 16/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0017\n",
      "Epoch 17/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0015\n",
      "Epoch 18/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0014\n",
      "Epoch 19/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 0.0013\n",
      "Epoch 20/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 0.0013\n",
      "Epoch 21/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0012\n",
      "Epoch 22/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0012\n",
      "Epoch 23/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0011\n",
      "Epoch 24/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0011\n",
      "Epoch 25/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 0.0011\n",
      "Epoch 26/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 0.0011\n",
      "Epoch 27/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 0.0010\n",
      "Epoch 28/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 0.0010\n",
      "Epoch 29/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 0.0010\n",
      "Epoch 30/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 9.8730e-04\n",
      "Epoch 31/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 9.6566e-04\n",
      "Epoch 32/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 9.4396e-04\n",
      "Epoch 33/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 9.3709e-04\n",
      "Epoch 34/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 9.0612e-04\n",
      "Epoch 35/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 8.8989e-04\n",
      "Epoch 36/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 8.6981e-04\n",
      "Epoch 37/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 8.7456e-04\n",
      "Epoch 38/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 8.5974e-04\n",
      "Epoch 39/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 8.3865e-04\n",
      "Epoch 40/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 8.0727e-04\n",
      "Epoch 41/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 8.5704e-04\n",
      "Epoch 42/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 8.1441e-04\n",
      "Epoch 43/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 7.7802e-04\n",
      "Epoch 44/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 7.5426e-04\n",
      "Epoch 45/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 7.7078e-04\n",
      "Epoch 46/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 7.3205e-04\n",
      "Epoch 47/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 7.3547e-04\n",
      "Epoch 48/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 6.7608e-04\n",
      "Epoch 49/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 7.0482e-04\n",
      "Epoch 50/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 8.6732e-04\n",
      "Epoch 51/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 6.4670e-04\n",
      "Epoch 52/90\n",
      "4031/4031 [==============================] - 1s 172us/sample - loss: 6.0242e-04\n",
      "Epoch 53/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.8242e-04\n",
      "Epoch 54/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.6424e-04\n",
      "Epoch 55/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 5.6199e-04\n",
      "Epoch 56/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 6.1986e-04\n",
      "Epoch 57/90\n",
      "4031/4031 [==============================] - 1s 172us/sample - loss: 5.4785e-04\n",
      "Epoch 58/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.4093e-04\n",
      "Epoch 59/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.4603e-04\n",
      "Epoch 60/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 5.4703e-04\n",
      "Epoch 61/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 4.9231e-04\n",
      "Epoch 62/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.5172e-04\n",
      "Epoch 63/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.2802e-04\n",
      "Epoch 64/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 4.6865e-04\n",
      "Epoch 65/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 5.0310e-04\n",
      "Epoch 66/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 5.4226e-04\n",
      "Epoch 67/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 4.6751e-04\n",
      "Epoch 68/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 4.1935e-04\n",
      "Epoch 69/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.1482e-04\n",
      "Epoch 70/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 5.1356e-04\n",
      "Epoch 71/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 4.2236e-04\n",
      "Epoch 72/90\n",
      "4031/4031 [==============================] - 1s 167us/sample - loss: 4.0645e-04\n",
      "Epoch 73/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 4.1307e-04\n",
      "Epoch 74/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 5.5554e-04\n",
      "Epoch 75/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 4.3693e-04\n",
      "Epoch 76/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 4.0093e-04\n",
      "Epoch 77/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 4.1041e-04\n",
      "Epoch 78/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 3.9706e-04\n",
      "Epoch 79/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 4.8579e-04\n",
      "Epoch 80/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 4.0883e-04\n",
      "Epoch 81/90\n",
      "4031/4031 [==============================] - 1s 172us/sample - loss: 3.8581e-04\n",
      "Epoch 82/90\n",
      "4031/4031 [==============================] - 1s 168us/sample - loss: 4.7114e-04\n",
      "Epoch 83/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 4.1477e-04\n",
      "Epoch 84/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 4.1403e-04\n",
      "Epoch 85/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 4.1316e-04\n",
      "Epoch 86/90\n",
      "4031/4031 [==============================] - 1s 171us/sample - loss: 4.4057e-04\n",
      "Epoch 87/90\n",
      "4031/4031 [==============================] - 1s 172us/sample - loss: 4.4755e-04\n",
      "Epoch 88/90\n",
      "4031/4031 [==============================] - 1s 170us/sample - loss: 4.1582e-04\n",
      "Epoch 89/90\n",
      "4031/4031 [==============================] - 1s 169us/sample - loss: 3.6702e-04\n",
      "Epoch 90/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4031/4031 [==============================] - 1s 170us/sample - loss: 3.8499e-04\n",
      "Pretraining time:  62.251858949661255\n",
      "Pretrained weights are saved to temp/pretrain_cae_model.h5\n"
     ]
    }
   ],
   "source": [
    "dcec.compile(loss=['kld', 'mse'], loss_weights=[Ngamma, 1], optimizer=optimizer)\n",
    "t0 = time() \n",
    "dcec.dopretrain(x,cae_weights=None,batch_size=256,epochs=90)\n",
    "#dcec.dopretrain(x,cae_weights=1,batch_size=256,epochs=30)\n",
    "t1 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t10 = time()\n",
    "dcec.fit(x,  tol=Ntol, maxiter=2e3,\n",
    "             update_interval=140)\n",
    "y_pred = dcec.y_pred\n",
    "t11 = time()\n",
    "\n",
    "printMetrics('end:',y_pred,[0])\n",
    "#print('acc = %.4f, nmi = %.4f, ari = %.4f' % (metrics.acc(y, y_pred), metrics.nmi(y, y_pred), metrics.ari(y, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(fname=\"ypred.csv\",fmt=\"%d\", delimiter=\",\", X=y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25,3))\n",
    "\n",
    "plt.plot(y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YY['labeld']=y_pred\n",
    "YY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "np.savetxt(fname=\"mreresult.csv\",fmt=\"%d\\t%d\\t%f\", delimiter=\"\\t\",header=\"time\\tnn\\tlabeld\", X=YY,comments='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pretrain time:  ', t1 - t0)\n",
    "print('Clustering time:', t11 - t10)\n",
    "#print('Total time:     ', t3 - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
