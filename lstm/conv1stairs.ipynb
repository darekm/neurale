{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bartek/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Conv2D,Flatten\n",
    "from keras.backend import argmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "csv_url='mrec20190331sfft.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>training</th>\n",
       "      <th>step</th>\n",
       "      <th>up</th>\n",
       "      <th>slice</th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>...</th>\n",
       "      <th>F_119</th>\n",
       "      <th>F_120</th>\n",
       "      <th>F_121</th>\n",
       "      <th>F_122</th>\n",
       "      <th>F_123</th>\n",
       "      <th>F_124</th>\n",
       "      <th>F_125</th>\n",
       "      <th>F_126</th>\n",
       "      <th>F_127</th>\n",
       "      <th>F_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1554034538313</td>\n",
       "      <td>13:15:38.313</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20749</td>\n",
       "      <td>20.056</td>\n",
       "      <td>94.1213</td>\n",
       "      <td>56.6893</td>\n",
       "      <td>244.1506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.6455</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>1.0433</td>\n",
       "      <td>1.3137</td>\n",
       "      <td>2.4414</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>1.2927</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>1.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1554034538958</td>\n",
       "      <td>13:15:38.958</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11691</td>\n",
       "      <td>42.667</td>\n",
       "      <td>47.5442</td>\n",
       "      <td>72.8170</td>\n",
       "      <td>77.0697</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>1.2355</td>\n",
       "      <td>1.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1554034539517</td>\n",
       "      <td>13:15:39.517</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>16388</td>\n",
       "      <td>14.943</td>\n",
       "      <td>104.5249</td>\n",
       "      <td>123.5440</td>\n",
       "      <td>325.2235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>2.3395</td>\n",
       "      <td>1.9636</td>\n",
       "      <td>2.7001</td>\n",
       "      <td>1.6907</td>\n",
       "      <td>1.3859</td>\n",
       "      <td>1.2406</td>\n",
       "      <td>1.8272</td>\n",
       "      <td>2.2453</td>\n",
       "      <td>1.9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1554034540279</td>\n",
       "      <td>13:15:40.279</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7260</td>\n",
       "      <td>67.612</td>\n",
       "      <td>65.4221</td>\n",
       "      <td>27.6171</td>\n",
       "      <td>155.5133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1.1747</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.3874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1554034540774</td>\n",
       "      <td>13:15:40.774</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>10254</td>\n",
       "      <td>51.408</td>\n",
       "      <td>151.3434</td>\n",
       "      <td>105.4404</td>\n",
       "      <td>376.3469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4306</td>\n",
       "      <td>1.2238</td>\n",
       "      <td>1.7379</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>1.4681</td>\n",
       "      <td>0.6975</td>\n",
       "      <td>1.1393</td>\n",
       "      <td>1.1507</td>\n",
       "      <td>0.8577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1554034541375</td>\n",
       "      <td>13:15:41.375</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12306</td>\n",
       "      <td>43.714</td>\n",
       "      <td>197.7310</td>\n",
       "      <td>287.9238</td>\n",
       "      <td>458.5304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.2905</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.8668</td>\n",
       "      <td>1.0058</td>\n",
       "      <td>0.3264</td>\n",
       "      <td>0.7247</td>\n",
       "      <td>0.2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1554034542523</td>\n",
       "      <td>13:15:42.523</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>12275</td>\n",
       "      <td>36.233</td>\n",
       "      <td>4.3915</td>\n",
       "      <td>16.2003</td>\n",
       "      <td>67.3880</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1263</td>\n",
       "      <td>3.0966</td>\n",
       "      <td>0.7637</td>\n",
       "      <td>4.8209</td>\n",
       "      <td>2.8054</td>\n",
       "      <td>2.8560</td>\n",
       "      <td>6.5081</td>\n",
       "      <td>4.1053</td>\n",
       "      <td>4.8375</td>\n",
       "      <td>6.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1554034542921</td>\n",
       "      <td>13:15:42.921</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>11018</td>\n",
       "      <td>8.626</td>\n",
       "      <td>32.5530</td>\n",
       "      <td>114.6319</td>\n",
       "      <td>209.5523</td>\n",
       "      <td>...</td>\n",
       "      <td>12.8564</td>\n",
       "      <td>10.8709</td>\n",
       "      <td>12.2103</td>\n",
       "      <td>5.6484</td>\n",
       "      <td>4.5759</td>\n",
       "      <td>3.3081</td>\n",
       "      <td>11.8470</td>\n",
       "      <td>11.5552</td>\n",
       "      <td>9.8679</td>\n",
       "      <td>11.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1554034543324</td>\n",
       "      <td>13:15:43.324</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>48631</td>\n",
       "      <td>32.854</td>\n",
       "      <td>117.6710</td>\n",
       "      <td>414.5323</td>\n",
       "      <td>808.1890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7231</td>\n",
       "      <td>2.2230</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>1.8384</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>1.7533</td>\n",
       "      <td>0.5601</td>\n",
       "      <td>1.9654</td>\n",
       "      <td>1.1408</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1554034543872</td>\n",
       "      <td>13:15:43.872</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12990</td>\n",
       "      <td>95.478</td>\n",
       "      <td>328.9371</td>\n",
       "      <td>369.8189</td>\n",
       "      <td>122.3947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1821</td>\n",
       "      <td>0.3310</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>1.0916</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.7634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1554034552741</td>\n",
       "      <td>13:15:52.741</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7295</td>\n",
       "      <td>51.179</td>\n",
       "      <td>110.7519</td>\n",
       "      <td>121.1264</td>\n",
       "      <td>48.1045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1168</td>\n",
       "      <td>1.2650</td>\n",
       "      <td>1.0660</td>\n",
       "      <td>0.5436</td>\n",
       "      <td>1.8927</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.0917</td>\n",
       "      <td>1.9604</td>\n",
       "      <td>2.4926</td>\n",
       "      <td>2.0846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1554034553464</td>\n",
       "      <td>13:15:53.464</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15670</td>\n",
       "      <td>91.319</td>\n",
       "      <td>297.7418</td>\n",
       "      <td>238.2316</td>\n",
       "      <td>187.9039</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0638</td>\n",
       "      <td>2.1898</td>\n",
       "      <td>5.2268</td>\n",
       "      <td>3.8690</td>\n",
       "      <td>1.8847</td>\n",
       "      <td>4.5757</td>\n",
       "      <td>2.6038</td>\n",
       "      <td>3.7672</td>\n",
       "      <td>3.9683</td>\n",
       "      <td>2.8586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1554034554064</td>\n",
       "      <td>13:15:54.064</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7187</td>\n",
       "      <td>89.670</td>\n",
       "      <td>25.7131</td>\n",
       "      <td>114.5191</td>\n",
       "      <td>124.2920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6383</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>1.0036</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>1.6553</td>\n",
       "      <td>0.9874</td>\n",
       "      <td>0.3588</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.5634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1554034555772</td>\n",
       "      <td>13:15:55.772</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>63054</td>\n",
       "      <td>176.856</td>\n",
       "      <td>283.4218</td>\n",
       "      <td>629.3238</td>\n",
       "      <td>135.3471</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4961</td>\n",
       "      <td>1.1693</td>\n",
       "      <td>1.2748</td>\n",
       "      <td>0.8044</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.7229</td>\n",
       "      <td>1.1219</td>\n",
       "      <td>1.4850</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.9767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1554034556313</td>\n",
       "      <td>13:15:56.313</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7979</td>\n",
       "      <td>92.689</td>\n",
       "      <td>82.8024</td>\n",
       "      <td>227.5237</td>\n",
       "      <td>131.2919</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2423</td>\n",
       "      <td>7.1212</td>\n",
       "      <td>8.3202</td>\n",
       "      <td>2.1716</td>\n",
       "      <td>5.9391</td>\n",
       "      <td>6.3946</td>\n",
       "      <td>2.9856</td>\n",
       "      <td>4.9899</td>\n",
       "      <td>4.1904</td>\n",
       "      <td>3.5582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1554034556749</td>\n",
       "      <td>13:15:56.749</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9095</td>\n",
       "      <td>60.448</td>\n",
       "      <td>116.9732</td>\n",
       "      <td>83.3200</td>\n",
       "      <td>21.3689</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5014</td>\n",
       "      <td>2.7870</td>\n",
       "      <td>2.7309</td>\n",
       "      <td>1.7733</td>\n",
       "      <td>2.5238</td>\n",
       "      <td>1.4445</td>\n",
       "      <td>3.5992</td>\n",
       "      <td>1.5622</td>\n",
       "      <td>1.9059</td>\n",
       "      <td>1.0923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1554034557167</td>\n",
       "      <td>13:15:57.167</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10473</td>\n",
       "      <td>5.745</td>\n",
       "      <td>86.2626</td>\n",
       "      <td>214.0896</td>\n",
       "      <td>92.3728</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3324</td>\n",
       "      <td>5.7336</td>\n",
       "      <td>7.9026</td>\n",
       "      <td>9.3345</td>\n",
       "      <td>12.5422</td>\n",
       "      <td>9.1397</td>\n",
       "      <td>12.9683</td>\n",
       "      <td>10.5161</td>\n",
       "      <td>12.1184</td>\n",
       "      <td>8.5896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1554034557652</td>\n",
       "      <td>13:15:57.652</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9539</td>\n",
       "      <td>105.918</td>\n",
       "      <td>222.3163</td>\n",
       "      <td>146.9655</td>\n",
       "      <td>107.1227</td>\n",
       "      <td>...</td>\n",
       "      <td>23.9141</td>\n",
       "      <td>20.8094</td>\n",
       "      <td>6.7922</td>\n",
       "      <td>16.0169</td>\n",
       "      <td>20.7039</td>\n",
       "      <td>16.0992</td>\n",
       "      <td>14.9737</td>\n",
       "      <td>23.0030</td>\n",
       "      <td>20.7990</td>\n",
       "      <td>13.1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1554034558096</td>\n",
       "      <td>13:15:58.096</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>8128</td>\n",
       "      <td>113.464</td>\n",
       "      <td>202.6727</td>\n",
       "      <td>179.4488</td>\n",
       "      <td>142.2376</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3024</td>\n",
       "      <td>5.0787</td>\n",
       "      <td>5.5172</td>\n",
       "      <td>4.9711</td>\n",
       "      <td>3.8353</td>\n",
       "      <td>3.0314</td>\n",
       "      <td>3.4189</td>\n",
       "      <td>3.2492</td>\n",
       "      <td>1.4511</td>\n",
       "      <td>0.9276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1554034558486</td>\n",
       "      <td>13:15:58.486</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>8780</td>\n",
       "      <td>72.946</td>\n",
       "      <td>35.3417</td>\n",
       "      <td>163.8728</td>\n",
       "      <td>139.9921</td>\n",
       "      <td>...</td>\n",
       "      <td>19.7552</td>\n",
       "      <td>7.4400</td>\n",
       "      <td>12.9592</td>\n",
       "      <td>8.1229</td>\n",
       "      <td>18.1363</td>\n",
       "      <td>8.3398</td>\n",
       "      <td>11.5341</td>\n",
       "      <td>16.8605</td>\n",
       "      <td>8.9856</td>\n",
       "      <td>15.4814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1554034568646</td>\n",
       "      <td>13:16:08.646</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8535</td>\n",
       "      <td>39.649</td>\n",
       "      <td>54.2175</td>\n",
       "      <td>21.5935</td>\n",
       "      <td>108.3749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>1.4058</td>\n",
       "      <td>1.1511</td>\n",
       "      <td>0.6821</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>1.3633</td>\n",
       "      <td>1.1481</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>1.3334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1554034569220</td>\n",
       "      <td>13:16:09.220</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13869</td>\n",
       "      <td>33.400</td>\n",
       "      <td>36.6111</td>\n",
       "      <td>34.0357</td>\n",
       "      <td>119.4974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.9182</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>0.3038</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>0.6368</td>\n",
       "      <td>0.6733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1554034569985</td>\n",
       "      <td>13:16:09.985</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12336</td>\n",
       "      <td>53.202</td>\n",
       "      <td>15.5826</td>\n",
       "      <td>76.3712</td>\n",
       "      <td>129.6231</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1067</td>\n",
       "      <td>5.6749</td>\n",
       "      <td>1.3833</td>\n",
       "      <td>3.7632</td>\n",
       "      <td>4.2902</td>\n",
       "      <td>3.4170</td>\n",
       "      <td>2.1096</td>\n",
       "      <td>5.3670</td>\n",
       "      <td>2.9436</td>\n",
       "      <td>5.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1554034570413</td>\n",
       "      <td>13:16:10.413</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9037</td>\n",
       "      <td>78.870</td>\n",
       "      <td>53.4001</td>\n",
       "      <td>116.1074</td>\n",
       "      <td>411.5939</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9045</td>\n",
       "      <td>1.8140</td>\n",
       "      <td>1.5852</td>\n",
       "      <td>1.9278</td>\n",
       "      <td>1.9029</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>0.8157</td>\n",
       "      <td>0.7449</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>1.2878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time          date  training  step  up  slice      F_1       F_2  \\\n",
       "0   1554034538313  13:15:38.313         0     3   1  20749   20.056   94.1213   \n",
       "1   1554034538958  13:15:38.958         0     4   1  11691   42.667   47.5442   \n",
       "2   1554034539517  13:15:39.517         0     5   1  16388   14.943  104.5249   \n",
       "3   1554034540279  13:15:40.279         0     7   1   7260   67.612   65.4221   \n",
       "4   1554034540774  13:15:40.774         0     8   1  10254   51.408  151.3434   \n",
       "5   1554034541375  13:15:41.375         0     9   1  12306   43.714  197.7310   \n",
       "6   1554034542523  13:15:42.523         0    11   1  12275   36.233    4.3915   \n",
       "7   1554034542921  13:15:42.921         0    12   1  11018    8.626   32.5530   \n",
       "8   1554034543324  13:15:43.324         0    13   1  48631   32.854  117.6710   \n",
       "9   1554034543872  13:15:43.872         0    14   1  12990   95.478  328.9371   \n",
       "10  1554034552741  13:15:52.741         0     3   2   7295   51.179  110.7519   \n",
       "11  1554034553464  13:15:53.464         0     4   2  15670   91.319  297.7418   \n",
       "12  1554034554064  13:15:54.064         0     5   2   7187   89.670   25.7131   \n",
       "13  1554034555772  13:15:55.772         0     7   2  63054  176.856  283.4218   \n",
       "14  1554034556313  13:15:56.313         0     8   2   7979   92.689   82.8024   \n",
       "15  1554034556749  13:15:56.749         0     9   2   9095   60.448  116.9732   \n",
       "16  1554034557167  13:15:57.167         0    10   2  10473    5.745   86.2626   \n",
       "17  1554034557652  13:15:57.652         0    12   2   9539  105.918  222.3163   \n",
       "18  1554034558096  13:15:58.096         0    13   2   8128  113.464  202.6727   \n",
       "19  1554034558486  13:15:58.486         0    14   2   8780   72.946   35.3417   \n",
       "20  1554034568646  13:16:08.646         0     3   1   8535   39.649   54.2175   \n",
       "21  1554034569220  13:16:09.220         0     4   1  13869   33.400   36.6111   \n",
       "22  1554034569985  13:16:09.985         0     6   1  12336   53.202   15.5826   \n",
       "23  1554034570413  13:16:10.413         0     7   1   9037   78.870   53.4001   \n",
       "\n",
       "         F_3       F_4   ...       F_119    F_120    F_121    F_122    F_123  \\\n",
       "0    56.6893  244.1506   ...      0.9821   1.6455   1.2656   1.0433   1.3137   \n",
       "1    72.8170   77.0697   ...      1.0073   0.9375   0.8401   0.9308   0.8793   \n",
       "2   123.5440  325.2235   ...      0.4323   2.3395   1.9636   2.7001   1.6907   \n",
       "3    27.6171  155.5133   ...      0.7560   1.1747   0.5885   0.3979   0.7220   \n",
       "4   105.4404  376.3469   ...      0.4306   1.2238   1.7379   0.3559   0.4861   \n",
       "5   287.9238  458.5304   ...      0.1554   0.9977   0.3534   0.2905   0.0378   \n",
       "6    16.2003   67.3880   ...      1.1263   3.0966   0.7637   4.8209   2.8054   \n",
       "7   114.6319  209.5523   ...     12.8564  10.8709  12.2103   5.6484   4.5759   \n",
       "8   414.5323  808.1890   ...      0.7231   2.2230   0.1032   1.8384   0.9183   \n",
       "9   369.8189  122.3947   ...      0.1821   0.3310   0.1429   0.9825   0.4588   \n",
       "10  121.1264   48.1045   ...      1.1168   1.2650   1.0660   0.5436   1.8927   \n",
       "11  238.2316  187.9039   ...      6.0638   2.1898   5.2268   3.8690   1.8847   \n",
       "12  114.5191  124.2920   ...      0.6383   0.2362   1.0036   1.5005   1.8103   \n",
       "13  629.3238  135.3471   ...      1.4961   1.1693   1.2748   0.8044   0.6667   \n",
       "14  227.5237  131.2919   ...      2.2423   7.1212   8.3202   2.1716   5.9391   \n",
       "15   83.3200   21.3689   ...      2.5014   2.7870   2.7309   1.7733   2.5238   \n",
       "16  214.0896   92.3728   ...      7.3324   5.7336   7.9026   9.3345  12.5422   \n",
       "17  146.9655  107.1227   ...     23.9141  20.8094   6.7922  16.0169  20.7039   \n",
       "18  179.4488  142.2376   ...      4.3024   5.0787   5.5172   4.9711   3.8353   \n",
       "19  163.8728  139.9921   ...     19.7552   7.4400  12.9592   8.1229  18.1363   \n",
       "20   21.5935  108.3749   ...      0.8681   1.4058   1.1511   0.6821   0.3394   \n",
       "21   34.0357  119.4974   ...      0.1142   0.7304   0.9182   0.3173   0.3038   \n",
       "22   76.3712  129.6231   ...      3.1067   5.6749   1.3833   3.7632   4.2902   \n",
       "23  116.1074  411.5939   ...      1.9045   1.8140   1.5852   1.9278   1.9029   \n",
       "\n",
       "      F_124    F_125    F_126    F_127    F_128  \n",
       "0    2.4414   1.0232   1.2927   0.7879   1.7651  \n",
       "1    1.0529   1.0475   1.0450   1.2355   1.4466  \n",
       "2    1.3859   1.2406   1.8272   2.2453   1.9073  \n",
       "3    0.7708   0.3064   0.8781   0.7001   0.3874  \n",
       "4    1.4681   0.6975   1.1393   1.1507   0.8577  \n",
       "5    0.8668   1.0058   0.3264   0.7247   0.2743  \n",
       "6    2.8560   6.5081   4.1053   4.8375   6.3500  \n",
       "7    3.3081  11.8470  11.5552   9.8679  11.1101  \n",
       "8    1.7533   0.5601   1.9654   1.1408   0.8986  \n",
       "9    1.0916   0.6122   0.9901   0.4200   0.7634  \n",
       "10   0.9676   0.0917   1.9604   2.4926   2.0846  \n",
       "11   4.5757   2.6038   3.7672   3.9683   2.8586  \n",
       "12   1.6553   0.9874   0.3588   0.1287   0.5634  \n",
       "13   0.7229   1.1219   1.4850   0.7797   0.9767  \n",
       "14   6.3946   2.9856   4.9899   4.1904   3.5582  \n",
       "15   1.4445   3.5992   1.5622   1.9059   1.0923  \n",
       "16   9.1397  12.9683  10.5161  12.1184   8.5896  \n",
       "17  16.0992  14.9737  23.0030  20.7990  13.1253  \n",
       "18   3.0314   3.4189   3.2492   1.4511   0.9276  \n",
       "19   8.3398  11.5341  16.8605   8.9856  15.4814  \n",
       "20   1.3633   1.1481   0.7551   0.1447   1.3334  \n",
       "21   0.1145   0.1936   0.7005   0.6368   0.6733  \n",
       "22   3.4170   2.1096   5.3670   2.9436   5.0052  \n",
       "23   0.5600   0.8157   0.7449   0.7691   1.2878  \n",
       "\n",
       "[24 rows x 134 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MM= pandas.read_csv(csv_url)\n",
    "#Mdataset=shuffle(MM)\n",
    "Mdataset=MM\n",
    "#Mdataset=MM.sample(frac=1)\n",
    "Mdataset.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>F_5</th>\n",
       "      <th>F_6</th>\n",
       "      <th>F_7</th>\n",
       "      <th>F_8</th>\n",
       "      <th>F_9</th>\n",
       "      <th>F_10</th>\n",
       "      <th>...</th>\n",
       "      <th>F_119</th>\n",
       "      <th>F_120</th>\n",
       "      <th>F_121</th>\n",
       "      <th>F_122</th>\n",
       "      <th>F_123</th>\n",
       "      <th>F_124</th>\n",
       "      <th>F_125</th>\n",
       "      <th>F_126</th>\n",
       "      <th>F_127</th>\n",
       "      <th>F_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.056</td>\n",
       "      <td>94.1213</td>\n",
       "      <td>56.6893</td>\n",
       "      <td>244.1506</td>\n",
       "      <td>297.1014</td>\n",
       "      <td>68.4912</td>\n",
       "      <td>73.1389</td>\n",
       "      <td>86.4008</td>\n",
       "      <td>46.9100</td>\n",
       "      <td>33.9853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.6455</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>1.0433</td>\n",
       "      <td>1.3137</td>\n",
       "      <td>2.4414</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>1.2927</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>1.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.667</td>\n",
       "      <td>47.5442</td>\n",
       "      <td>72.8170</td>\n",
       "      <td>77.0697</td>\n",
       "      <td>35.0869</td>\n",
       "      <td>51.4889</td>\n",
       "      <td>103.3456</td>\n",
       "      <td>73.5672</td>\n",
       "      <td>6.5732</td>\n",
       "      <td>18.0737</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>1.2355</td>\n",
       "      <td>1.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.943</td>\n",
       "      <td>104.5249</td>\n",
       "      <td>123.5440</td>\n",
       "      <td>325.2235</td>\n",
       "      <td>216.0460</td>\n",
       "      <td>188.1778</td>\n",
       "      <td>216.8083</td>\n",
       "      <td>106.9825</td>\n",
       "      <td>14.6230</td>\n",
       "      <td>98.7340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>2.3395</td>\n",
       "      <td>1.9636</td>\n",
       "      <td>2.7001</td>\n",
       "      <td>1.6907</td>\n",
       "      <td>1.3859</td>\n",
       "      <td>1.2406</td>\n",
       "      <td>1.8272</td>\n",
       "      <td>2.2453</td>\n",
       "      <td>1.9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.612</td>\n",
       "      <td>65.4221</td>\n",
       "      <td>27.6171</td>\n",
       "      <td>155.5133</td>\n",
       "      <td>82.3221</td>\n",
       "      <td>36.4214</td>\n",
       "      <td>46.7972</td>\n",
       "      <td>11.0348</td>\n",
       "      <td>7.9684</td>\n",
       "      <td>6.5255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1.1747</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.3874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      F_1       F_2       F_3       F_4       F_5       F_6       F_7  \\\n",
       "0  20.056   94.1213   56.6893  244.1506  297.1014   68.4912   73.1389   \n",
       "1  42.667   47.5442   72.8170   77.0697   35.0869   51.4889  103.3456   \n",
       "2  14.943  104.5249  123.5440  325.2235  216.0460  188.1778  216.8083   \n",
       "3  67.612   65.4221   27.6171  155.5133   82.3221   36.4214   46.7972   \n",
       "\n",
       "        F_8      F_9     F_10   ...     F_119   F_120   F_121   F_122   F_123  \\\n",
       "0   86.4008  46.9100  33.9853   ...    0.9821  1.6455  1.2656  1.0433  1.3137   \n",
       "1   73.5672   6.5732  18.0737   ...    1.0073  0.9375  0.8401  0.9308  0.8793   \n",
       "2  106.9825  14.6230  98.7340   ...    0.4323  2.3395  1.9636  2.7001  1.6907   \n",
       "3   11.0348   7.9684   6.5255   ...    0.7560  1.1747  0.5885  0.3979  0.7220   \n",
       "\n",
       "    F_124   F_125   F_126   F_127   F_128  \n",
       "0  2.4414  1.0232  1.2927  0.7879  1.7651  \n",
       "1  1.0529  1.0475  1.0450  1.2355  1.4466  \n",
       "2  1.3859  1.2406  1.8272  2.2453  1.9073  \n",
       "3  0.7708  0.3064  0.8781  0.7001  0.3874  \n",
       "\n",
       "[4 rows x 128 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdataset=Mdataset.iloc[:,6:]\n",
    "Xdataset.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "5     1\n",
       "6     1\n",
       "7     1\n",
       "8     1\n",
       "9     1\n",
       "10    2\n",
       "11    2\n",
       "12    2\n",
       "13    2\n",
       "Name: up, dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ydataset = Mdataset['up']\n",
    "Ydataset.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05107705, 0.13282973, 0.0315936 , 0.11904173, 0.41328094,\n",
       "       0.13013288, 0.06080097, 0.17240561, 0.11593018, 0.13377925,\n",
       "       0.16578296, 0.22347617, 0.18588993, 0.24958625, 0.24999181,\n",
       "       0.21088881, 0.25110176, 0.23986342, 0.18482355, 0.17883252,\n",
       "       0.08571672, 0.04426925, 0.07225791, 0.04398994, 0.04239361,\n",
       "       0.09207876, 0.02792083, 0.05240673, 0.03630493, 0.03536118,\n",
       "       0.03906149, 0.05114374, 0.31190407, 0.07596091, 0.0062227 ,\n",
       "       0.11509733, 0.05813498, 0.06469943, 0.03883907, 0.06532665,\n",
       "       0.08104072, 0.15104461, 0.02481355, 0.04821453, 0.03800076,\n",
       "       0.06716089, 0.03636204, 0.04252229, 0.02039525, 0.05664509,\n",
       "       0.08343108, 0.0506808 , 0.05086898, 0.10339523, 0.06999565,\n",
       "       0.10077718, 0.07215901, 0.05073296, 0.03928588, 0.08224652,\n",
       "       0.10731719, 0.07810754, 0.09611584, 0.06576441, 0.04820583,\n",
       "       0.01855257, 0.01783409, 0.0669564 , 0.05401583, 0.02569278,\n",
       "       0.0042775 , 0.02914104, 0.00444942, 0.01840004, 0.03252842,\n",
       "       0.01247216, 0.02284181, 0.01525286, 0.01313781, 0.00801906,\n",
       "       0.01558545, 0.01095846, 0.01185073, 0.00987019, 0.01022947,\n",
       "       0.00759526, 0.00351516, 0.00380238, 0.01086448, 0.00781609,\n",
       "       0.01839407, 0.02454824, 0.01837983, 0.01915168, 0.01719985,\n",
       "       0.01444853, 0.07601717, 0.00664145, 0.00400803, 0.02043834,\n",
       "       0.01854563, 0.00908576, 0.00544105, 0.01766119, 0.02045896,\n",
       "       0.01995918, 0.00327982, 0.00412347, 0.01148738, 0.00868088,\n",
       "       0.00191083, 0.00615714, 0.01725313, 0.01454628, 0.01727621,\n",
       "       0.01270132, 0.00898884, 0.02366211, 0.01558594, 0.02758063,\n",
       "       0.01705962, 0.01649737, 0.02054278, 0.04137462, 0.01304682,\n",
       "       0.01925503, 0.01511903, 0.02898126])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(Xdataset)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329, 128, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.05107705])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset=dataset.reshape((dataset.shape[0],1,-1))\n",
    "dataset=dataset.reshape(dataset.shape+(1,))\n",
    "print(dataset.shape)\n",
    "dataset[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 109\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(trainX), len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_width = 48\n",
    "look_height=32\n",
    "from sklearn import preprocessing \n",
    "from sklearn import utils\n",
    "from io import StringIO\n",
    "\n",
    "#trainX, trainY = create_dataset(train, look_back)\n",
    "#testX, testY = create_dataset(test, look_back)\n",
    "le = preprocessing.LabelEncoder()\n",
    "YN=utils.column_or_1d(Ydataset, warn=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "YO=le.fit_transform(YN)\n",
    "YO\n",
    "len(le.classes_)\n",
    "Yclasses=len(le.classes_)\n",
    "print(Yclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 220 109\n"
     ]
    }
   ],
   "source": [
    "trainY = YO[0:train_size]\n",
    "testY=YO[train_size:len(YO)]\n",
    "print('data:',len(trainY), len(testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 128, 1)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "#trainX = numpy.reshape(trainX, (trainX.shape[0],  trainX.shape[2],2))\n",
    "#testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 128, 1)\n",
      "(220,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "#dataset.shape\n",
    "print(trainY.shape)\n",
    "#print('trainyo',trainYO.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_114 (Conv1D)          (None, 128, 32)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_115 (Conv1D)          (None, 128, 8)            264       \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 128, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_116 (Conv1D)          (None, 128, 128)          1152      \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 2)                 32770     \n",
      "=================================================================\n",
      "Total params: 34,250\n",
      "Trainable params: 34,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_length = trainX.shape[1]\n",
    "print(seq_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 1, activation='relu', input_shape=(seq_length,1)))\n",
    "model.add(MaxPooling1D(1))\n",
    "model.add(Conv1D(8, 1, activation='relu'))\n",
    "#model.add(MaxPooling1D(1))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dense(128))\n",
    "model.add(Conv1D(128,1 ))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Conv1D(128,1 ))\n",
    "#model.add(MaxPooling1D())\n",
    "#model.add(Conv1D(128, 1, activation='relu'))\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Yclasses, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170\n",
      "220/220 [==============================] - 1s 5ms/step - loss: 0.7964 - acc: 0.5091\n",
      "Epoch 2/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 2.6086 - acc: 0.4636\n",
      "Epoch 3/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.9247 - acc: 0.5364\n",
      "Epoch 4/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.6232 - acc: 0.6591\n",
      "Epoch 5/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.5464 - acc: 0.7182\n",
      "Epoch 6/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.4742 - acc: 0.8045\n",
      "Epoch 7/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.4849 - acc: 0.7500\n",
      "Epoch 8/170\n",
      "220/220 [==============================] - 0s 250us/step - loss: 0.3946 - acc: 0.8591\n",
      "Epoch 9/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.3782 - acc: 0.8409\n",
      "Epoch 10/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.4431 - acc: 0.8000\n",
      "Epoch 11/170\n",
      "220/220 [==============================] - 0s 271us/step - loss: 0.3555 - acc: 0.8455\n",
      "Epoch 12/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.2951 - acc: 0.8818\n",
      "Epoch 13/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.2996 - acc: 0.8682\n",
      "Epoch 14/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.2785 - acc: 0.8682\n",
      "Epoch 15/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.2664 - acc: 0.8864\n",
      "Epoch 16/170\n",
      "220/220 [==============================] - 0s 260us/step - loss: 0.3305 - acc: 0.8545\n",
      "Epoch 17/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.2909 - acc: 0.8773\n",
      "Epoch 18/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.2848 - acc: 0.8591\n",
      "Epoch 19/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.2378 - acc: 0.9091\n",
      "Epoch 20/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.2785 - acc: 0.8591\n",
      "Epoch 21/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.3195 - acc: 0.8682\n",
      "Epoch 22/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.2757 - acc: 0.8909\n",
      "Epoch 23/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.3994 - acc: 0.8545\n",
      "Epoch 24/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.2532 - acc: 0.9000\n",
      "Epoch 25/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.2946 - acc: 0.8500\n",
      "Epoch 26/170\n",
      "220/220 [==============================] - 0s 251us/step - loss: 0.4591 - acc: 0.8045\n",
      "Epoch 27/170\n",
      "220/220 [==============================] - 0s 261us/step - loss: 0.3852 - acc: 0.8318\n",
      "Epoch 28/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.3974 - acc: 0.8636\n",
      "Epoch 29/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.2843 - acc: 0.8682\n",
      "Epoch 30/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1999 - acc: 0.9136\n",
      "Epoch 31/170\n",
      "220/220 [==============================] - 0s 262us/step - loss: 0.1631 - acc: 0.9364\n",
      "Epoch 32/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.2469 - acc: 0.8818\n",
      "Epoch 33/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.2692 - acc: 0.9136\n",
      "Epoch 34/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1835 - acc: 0.9091\n",
      "Epoch 35/170\n",
      "220/220 [==============================] - 0s 250us/step - loss: 0.2100 - acc: 0.9182\n",
      "Epoch 36/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.2218 - acc: 0.9045\n",
      "Epoch 37/170\n",
      "220/220 [==============================] - 0s 265us/step - loss: 0.2241 - acc: 0.9091\n",
      "Epoch 38/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.1996 - acc: 0.9182\n",
      "Epoch 39/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1646 - acc: 0.9318\n",
      "Epoch 40/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.1660 - acc: 0.9409\n",
      "Epoch 41/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.2860 - acc: 0.8818\n",
      "Epoch 42/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.2010 - acc: 0.9273\n",
      "Epoch 43/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.1403 - acc: 0.9591\n",
      "Epoch 44/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1730 - acc: 0.9318\n",
      "Epoch 45/170\n",
      "220/220 [==============================] - 0s 260us/step - loss: 0.1784 - acc: 0.9409\n",
      "Epoch 46/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1764 - acc: 0.9091\n",
      "Epoch 47/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.1097 - acc: 0.9682\n",
      "Epoch 48/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.1964 - acc: 0.9227\n",
      "Epoch 49/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1314 - acc: 0.9409\n",
      "Epoch 50/170\n",
      "220/220 [==============================] - 0s 269us/step - loss: 0.1525 - acc: 0.9364\n",
      "Epoch 51/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.2506 - acc: 0.9000\n",
      "Epoch 52/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1217 - acc: 0.9455\n",
      "Epoch 53/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.1710 - acc: 0.9136\n",
      "Epoch 54/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0832 - acc: 0.9682\n",
      "Epoch 55/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.2269 - acc: 0.9318\n",
      "Epoch 56/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1420 - acc: 0.9591\n",
      "Epoch 57/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.0850 - acc: 0.9591\n",
      "Epoch 58/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.1169 - acc: 0.9545\n",
      "Epoch 59/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1942 - acc: 0.9273\n",
      "Epoch 60/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.1544 - acc: 0.9273\n",
      "Epoch 61/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1273 - acc: 0.9273\n",
      "Epoch 62/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.2488 - acc: 0.9045\n",
      "Epoch 63/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.2316 - acc: 0.9318\n",
      "Epoch 64/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.1049 - acc: 0.9636\n",
      "Epoch 65/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0662 - acc: 0.9864\n",
      "Epoch 66/170\n",
      "220/220 [==============================] - 0s 262us/step - loss: 0.0746 - acc: 0.9636\n",
      "Epoch 67/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.1782 - acc: 0.9273\n",
      "Epoch 68/170\n",
      "220/220 [==============================] - 0s 250us/step - loss: 0.1514 - acc: 0.9364\n",
      "Epoch 69/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.0723 - acc: 0.9818\n",
      "Epoch 70/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0851 - acc: 0.9682\n",
      "Epoch 71/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.2164 - acc: 0.9273\n",
      "Epoch 72/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1291 - acc: 0.9364\n",
      "Epoch 73/170\n",
      "220/220 [==============================] - 0s 262us/step - loss: 0.1562 - acc: 0.9455\n",
      "Epoch 74/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.1228 - acc: 0.9545\n",
      "Epoch 75/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0929 - acc: 0.9636\n",
      "Epoch 76/170\n",
      "220/220 [==============================] - 0s 261us/step - loss: 0.1218 - acc: 0.9500\n",
      "Epoch 77/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0880 - acc: 0.9591\n",
      "Epoch 78/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.0649 - acc: 0.9727\n",
      "Epoch 79/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0883 - acc: 0.9636\n",
      "Epoch 80/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.0569 - acc: 0.9727\n",
      "Epoch 81/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.1195 - acc: 0.9500\n",
      "Epoch 82/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.0453 - acc: 0.9864\n",
      "Epoch 83/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 255us/step - loss: 0.1179 - acc: 0.9500\n",
      "Epoch 84/170\n",
      "220/220 [==============================] - 0s 261us/step - loss: 0.0492 - acc: 0.9864\n",
      "Epoch 85/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.0377 - acc: 0.9818\n",
      "Epoch 86/170\n",
      "220/220 [==============================] - 0s 263us/step - loss: 0.1753 - acc: 0.9364\n",
      "Epoch 87/170\n",
      "220/220 [==============================] - 0s 251us/step - loss: 0.1553 - acc: 0.9636\n",
      "Epoch 88/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.1256 - acc: 0.9500\n",
      "Epoch 89/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.1371 - acc: 0.9455\n",
      "Epoch 90/170\n",
      "220/220 [==============================] - 0s 260us/step - loss: 0.1459 - acc: 0.9545\n",
      "Epoch 91/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.1432 - acc: 0.9591\n",
      "Epoch 92/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.1794 - acc: 0.9364\n",
      "Epoch 93/170\n",
      "220/220 [==============================] - 0s 249us/step - loss: 0.2814 - acc: 0.9091\n",
      "Epoch 94/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0896 - acc: 0.9636\n",
      "Epoch 95/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1304 - acc: 0.9545\n",
      "Epoch 96/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0266 - acc: 0.9909\n",
      "Epoch 97/170\n",
      "220/220 [==============================] - 0s 265us/step - loss: 0.0315 - acc: 0.9955\n",
      "Epoch 98/170\n",
      "220/220 [==============================] - 0s 250us/step - loss: 0.0315 - acc: 0.9955\n",
      "Epoch 99/170\n",
      "220/220 [==============================] - 0s 260us/step - loss: 0.0739 - acc: 0.9727\n",
      "Epoch 100/170\n",
      "220/220 [==============================] - 0s 260us/step - loss: 0.1495 - acc: 0.9455\n",
      "Epoch 101/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.0648 - acc: 0.9818\n",
      "Epoch 102/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.1187 - acc: 0.9455\n",
      "Epoch 103/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0812 - acc: 0.9727\n",
      "Epoch 104/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.0725 - acc: 0.9682\n",
      "Epoch 105/170\n",
      "220/220 [==============================] - 0s 251us/step - loss: 0.0860 - acc: 0.9864\n",
      "Epoch 106/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.0657 - acc: 0.9773\n",
      "Epoch 107/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.0439 - acc: 0.9773\n",
      "Epoch 108/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.0647 - acc: 0.9864\n",
      "Epoch 109/170\n",
      "220/220 [==============================] - 0s 263us/step - loss: 0.0885 - acc: 0.9818\n",
      "Epoch 110/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.0420 - acc: 0.9864\n",
      "Epoch 111/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.0909 - acc: 0.9636\n",
      "Epoch 112/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.1092 - acc: 0.9591\n",
      "Epoch 113/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.0604 - acc: 0.9818\n",
      "Epoch 114/170\n",
      "220/220 [==============================] - 0s 248us/step - loss: 0.1204 - acc: 0.9591\n",
      "Epoch 115/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.1636 - acc: 0.9364\n",
      "Epoch 116/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0547 - acc: 0.9727\n",
      "Epoch 117/170\n",
      "220/220 [==============================] - 0s 263us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 118/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1016 - acc: 0.9773\n",
      "Epoch 119/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.1046 - acc: 0.9591\n",
      "Epoch 120/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.1027 - acc: 0.9682\n",
      "Epoch 121/170\n",
      "220/220 [==============================] - 0s 251us/step - loss: 0.0886 - acc: 0.9591\n",
      "Epoch 122/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0614 - acc: 0.9818\n",
      "Epoch 123/170\n",
      "220/220 [==============================] - 0s 248us/step - loss: 0.0626 - acc: 0.9727\n",
      "Epoch 124/170\n",
      "220/220 [==============================] - 0s 260us/step - loss: 0.0405 - acc: 0.9818\n",
      "Epoch 125/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0817 - acc: 0.9818\n",
      "Epoch 126/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0785 - acc: 0.9682\n",
      "Epoch 127/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0842 - acc: 0.9682\n",
      "Epoch 128/170\n",
      "220/220 [==============================] - 0s 251us/step - loss: 0.0950 - acc: 0.9818\n",
      "Epoch 129/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.0917 - acc: 0.9727\n",
      "Epoch 130/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0532 - acc: 0.9909\n",
      "Epoch 131/170\n",
      "220/220 [==============================] - 0s 262us/step - loss: 0.1129 - acc: 0.9682\n",
      "Epoch 132/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1259 - acc: 0.9545\n",
      "Epoch 133/170\n",
      "220/220 [==============================] - 0s 251us/step - loss: 0.0957 - acc: 0.9773\n",
      "Epoch 134/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1232 - acc: 0.9636\n",
      "Epoch 135/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.0964 - acc: 0.9682\n",
      "Epoch 136/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.1257 - acc: 0.9455\n",
      "Epoch 137/170\n",
      "220/220 [==============================] - 0s 262us/step - loss: 0.1175 - acc: 0.9545\n",
      "Epoch 138/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.0473 - acc: 0.9864\n",
      "Epoch 139/170\n",
      "220/220 [==============================] - 0s 250us/step - loss: 0.0356 - acc: 0.9864\n",
      "Epoch 140/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.1125 - acc: 0.9545\n",
      "Epoch 141/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.2364 - acc: 0.9500\n",
      "Epoch 142/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0591 - acc: 0.9818\n",
      "Epoch 143/170\n",
      "220/220 [==============================] - 0s 260us/step - loss: 0.0315 - acc: 0.9818\n",
      "Epoch 144/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0662 - acc: 0.9773\n",
      "Epoch 145/170\n",
      "220/220 [==============================] - 0s 263us/step - loss: 0.0223 - acc: 0.9909\n",
      "Epoch 146/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.0610 - acc: 0.9773\n",
      "Epoch 147/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0590 - acc: 0.9864\n",
      "Epoch 148/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.0128 - acc: 0.9955\n",
      "Epoch 149/170\n",
      "220/220 [==============================] - 0s 254us/step - loss: 0.0519 - acc: 0.9773\n",
      "Epoch 150/170\n",
      "220/220 [==============================] - 0s 250us/step - loss: 0.0367 - acc: 0.9909\n",
      "Epoch 151/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0677 - acc: 0.9682\n",
      "Epoch 152/170\n",
      "220/220 [==============================] - 0s 259us/step - loss: 0.0603 - acc: 0.9727\n",
      "Epoch 153/170\n",
      "220/220 [==============================] - 0s 261us/step - loss: 0.0699 - acc: 0.9682\n",
      "Epoch 154/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0630 - acc: 0.9818\n",
      "Epoch 155/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.0508 - acc: 0.9909\n",
      "Epoch 156/170\n",
      "220/220 [==============================] - 0s 255us/step - loss: 0.0778 - acc: 0.9864\n",
      "Epoch 157/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.0291 - acc: 0.9818\n",
      "Epoch 158/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 159/170\n",
      "220/220 [==============================] - 0s 258us/step - loss: 0.0233 - acc: 0.9909\n",
      "Epoch 160/170\n",
      "220/220 [==============================] - 0s 257us/step - loss: 0.1222 - acc: 0.9636\n",
      "Epoch 161/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.1648 - acc: 0.9545\n",
      "Epoch 162/170\n",
      "220/220 [==============================] - 0s 256us/step - loss: 0.0944 - acc: 0.9682\n",
      "Epoch 163/170\n",
      "220/220 [==============================] - 0s 253us/step - loss: 0.0216 - acc: 0.9909\n",
      "Epoch 164/170\n",
      "220/220 [==============================] - 0s 245us/step - loss: 0.0822 - acc: 0.9818\n",
      "Epoch 165/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 244us/step - loss: 0.0492 - acc: 0.9818\n",
      "Epoch 166/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.1689 - acc: 0.9545\n",
      "Epoch 167/170\n",
      "220/220 [==============================] - 0s 247us/step - loss: 0.0888 - acc: 0.9727\n",
      "Epoch 168/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0425 - acc: 0.9864\n",
      "Epoch 169/170\n",
      "220/220 [==============================] - 0s 252us/step - loss: 0.0615 - acc: 0.9773\n",
      "Epoch 170/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.0301 - acc: 0.9909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f337a389c88>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainYO = to_categorical(trainY, num_classes=Yclasses)\n",
    "testYO = to_categorical(testY, num_classes=Yclasses)\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model.fit(trainX, trainYO, epochs=170)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 4ms/step\n",
      "[2.0378908192345855, 0.7247706435689139]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testYO)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 86us/step\n",
      "Test loss: 0.010272241053594785\n",
      "Test accuracy: 0.9954545454545455\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(trainX, trainYO, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7247706435689139\n",
      "0.9954545454545455\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Keras reported accuracy:\n",
    "score = model.evaluate(testX,testYO, verbose=0) \n",
    "print(score[1])\n",
    "# 0.98580000000000001\n",
    "\n",
    "# Actual accuracy calculated manually:\n",
    "predY= model.predict(trainX)\n",
    "acc = sum([numpy.argmax(trainYO[i])==numpy.argmax(predY[i]) for i in range(len(trainYO))])/len(trainYO)\n",
    "print(acc)\n",
    "# 0.98580000000000001\n",
    "\n",
    "print(score[1]==acc)\n",
    "# True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict1 = model.predict(trainX)\n",
    "testPredict1 = model.predict(testX)\n",
    "#trainPredict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax(trainPredict)\n",
    "trainPredict=numpy.argmax(trainPredict1,1)\n",
    "testPredict=numpy.argmax(testPredict1,1)\n",
    "diffY=trainPredict-trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(diffY)\n",
    "plt.plot(trainY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# invert predictions\n",
    "#trainPredict = scaler.inverse_transform(trainPredict)\n",
    "#trainY = scaler.inverse_transform([trainY])\n",
    "#testPredict = scaler.inverse_transform(testPredict)\n",
    "#testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.117 RMSE\n",
      "Test Score: 0.488 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "print('Train Score: %.3f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY, testPredict))\n",
    "print('Test Score: %.3f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-62-0860eeb65c4e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-0860eeb65c4e>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    trainPredictPlot.reshape(,,-1)\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot.reshape(,,-1)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "print(trainPredictPlot.shape)\n",
    "trainPredictPlot[0:len(trainPredict),0] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(dataset)-len(testPredict):len(dataset), 0] = testPredict\n",
    "# plot baseline and predictions\n",
    "#plt.plot(scaler.inverse_transform(dataset),label=\"set\")\n",
    "plt.plot(trainPredictPlot,label=\"train\")\n",
    "plt.plot(testPredictPlot,label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (387) into shape (387,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-2d6065407b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwws\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainPredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mwws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# shift test predictions for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (387) into shape (387,1)"
     ]
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "wws=2;\n",
    "trainPredictPlot[wws:len(trainPredict)+wws,0] = trainPredict\n",
    "wws=60\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+wws+1:len(dataset)-look_back-1, 0] = testPredict\n",
    "# plot baseline and predictions\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(trainY,label=\"set\")\n",
    "ax1.plot(trainPredictPlot,label=\"train\")\n",
    "ax2.plot(testPredictPlot,label=\"testm\")\n",
    "#plt.figure(figsize=(15,3))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
