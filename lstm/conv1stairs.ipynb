{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-997ae06eb476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Conv2D,Flatten\n",
    "from keras.backend import argmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "csv_url='mrec20190331sfft.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>training</th>\n",
       "      <th>step</th>\n",
       "      <th>up</th>\n",
       "      <th>slice</th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>...</th>\n",
       "      <th>F_119</th>\n",
       "      <th>F_120</th>\n",
       "      <th>F_121</th>\n",
       "      <th>F_122</th>\n",
       "      <th>F_123</th>\n",
       "      <th>F_124</th>\n",
       "      <th>F_125</th>\n",
       "      <th>F_126</th>\n",
       "      <th>F_127</th>\n",
       "      <th>F_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1554034538313</td>\n",
       "      <td>13:15:38.313</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20749</td>\n",
       "      <td>20.056</td>\n",
       "      <td>94.1213</td>\n",
       "      <td>56.6893</td>\n",
       "      <td>244.1506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.6455</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>1.0433</td>\n",
       "      <td>1.3137</td>\n",
       "      <td>2.4414</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>1.2927</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>1.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1554034538958</td>\n",
       "      <td>13:15:38.958</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11691</td>\n",
       "      <td>42.667</td>\n",
       "      <td>47.5442</td>\n",
       "      <td>72.8170</td>\n",
       "      <td>77.0697</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>1.2355</td>\n",
       "      <td>1.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1554034539517</td>\n",
       "      <td>13:15:39.517</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>16388</td>\n",
       "      <td>14.943</td>\n",
       "      <td>104.5249</td>\n",
       "      <td>123.5440</td>\n",
       "      <td>325.2235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>2.3395</td>\n",
       "      <td>1.9636</td>\n",
       "      <td>2.7001</td>\n",
       "      <td>1.6907</td>\n",
       "      <td>1.3859</td>\n",
       "      <td>1.2406</td>\n",
       "      <td>1.8272</td>\n",
       "      <td>2.2453</td>\n",
       "      <td>1.9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1554034540279</td>\n",
       "      <td>13:15:40.279</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7260</td>\n",
       "      <td>67.612</td>\n",
       "      <td>65.4221</td>\n",
       "      <td>27.6171</td>\n",
       "      <td>155.5133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1.1747</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.3874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1554034540774</td>\n",
       "      <td>13:15:40.774</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>10254</td>\n",
       "      <td>51.408</td>\n",
       "      <td>151.3434</td>\n",
       "      <td>105.4404</td>\n",
       "      <td>376.3469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4306</td>\n",
       "      <td>1.2238</td>\n",
       "      <td>1.7379</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>1.4681</td>\n",
       "      <td>0.6975</td>\n",
       "      <td>1.1393</td>\n",
       "      <td>1.1507</td>\n",
       "      <td>0.8577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1554034541375</td>\n",
       "      <td>13:15:41.375</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12306</td>\n",
       "      <td>43.714</td>\n",
       "      <td>197.7310</td>\n",
       "      <td>287.9238</td>\n",
       "      <td>458.5304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.2905</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.8668</td>\n",
       "      <td>1.0058</td>\n",
       "      <td>0.3264</td>\n",
       "      <td>0.7247</td>\n",
       "      <td>0.2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1554034542523</td>\n",
       "      <td>13:15:42.523</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>12275</td>\n",
       "      <td>36.233</td>\n",
       "      <td>4.3915</td>\n",
       "      <td>16.2003</td>\n",
       "      <td>67.3880</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1263</td>\n",
       "      <td>3.0966</td>\n",
       "      <td>0.7637</td>\n",
       "      <td>4.8209</td>\n",
       "      <td>2.8054</td>\n",
       "      <td>2.8560</td>\n",
       "      <td>6.5081</td>\n",
       "      <td>4.1053</td>\n",
       "      <td>4.8375</td>\n",
       "      <td>6.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1554034542921</td>\n",
       "      <td>13:15:42.921</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>11018</td>\n",
       "      <td>8.626</td>\n",
       "      <td>32.5530</td>\n",
       "      <td>114.6319</td>\n",
       "      <td>209.5523</td>\n",
       "      <td>...</td>\n",
       "      <td>12.8564</td>\n",
       "      <td>10.8709</td>\n",
       "      <td>12.2103</td>\n",
       "      <td>5.6484</td>\n",
       "      <td>4.5759</td>\n",
       "      <td>3.3081</td>\n",
       "      <td>11.8470</td>\n",
       "      <td>11.5552</td>\n",
       "      <td>9.8679</td>\n",
       "      <td>11.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1554034543324</td>\n",
       "      <td>13:15:43.324</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>48631</td>\n",
       "      <td>32.854</td>\n",
       "      <td>117.6710</td>\n",
       "      <td>414.5323</td>\n",
       "      <td>808.1890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7231</td>\n",
       "      <td>2.2230</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>1.8384</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>1.7533</td>\n",
       "      <td>0.5601</td>\n",
       "      <td>1.9654</td>\n",
       "      <td>1.1408</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1554034543872</td>\n",
       "      <td>13:15:43.872</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12990</td>\n",
       "      <td>95.478</td>\n",
       "      <td>328.9371</td>\n",
       "      <td>369.8189</td>\n",
       "      <td>122.3947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1821</td>\n",
       "      <td>0.3310</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>1.0916</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.7634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1554034552741</td>\n",
       "      <td>13:15:52.741</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7295</td>\n",
       "      <td>51.179</td>\n",
       "      <td>110.7519</td>\n",
       "      <td>121.1264</td>\n",
       "      <td>48.1045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1168</td>\n",
       "      <td>1.2650</td>\n",
       "      <td>1.0660</td>\n",
       "      <td>0.5436</td>\n",
       "      <td>1.8927</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.0917</td>\n",
       "      <td>1.9604</td>\n",
       "      <td>2.4926</td>\n",
       "      <td>2.0846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1554034553464</td>\n",
       "      <td>13:15:53.464</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15670</td>\n",
       "      <td>91.319</td>\n",
       "      <td>297.7418</td>\n",
       "      <td>238.2316</td>\n",
       "      <td>187.9039</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0638</td>\n",
       "      <td>2.1898</td>\n",
       "      <td>5.2268</td>\n",
       "      <td>3.8690</td>\n",
       "      <td>1.8847</td>\n",
       "      <td>4.5757</td>\n",
       "      <td>2.6038</td>\n",
       "      <td>3.7672</td>\n",
       "      <td>3.9683</td>\n",
       "      <td>2.8586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1554034554064</td>\n",
       "      <td>13:15:54.064</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7187</td>\n",
       "      <td>89.670</td>\n",
       "      <td>25.7131</td>\n",
       "      <td>114.5191</td>\n",
       "      <td>124.2920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6383</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>1.0036</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>1.6553</td>\n",
       "      <td>0.9874</td>\n",
       "      <td>0.3588</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.5634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1554034555772</td>\n",
       "      <td>13:15:55.772</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>63054</td>\n",
       "      <td>176.856</td>\n",
       "      <td>283.4218</td>\n",
       "      <td>629.3238</td>\n",
       "      <td>135.3471</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4961</td>\n",
       "      <td>1.1693</td>\n",
       "      <td>1.2748</td>\n",
       "      <td>0.8044</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.7229</td>\n",
       "      <td>1.1219</td>\n",
       "      <td>1.4850</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.9767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1554034556313</td>\n",
       "      <td>13:15:56.313</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7979</td>\n",
       "      <td>92.689</td>\n",
       "      <td>82.8024</td>\n",
       "      <td>227.5237</td>\n",
       "      <td>131.2919</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2423</td>\n",
       "      <td>7.1212</td>\n",
       "      <td>8.3202</td>\n",
       "      <td>2.1716</td>\n",
       "      <td>5.9391</td>\n",
       "      <td>6.3946</td>\n",
       "      <td>2.9856</td>\n",
       "      <td>4.9899</td>\n",
       "      <td>4.1904</td>\n",
       "      <td>3.5582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1554034556749</td>\n",
       "      <td>13:15:56.749</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9095</td>\n",
       "      <td>60.448</td>\n",
       "      <td>116.9732</td>\n",
       "      <td>83.3200</td>\n",
       "      <td>21.3689</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5014</td>\n",
       "      <td>2.7870</td>\n",
       "      <td>2.7309</td>\n",
       "      <td>1.7733</td>\n",
       "      <td>2.5238</td>\n",
       "      <td>1.4445</td>\n",
       "      <td>3.5992</td>\n",
       "      <td>1.5622</td>\n",
       "      <td>1.9059</td>\n",
       "      <td>1.0923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1554034557167</td>\n",
       "      <td>13:15:57.167</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10473</td>\n",
       "      <td>5.745</td>\n",
       "      <td>86.2626</td>\n",
       "      <td>214.0896</td>\n",
       "      <td>92.3728</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3324</td>\n",
       "      <td>5.7336</td>\n",
       "      <td>7.9026</td>\n",
       "      <td>9.3345</td>\n",
       "      <td>12.5422</td>\n",
       "      <td>9.1397</td>\n",
       "      <td>12.9683</td>\n",
       "      <td>10.5161</td>\n",
       "      <td>12.1184</td>\n",
       "      <td>8.5896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1554034557652</td>\n",
       "      <td>13:15:57.652</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9539</td>\n",
       "      <td>105.918</td>\n",
       "      <td>222.3163</td>\n",
       "      <td>146.9655</td>\n",
       "      <td>107.1227</td>\n",
       "      <td>...</td>\n",
       "      <td>23.9141</td>\n",
       "      <td>20.8094</td>\n",
       "      <td>6.7922</td>\n",
       "      <td>16.0169</td>\n",
       "      <td>20.7039</td>\n",
       "      <td>16.0992</td>\n",
       "      <td>14.9737</td>\n",
       "      <td>23.0030</td>\n",
       "      <td>20.7990</td>\n",
       "      <td>13.1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1554034558096</td>\n",
       "      <td>13:15:58.096</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>8128</td>\n",
       "      <td>113.464</td>\n",
       "      <td>202.6727</td>\n",
       "      <td>179.4488</td>\n",
       "      <td>142.2376</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3024</td>\n",
       "      <td>5.0787</td>\n",
       "      <td>5.5172</td>\n",
       "      <td>4.9711</td>\n",
       "      <td>3.8353</td>\n",
       "      <td>3.0314</td>\n",
       "      <td>3.4189</td>\n",
       "      <td>3.2492</td>\n",
       "      <td>1.4511</td>\n",
       "      <td>0.9276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1554034558486</td>\n",
       "      <td>13:15:58.486</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>8780</td>\n",
       "      <td>72.946</td>\n",
       "      <td>35.3417</td>\n",
       "      <td>163.8728</td>\n",
       "      <td>139.9921</td>\n",
       "      <td>...</td>\n",
       "      <td>19.7552</td>\n",
       "      <td>7.4400</td>\n",
       "      <td>12.9592</td>\n",
       "      <td>8.1229</td>\n",
       "      <td>18.1363</td>\n",
       "      <td>8.3398</td>\n",
       "      <td>11.5341</td>\n",
       "      <td>16.8605</td>\n",
       "      <td>8.9856</td>\n",
       "      <td>15.4814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1554034568646</td>\n",
       "      <td>13:16:08.646</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8535</td>\n",
       "      <td>39.649</td>\n",
       "      <td>54.2175</td>\n",
       "      <td>21.5935</td>\n",
       "      <td>108.3749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>1.4058</td>\n",
       "      <td>1.1511</td>\n",
       "      <td>0.6821</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>1.3633</td>\n",
       "      <td>1.1481</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>1.3334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1554034569220</td>\n",
       "      <td>13:16:09.220</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13869</td>\n",
       "      <td>33.400</td>\n",
       "      <td>36.6111</td>\n",
       "      <td>34.0357</td>\n",
       "      <td>119.4974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.9182</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>0.3038</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>0.6368</td>\n",
       "      <td>0.6733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1554034569985</td>\n",
       "      <td>13:16:09.985</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12336</td>\n",
       "      <td>53.202</td>\n",
       "      <td>15.5826</td>\n",
       "      <td>76.3712</td>\n",
       "      <td>129.6231</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1067</td>\n",
       "      <td>5.6749</td>\n",
       "      <td>1.3833</td>\n",
       "      <td>3.7632</td>\n",
       "      <td>4.2902</td>\n",
       "      <td>3.4170</td>\n",
       "      <td>2.1096</td>\n",
       "      <td>5.3670</td>\n",
       "      <td>2.9436</td>\n",
       "      <td>5.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1554034570413</td>\n",
       "      <td>13:16:10.413</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9037</td>\n",
       "      <td>78.870</td>\n",
       "      <td>53.4001</td>\n",
       "      <td>116.1074</td>\n",
       "      <td>411.5939</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9045</td>\n",
       "      <td>1.8140</td>\n",
       "      <td>1.5852</td>\n",
       "      <td>1.9278</td>\n",
       "      <td>1.9029</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>0.8157</td>\n",
       "      <td>0.7449</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>1.2878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time          date  training  step  up  slice      F_1       F_2  \\\n",
       "0   1554034538313  13:15:38.313         0     3   1  20749   20.056   94.1213   \n",
       "1   1554034538958  13:15:38.958         0     4   1  11691   42.667   47.5442   \n",
       "2   1554034539517  13:15:39.517         0     5   1  16388   14.943  104.5249   \n",
       "3   1554034540279  13:15:40.279         0     7   1   7260   67.612   65.4221   \n",
       "4   1554034540774  13:15:40.774         0     8   1  10254   51.408  151.3434   \n",
       "5   1554034541375  13:15:41.375         0     9   1  12306   43.714  197.7310   \n",
       "6   1554034542523  13:15:42.523         0    11   1  12275   36.233    4.3915   \n",
       "7   1554034542921  13:15:42.921         0    12   1  11018    8.626   32.5530   \n",
       "8   1554034543324  13:15:43.324         0    13   1  48631   32.854  117.6710   \n",
       "9   1554034543872  13:15:43.872         0    14   1  12990   95.478  328.9371   \n",
       "10  1554034552741  13:15:52.741         0     3   2   7295   51.179  110.7519   \n",
       "11  1554034553464  13:15:53.464         0     4   2  15670   91.319  297.7418   \n",
       "12  1554034554064  13:15:54.064         0     5   2   7187   89.670   25.7131   \n",
       "13  1554034555772  13:15:55.772         0     7   2  63054  176.856  283.4218   \n",
       "14  1554034556313  13:15:56.313         0     8   2   7979   92.689   82.8024   \n",
       "15  1554034556749  13:15:56.749         0     9   2   9095   60.448  116.9732   \n",
       "16  1554034557167  13:15:57.167         0    10   2  10473    5.745   86.2626   \n",
       "17  1554034557652  13:15:57.652         0    12   2   9539  105.918  222.3163   \n",
       "18  1554034558096  13:15:58.096         0    13   2   8128  113.464  202.6727   \n",
       "19  1554034558486  13:15:58.486         0    14   2   8780   72.946   35.3417   \n",
       "20  1554034568646  13:16:08.646         0     3   1   8535   39.649   54.2175   \n",
       "21  1554034569220  13:16:09.220         0     4   1  13869   33.400   36.6111   \n",
       "22  1554034569985  13:16:09.985         0     6   1  12336   53.202   15.5826   \n",
       "23  1554034570413  13:16:10.413         0     7   1   9037   78.870   53.4001   \n",
       "\n",
       "         F_3       F_4   ...       F_119    F_120    F_121    F_122    F_123  \\\n",
       "0    56.6893  244.1506   ...      0.9821   1.6455   1.2656   1.0433   1.3137   \n",
       "1    72.8170   77.0697   ...      1.0073   0.9375   0.8401   0.9308   0.8793   \n",
       "2   123.5440  325.2235   ...      0.4323   2.3395   1.9636   2.7001   1.6907   \n",
       "3    27.6171  155.5133   ...      0.7560   1.1747   0.5885   0.3979   0.7220   \n",
       "4   105.4404  376.3469   ...      0.4306   1.2238   1.7379   0.3559   0.4861   \n",
       "5   287.9238  458.5304   ...      0.1554   0.9977   0.3534   0.2905   0.0378   \n",
       "6    16.2003   67.3880   ...      1.1263   3.0966   0.7637   4.8209   2.8054   \n",
       "7   114.6319  209.5523   ...     12.8564  10.8709  12.2103   5.6484   4.5759   \n",
       "8   414.5323  808.1890   ...      0.7231   2.2230   0.1032   1.8384   0.9183   \n",
       "9   369.8189  122.3947   ...      0.1821   0.3310   0.1429   0.9825   0.4588   \n",
       "10  121.1264   48.1045   ...      1.1168   1.2650   1.0660   0.5436   1.8927   \n",
       "11  238.2316  187.9039   ...      6.0638   2.1898   5.2268   3.8690   1.8847   \n",
       "12  114.5191  124.2920   ...      0.6383   0.2362   1.0036   1.5005   1.8103   \n",
       "13  629.3238  135.3471   ...      1.4961   1.1693   1.2748   0.8044   0.6667   \n",
       "14  227.5237  131.2919   ...      2.2423   7.1212   8.3202   2.1716   5.9391   \n",
       "15   83.3200   21.3689   ...      2.5014   2.7870   2.7309   1.7733   2.5238   \n",
       "16  214.0896   92.3728   ...      7.3324   5.7336   7.9026   9.3345  12.5422   \n",
       "17  146.9655  107.1227   ...     23.9141  20.8094   6.7922  16.0169  20.7039   \n",
       "18  179.4488  142.2376   ...      4.3024   5.0787   5.5172   4.9711   3.8353   \n",
       "19  163.8728  139.9921   ...     19.7552   7.4400  12.9592   8.1229  18.1363   \n",
       "20   21.5935  108.3749   ...      0.8681   1.4058   1.1511   0.6821   0.3394   \n",
       "21   34.0357  119.4974   ...      0.1142   0.7304   0.9182   0.3173   0.3038   \n",
       "22   76.3712  129.6231   ...      3.1067   5.6749   1.3833   3.7632   4.2902   \n",
       "23  116.1074  411.5939   ...      1.9045   1.8140   1.5852   1.9278   1.9029   \n",
       "\n",
       "      F_124    F_125    F_126    F_127    F_128  \n",
       "0    2.4414   1.0232   1.2927   0.7879   1.7651  \n",
       "1    1.0529   1.0475   1.0450   1.2355   1.4466  \n",
       "2    1.3859   1.2406   1.8272   2.2453   1.9073  \n",
       "3    0.7708   0.3064   0.8781   0.7001   0.3874  \n",
       "4    1.4681   0.6975   1.1393   1.1507   0.8577  \n",
       "5    0.8668   1.0058   0.3264   0.7247   0.2743  \n",
       "6    2.8560   6.5081   4.1053   4.8375   6.3500  \n",
       "7    3.3081  11.8470  11.5552   9.8679  11.1101  \n",
       "8    1.7533   0.5601   1.9654   1.1408   0.8986  \n",
       "9    1.0916   0.6122   0.9901   0.4200   0.7634  \n",
       "10   0.9676   0.0917   1.9604   2.4926   2.0846  \n",
       "11   4.5757   2.6038   3.7672   3.9683   2.8586  \n",
       "12   1.6553   0.9874   0.3588   0.1287   0.5634  \n",
       "13   0.7229   1.1219   1.4850   0.7797   0.9767  \n",
       "14   6.3946   2.9856   4.9899   4.1904   3.5582  \n",
       "15   1.4445   3.5992   1.5622   1.9059   1.0923  \n",
       "16   9.1397  12.9683  10.5161  12.1184   8.5896  \n",
       "17  16.0992  14.9737  23.0030  20.7990  13.1253  \n",
       "18   3.0314   3.4189   3.2492   1.4511   0.9276  \n",
       "19   8.3398  11.5341  16.8605   8.9856  15.4814  \n",
       "20   1.3633   1.1481   0.7551   0.1447   1.3334  \n",
       "21   0.1145   0.1936   0.7005   0.6368   0.6733  \n",
       "22   3.4170   2.1096   5.3670   2.9436   5.0052  \n",
       "23   0.5600   0.8157   0.7449   0.7691   1.2878  \n",
       "\n",
       "[24 rows x 134 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MM= pandas.read_csv(csv_url)\n",
    "#Mdataset=shuffle(MM)\n",
    "Mdataset=MM\n",
    "#Mdataset=MM.sample(frac=1)\n",
    "Mdataset.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>F_5</th>\n",
       "      <th>F_6</th>\n",
       "      <th>F_7</th>\n",
       "      <th>F_8</th>\n",
       "      <th>F_9</th>\n",
       "      <th>F_10</th>\n",
       "      <th>...</th>\n",
       "      <th>F_119</th>\n",
       "      <th>F_120</th>\n",
       "      <th>F_121</th>\n",
       "      <th>F_122</th>\n",
       "      <th>F_123</th>\n",
       "      <th>F_124</th>\n",
       "      <th>F_125</th>\n",
       "      <th>F_126</th>\n",
       "      <th>F_127</th>\n",
       "      <th>F_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.056</td>\n",
       "      <td>94.1213</td>\n",
       "      <td>56.6893</td>\n",
       "      <td>244.1506</td>\n",
       "      <td>297.1014</td>\n",
       "      <td>68.4912</td>\n",
       "      <td>73.1389</td>\n",
       "      <td>86.4008</td>\n",
       "      <td>46.9100</td>\n",
       "      <td>33.9853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.6455</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>1.0433</td>\n",
       "      <td>1.3137</td>\n",
       "      <td>2.4414</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>1.2927</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>1.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.667</td>\n",
       "      <td>47.5442</td>\n",
       "      <td>72.8170</td>\n",
       "      <td>77.0697</td>\n",
       "      <td>35.0869</td>\n",
       "      <td>51.4889</td>\n",
       "      <td>103.3456</td>\n",
       "      <td>73.5672</td>\n",
       "      <td>6.5732</td>\n",
       "      <td>18.0737</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>1.2355</td>\n",
       "      <td>1.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.943</td>\n",
       "      <td>104.5249</td>\n",
       "      <td>123.5440</td>\n",
       "      <td>325.2235</td>\n",
       "      <td>216.0460</td>\n",
       "      <td>188.1778</td>\n",
       "      <td>216.8083</td>\n",
       "      <td>106.9825</td>\n",
       "      <td>14.6230</td>\n",
       "      <td>98.7340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>2.3395</td>\n",
       "      <td>1.9636</td>\n",
       "      <td>2.7001</td>\n",
       "      <td>1.6907</td>\n",
       "      <td>1.3859</td>\n",
       "      <td>1.2406</td>\n",
       "      <td>1.8272</td>\n",
       "      <td>2.2453</td>\n",
       "      <td>1.9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.612</td>\n",
       "      <td>65.4221</td>\n",
       "      <td>27.6171</td>\n",
       "      <td>155.5133</td>\n",
       "      <td>82.3221</td>\n",
       "      <td>36.4214</td>\n",
       "      <td>46.7972</td>\n",
       "      <td>11.0348</td>\n",
       "      <td>7.9684</td>\n",
       "      <td>6.5255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1.1747</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.3874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      F_1       F_2       F_3       F_4       F_5       F_6       F_7  \\\n",
       "0  20.056   94.1213   56.6893  244.1506  297.1014   68.4912   73.1389   \n",
       "1  42.667   47.5442   72.8170   77.0697   35.0869   51.4889  103.3456   \n",
       "2  14.943  104.5249  123.5440  325.2235  216.0460  188.1778  216.8083   \n",
       "3  67.612   65.4221   27.6171  155.5133   82.3221   36.4214   46.7972   \n",
       "\n",
       "        F_8      F_9     F_10   ...     F_119   F_120   F_121   F_122   F_123  \\\n",
       "0   86.4008  46.9100  33.9853   ...    0.9821  1.6455  1.2656  1.0433  1.3137   \n",
       "1   73.5672   6.5732  18.0737   ...    1.0073  0.9375  0.8401  0.9308  0.8793   \n",
       "2  106.9825  14.6230  98.7340   ...    0.4323  2.3395  1.9636  2.7001  1.6907   \n",
       "3   11.0348   7.9684   6.5255   ...    0.7560  1.1747  0.5885  0.3979  0.7220   \n",
       "\n",
       "    F_124   F_125   F_126   F_127   F_128  \n",
       "0  2.4414  1.0232  1.2927  0.7879  1.7651  \n",
       "1  1.0529  1.0475  1.0450  1.2355  1.4466  \n",
       "2  1.3859  1.2406  1.8272  2.2453  1.9073  \n",
       "3  0.7708  0.3064  0.8781  0.7001  0.3874  \n",
       "\n",
       "[4 rows x 128 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdataset=Mdataset.iloc[:,6:]\n",
    "Xdataset.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "5     1\n",
       "6     1\n",
       "7     1\n",
       "8     1\n",
       "9     1\n",
       "10    2\n",
       "11    2\n",
       "12    2\n",
       "13    2\n",
       "Name: up, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ydataset = Mdataset['up']\n",
    "Ydataset.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05107705, 0.13282973, 0.0315936 , 0.11904173, 0.41328094,\n",
       "       0.13013288, 0.06080097, 0.17240561, 0.11593018, 0.13377925,\n",
       "       0.16578296, 0.22347617, 0.18588993, 0.24958625, 0.24999181,\n",
       "       0.21088881, 0.25110176, 0.23986342, 0.18482355, 0.17883252,\n",
       "       0.08571672, 0.04426925, 0.07225791, 0.04398994, 0.04239361,\n",
       "       0.09207876, 0.02792083, 0.05240673, 0.03630493, 0.03536118,\n",
       "       0.03906149, 0.05114374, 0.31190407, 0.07596091, 0.0062227 ,\n",
       "       0.11509733, 0.05813498, 0.06469943, 0.03883907, 0.06532665,\n",
       "       0.08104072, 0.15104461, 0.02481355, 0.04821453, 0.03800076,\n",
       "       0.06716089, 0.03636204, 0.04252229, 0.02039525, 0.05664509,\n",
       "       0.08343108, 0.0506808 , 0.05086898, 0.10339523, 0.06999565,\n",
       "       0.10077718, 0.07215901, 0.05073296, 0.03928588, 0.08224652,\n",
       "       0.10731719, 0.07810754, 0.09611584, 0.06576441, 0.04820583,\n",
       "       0.01855257, 0.01783409, 0.0669564 , 0.05401583, 0.02569278,\n",
       "       0.0042775 , 0.02914104, 0.00444942, 0.01840004, 0.03252842,\n",
       "       0.01247216, 0.02284181, 0.01525286, 0.01313781, 0.00801906,\n",
       "       0.01558545, 0.01095846, 0.01185073, 0.00987019, 0.01022947,\n",
       "       0.00759526, 0.00351516, 0.00380238, 0.01086448, 0.00781609,\n",
       "       0.01839407, 0.02454824, 0.01837983, 0.01915168, 0.01719985,\n",
       "       0.01444853, 0.07601717, 0.00664145, 0.00400803, 0.02043834,\n",
       "       0.01854563, 0.00908576, 0.00544105, 0.01766119, 0.02045896,\n",
       "       0.01995918, 0.00327982, 0.00412347, 0.01148738, 0.00868088,\n",
       "       0.00191083, 0.00615714, 0.01725313, 0.01454628, 0.01727621,\n",
       "       0.01270132, 0.00898884, 0.02366211, 0.01558594, 0.02758063,\n",
       "       0.01705962, 0.01649737, 0.02054278, 0.04137462, 0.01304682,\n",
       "       0.01925503, 0.01511903, 0.02898126])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(Xdataset)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329, 128, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.05107705])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset=dataset.reshape((dataset.shape[0],1,-1))\n",
    "dataset=dataset.reshape(dataset.shape+(1,))\n",
    "print(dataset.shape)\n",
    "dataset[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 109\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(trainX), len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_width = 48\n",
    "look_height=32\n",
    "from sklearn import preprocessing \n",
    "from sklearn import utils\n",
    "from io import StringIO\n",
    "\n",
    "#trainX, trainY = create_dataset(train, look_back)\n",
    "#testX, testY = create_dataset(test, look_back)\n",
    "le = preprocessing.LabelEncoder()\n",
    "YN=utils.column_or_1d(Ydataset, warn=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "YO=le.fit_transform(YN)\n",
    "YO\n",
    "len(le.classes_)\n",
    "Yclasses=len(le.classes_)\n",
    "print(Yclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 220 109\n"
     ]
    }
   ],
   "source": [
    "trainY = YO[0:train_size]\n",
    "testY=YO[train_size:len(YO)]\n",
    "print('data:',len(trainY), len(testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 128, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "#trainX = numpy.reshape(trainX, (trainX.shape[0],  trainX.shape[2],2))\n",
    "#testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 128, 1)\n",
      "(220,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "#dataset.shape\n",
    "print(trainY.shape)\n",
    "#print('trainyo',trainYO.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 128, 32)           64        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 128, 8)            264       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 8)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 128, 128)          1152      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 32770     \n",
      "=================================================================\n",
      "Total params: 34,250\n",
      "Trainable params: 34,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq_length = trainX.shape[1]\n",
    "print(seq_length)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 1, activation='relu', input_shape=(seq_length,1)))\n",
    "model.add(MaxPooling1D(1))\n",
    "model.add(Conv1D(8, 1, activation='relu'))\n",
    "#model.add(MaxPooling1D(1))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dense(128))\n",
    "model.add(Conv1D(128,1 ))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Conv1D(128,1 ))\n",
    "#model.add(MaxPooling1D())\n",
    "#model.add(Conv1D(128, 1, activation='r\n",
    "elu'))\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Yclasses, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 0.0494 - acc: 0.9773\n",
      "Epoch 2/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.1547 - acc: 0.9455\n",
      "Epoch 3/170\n",
      "220/220 [==============================] - 0s 234us/step - loss: 0.0863 - acc: 0.9727\n",
      "Epoch 4/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.3453 - acc: 0.9045\n",
      "Epoch 5/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.1006 - acc: 0.9591\n",
      "Epoch 6/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0310 - acc: 0.9909\n",
      "Epoch 7/170\n",
      "220/220 [==============================] - 0s 233us/step - loss: 0.2449 - acc: 0.9409\n",
      "Epoch 8/170\n",
      "220/220 [==============================] - 0s 233us/step - loss: 0.2138 - acc: 0.9273\n",
      "Epoch 9/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0564 - acc: 0.9727\n",
      "Epoch 10/170\n",
      "220/220 [==============================] - 0s 247us/step - loss: 0.1911 - acc: 0.9318\n",
      "Epoch 11/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.1649 - acc: 0.9545\n",
      "Epoch 12/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0380 - acc: 0.9864\n",
      "Epoch 13/170\n",
      "220/220 [==============================] - 0s 248us/step - loss: 0.0632 - acc: 0.9818\n",
      "Epoch 14/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.1144 - acc: 0.9682\n",
      "Epoch 15/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.0973 - acc: 0.9636\n",
      "Epoch 16/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0843 - acc: 0.9591\n",
      "Epoch 17/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.1187 - acc: 0.9545\n",
      "Epoch 18/170\n",
      "220/220 [==============================] - 0s 248us/step - loss: 0.1626 - acc: 0.9591\n",
      "Epoch 19/170\n",
      "220/220 [==============================] - 0s 234us/step - loss: 0.0840 - acc: 0.9727\n",
      "Epoch 20/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.0770 - acc: 0.9773\n",
      "Epoch 21/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0372 - acc: 0.9818\n",
      "Epoch 22/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.1182 - acc: 0.9455\n",
      "Epoch 23/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.0741 - acc: 0.9682\n",
      "Epoch 24/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.0636 - acc: 0.9727\n",
      "Epoch 25/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0187 - acc: 0.9909\n",
      "Epoch 26/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.2773 - acc: 0.9455\n",
      "Epoch 27/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.0226 - acc: 0.9955\n",
      "Epoch 28/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.1347 - acc: 0.9773\n",
      "Epoch 29/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.1888 - acc: 0.9409\n",
      "Epoch 30/170\n",
      "220/220 [==============================] - 0s 247us/step - loss: 0.0367 - acc: 0.9818\n",
      "Epoch 31/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.1025 - acc: 0.9682\n",
      "Epoch 32/170\n",
      "220/220 [==============================] - 0s 235us/step - loss: 0.1153 - acc: 0.9727\n",
      "Epoch 33/170\n",
      "220/220 [==============================] - 0s 242us/step - loss: 0.0913 - acc: 0.9636\n",
      "Epoch 34/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.1042 - acc: 0.9727\n",
      "Epoch 35/170\n",
      "220/220 [==============================] - 0s 250us/step - loss: 0.1344 - acc: 0.9682\n",
      "Epoch 36/170\n",
      "220/220 [==============================] - 0s 242us/step - loss: 0.0585 - acc: 0.9773\n",
      "Epoch 37/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0370 - acc: 0.9909\n",
      "Epoch 38/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 39/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0705 - acc: 0.9864\n",
      "Epoch 40/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0504 - acc: 0.9818\n",
      "Epoch 41/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.0797 - acc: 0.9636\n",
      "Epoch 42/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.1135 - acc: 0.9591\n",
      "Epoch 43/170\n",
      "220/220 [==============================] - 0s 242us/step - loss: 0.1340 - acc: 0.9500\n",
      "Epoch 44/170\n",
      "220/220 [==============================] - 0s 237us/step - loss: 0.1224 - acc: 0.9773\n",
      "Epoch 45/170\n",
      "220/220 [==============================] - 0s 248us/step - loss: 0.1861 - acc: 0.9500\n",
      "Epoch 46/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.1775 - acc: 0.9455\n",
      "Epoch 47/170\n",
      "220/220 [==============================] - 0s 248us/step - loss: 0.1469 - acc: 0.9364\n",
      "Epoch 48/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.1127 - acc: 0.9682\n",
      "Epoch 49/170\n",
      "220/220 [==============================] - 0s 249us/step - loss: 0.0844 - acc: 0.9773\n",
      "Epoch 50/170\n",
      "220/220 [==============================] - 0s 234us/step - loss: 0.0812 - acc: 0.9682\n",
      "Epoch 51/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.1152 - acc: 0.9682\n",
      "Epoch 52/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0900 - acc: 0.9727\n",
      "Epoch 53/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.1438 - acc: 0.9409\n",
      "Epoch 54/170\n",
      "220/220 [==============================] - 0s 242us/step - loss: 0.1488 - acc: 0.9500\n",
      "Epoch 55/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.0753 - acc: 0.9682\n",
      "Epoch 56/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.0973 - acc: 0.9545\n",
      "Epoch 57/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0662 - acc: 0.9864\n",
      "Epoch 58/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.0510 - acc: 0.9818\n",
      "Epoch 59/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0167 - acc: 0.9955\n",
      "Epoch 60/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.0757 - acc: 0.9818\n",
      "Epoch 61/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0861 - acc: 0.9864\n",
      "Epoch 62/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.0560 - acc: 0.9818\n",
      "Epoch 63/170\n",
      "220/220 [==============================] - 0s 245us/step - loss: 0.2004 - acc: 0.9636\n",
      "Epoch 64/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.1809 - acc: 0.9591\n",
      "Epoch 65/170\n",
      "220/220 [==============================] - 0s 249us/step - loss: 0.0355 - acc: 0.9909\n",
      "Epoch 66/170\n",
      "220/220 [==============================] - 0s 235us/step - loss: 0.1296 - acc: 0.9682\n",
      "Epoch 67/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0538 - acc: 0.9682\n",
      "Epoch 68/170\n",
      "220/220 [==============================] - 0s 235us/step - loss: 0.0598 - acc: 0.9818\n",
      "Epoch 69/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0351 - acc: 0.9818\n",
      "Epoch 70/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.0505 - acc: 0.9773\n",
      "Epoch 71/170\n",
      "220/220 [==============================] - 0s 245us/step - loss: 0.0666 - acc: 0.9773\n",
      "Epoch 72/170\n",
      "220/220 [==============================] - 0s 248us/step - loss: 0.0738 - acc: 0.9864\n",
      "Epoch 73/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.1354 - acc: 0.9591\n",
      "Epoch 74/170\n",
      "220/220 [==============================] - 0s 245us/step - loss: 0.2224 - acc: 0.9545\n",
      "Epoch 75/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.0197 - acc: 0.9864\n",
      "Epoch 76/170\n",
      "220/220 [==============================] - 0s 234us/step - loss: 0.0287 - acc: 0.9909\n",
      "Epoch 77/170\n",
      "220/220 [==============================] - 0s 237us/step - loss: 0.0421 - acc: 0.9909\n",
      "Epoch 78/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.0668 - acc: 0.9773\n",
      "Epoch 79/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0303 - acc: 0.9864\n",
      "Epoch 80/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0474 - acc: 0.9818\n",
      "Epoch 81/170\n",
      "220/220 [==============================] - 0s 232us/step - loss: 0.0568 - acc: 0.9727\n",
      "Epoch 82/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.2076 - acc: 0.9500\n",
      "Epoch 83/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.1308 - acc: 0.9682\n",
      "Epoch 84/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0246 - acc: 0.9864\n",
      "Epoch 85/170\n",
      "220/220 [==============================] - 0s 251us/step - loss: 0.0799 - acc: 0.9727\n",
      "Epoch 86/170\n",
      "220/220 [==============================] - 0s 237us/step - loss: 0.1258 - acc: 0.9818\n",
      "Epoch 87/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.0399 - acc: 0.9864\n",
      "Epoch 88/170\n",
      "220/220 [==============================] - 0s 242us/step - loss: 0.1696 - acc: 0.9591\n",
      "Epoch 89/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.0307 - acc: 0.9909\n",
      "Epoch 90/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0413 - acc: 0.9864\n",
      "Epoch 91/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.1363 - acc: 0.9591\n",
      "Epoch 92/170\n",
      "220/220 [==============================] - 0s 237us/step - loss: 0.2052 - acc: 0.9727\n",
      "Epoch 93/170\n",
      "220/220 [==============================] - 0s 234us/step - loss: 0.1823 - acc: 0.9636\n",
      "Epoch 94/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.1198 - acc: 0.9591\n",
      "Epoch 95/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0175 - acc: 0.9955\n",
      "Epoch 96/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.1737 - acc: 0.9636\n",
      "Epoch 97/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0852 - acc: 0.9727\n",
      "Epoch 98/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.0608 - acc: 0.9773\n",
      "Epoch 99/170\n",
      "220/220 [==============================] - 0s 230us/step - loss: 0.1536 - acc: 0.9636\n",
      "Epoch 100/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.2128 - acc: 0.9409\n",
      "Epoch 101/170\n",
      "220/220 [==============================] - 0s 242us/step - loss: 0.1549 - acc: 0.9682\n",
      "Epoch 102/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.0323 - acc: 0.9864\n",
      "Epoch 103/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.2087 - acc: 0.9636\n",
      "Epoch 104/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.0292 - acc: 0.9864\n",
      "Epoch 105/170\n",
      "220/220 [==============================] - 0s 245us/step - loss: 0.1634 - acc: 0.9636\n",
      "Epoch 106/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.0802 - acc: 0.9727\n",
      "Epoch 107/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.0169 - acc: 0.9909\n",
      "Epoch 108/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.0242 - acc: 0.9909\n",
      "Epoch 109/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.1337 - acc: 0.9591\n",
      "Epoch 110/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.1224 - acc: 0.9682\n",
      "Epoch 111/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.0626 - acc: 0.9818\n",
      "Epoch 112/170\n",
      "220/220 [==============================] - 0s 237us/step - loss: 0.0683 - acc: 0.9773\n",
      "Epoch 113/170\n",
      "220/220 [==============================] - 0s 249us/step - loss: 0.0185 - acc: 0.9909\n",
      "Epoch 114/170\n",
      "220/220 [==============================] - 0s 234us/step - loss: 0.0843 - acc: 0.9727\n",
      "Epoch 115/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.0774 - acc: 0.9727\n",
      "Epoch 116/170\n",
      "220/220 [==============================] - 0s 244us/step - loss: 0.1132 - acc: 0.9727\n",
      "Epoch 117/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.1505 - acc: 0.9591\n",
      "Epoch 118/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.0925 - acc: 0.9682\n",
      "Epoch 119/170\n",
      "220/220 [==============================] - 0s 241us/step - loss: 0.0620 - acc: 0.9727\n",
      "Epoch 120/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.1772 - acc: 0.9727\n",
      "Epoch 121/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.1126 - acc: 0.9727\n",
      "Epoch 122/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.1666 - acc: 0.9591\n",
      "Epoch 123/170\n",
      "220/220 [==============================] - 0s 238us/step - loss: 0.1508 - acc: 0.9636\n",
      "Epoch 124/170\n",
      "220/220 [==============================] - 0s 243us/step - loss: 0.0448 - acc: 0.9773\n",
      "Epoch 125/170\n",
      "220/220 [==============================] - 0s 247us/step - loss: 0.1395 - acc: 0.9727\n",
      "Epoch 126/170\n",
      "220/220 [==============================] - 0s 246us/step - loss: 0.1310 - acc: 0.9818\n",
      "Epoch 127/170\n",
      "220/220 [==============================] - 0s 236us/step - loss: 0.1479 - acc: 0.9591\n",
      "Epoch 128/170\n",
      "220/220 [==============================] - 0s 240us/step - loss: 0.2123 - acc: 0.9318\n",
      "Epoch 129/170\n",
      "220/220 [==============================] - 0s 237us/step - loss: 0.1881 - acc: 0.9409\n",
      "Epoch 130/170\n",
      "220/220 [==============================] - 0s 239us/step - loss: 0.0343 - acc: 0.9909\n",
      "Epoch 131/170\n",
      "220/220 [==============================] - 0s 536us/step - loss: 0.0634 - acc: 0.9773\n",
      "Epoch 132/170\n",
      "220/220 [==============================] - 0s 279us/step - loss: 0.0435 - acc: 0.9727\n",
      "Epoch 133/170\n",
      "220/220 [==============================] - 0s 280us/step - loss: 0.0562 - acc: 0.9682\n",
      "Epoch 134/170\n",
      "220/220 [==============================] - 0s 266us/step - loss: 0.1109 - acc: 0.9773\n",
      "Epoch 135/170\n",
      "220/220 [==============================] - 0s 280us/step - loss: 0.1014 - acc: 0.9727\n",
      "Epoch 136/170\n",
      "220/220 [==============================] - 0s 273us/step - loss: 0.0572 - acc: 0.9727\n",
      "Epoch 137/170\n",
      "220/220 [==============================] - 0s 275us/step - loss: 0.1534 - acc: 0.9545\n",
      "Epoch 138/170\n",
      "220/220 [==============================] - 0s 277us/step - loss: 0.1258 - acc: 0.9727\n",
      "Epoch 139/170\n",
      "220/220 [==============================] - 0s 278us/step - loss: 0.1584 - acc: 0.9773\n",
      "Epoch 140/170\n",
      "220/220 [==============================] - 0s 279us/step - loss: 0.0306 - acc: 0.9818\n",
      "Epoch 141/170\n",
      "220/220 [==============================] - 0s 279us/step - loss: 0.1406 - acc: 0.9636\n",
      "Epoch 142/170\n",
      "220/220 [==============================] - 0s 271us/step - loss: 0.0666 - acc: 0.9864\n",
      "Epoch 143/170\n",
      "220/220 [==============================] - 0s 277us/step - loss: 0.1928 - acc: 0.9545\n",
      "Epoch 144/170\n",
      "220/220 [==============================] - 0s 279us/step - loss: 0.1200 - acc: 0.9727\n",
      "Epoch 145/170\n",
      "220/220 [==============================] - 0s 263us/step - loss: 0.1103 - acc: 0.9682\n",
      "Epoch 146/170\n",
      "220/220 [==============================] - 0s 277us/step - loss: 0.1930 - acc: 0.9409\n",
      "Epoch 147/170\n",
      "220/220 [==============================] - 0s 284us/step - loss: 0.0869 - acc: 0.9727\n",
      "Epoch 148/170\n",
      "220/220 [==============================] - 0s 269us/step - loss: 0.1444 - acc: 0.9636\n",
      "Epoch 149/170\n",
      "220/220 [==============================] - 0s 271us/step - loss: 0.1391 - acc: 0.9591\n",
      "Epoch 150/170\n",
      "220/220 [==============================] - 0s 279us/step - loss: 0.0528 - acc: 0.9864\n",
      "Epoch 151/170\n",
      "220/220 [==============================] - 0s 266us/step - loss: 0.0918 - acc: 0.9864\n",
      "Epoch 152/170\n",
      "220/220 [==============================] - 0s 275us/step - loss: 0.0519 - acc: 0.9818\n",
      "Epoch 153/170\n",
      "220/220 [==============================] - 0s 284us/step - loss: 0.1523 - acc: 0.9727\n",
      "Epoch 154/170\n",
      "220/220 [==============================] - 0s 290us/step - loss: 0.0673 - acc: 0.9773\n",
      "Epoch 155/170\n",
      "220/220 [==============================] - 0s 269us/step - loss: 0.1919 - acc: 0.9773\n",
      "Epoch 156/170\n",
      "220/220 [==============================] - 0s 278us/step - loss: 0.0284 - acc: 0.9909\n",
      "Epoch 157/170\n",
      "220/220 [==============================] - 0s 267us/step - loss: 0.1077 - acc: 0.9682\n",
      "Epoch 158/170\n",
      "220/220 [==============================] - 0s 274us/step - loss: 0.0446 - acc: 0.9909\n",
      "Epoch 159/170\n",
      "220/220 [==============================] - 0s 280us/step - loss: 0.0092 - acc: 0.9955\n",
      "Epoch 160/170\n",
      "220/220 [==============================] - 0s 271us/step - loss: 0.1212 - acc: 0.9545\n",
      "Epoch 161/170\n",
      "220/220 [==============================] - 0s 277us/step - loss: 0.1320 - acc: 0.9591\n",
      "Epoch 162/170\n",
      "220/220 [==============================] - 0s 267us/step - loss: 0.1008 - acc: 0.9773\n",
      "Epoch 163/170\n",
      "220/220 [==============================] - 0s 269us/step - loss: 0.1932 - acc: 0.9545\n",
      "Epoch 164/170\n",
      "220/220 [==============================] - 0s 269us/step - loss: 0.1290 - acc: 0.9773\n",
      "Epoch 165/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 268us/step - loss: 0.0735 - acc: 0.9818\n",
      "Epoch 166/170\n",
      "220/220 [==============================] - 0s 278us/step - loss: 0.1047 - acc: 0.9682\n",
      "Epoch 167/170\n",
      "220/220 [==============================] - 0s 266us/step - loss: 0.1147 - acc: 0.9773\n",
      "Epoch 168/170\n",
      "220/220 [==============================] - 0s 277us/step - loss: 0.0494 - acc: 0.9818\n",
      "Epoch 169/170\n",
      "220/220 [==============================] - 0s 277us/step - loss: 0.0868 - acc: 0.9818\n",
      "Epoch 170/170\n",
      "220/220 [==============================] - 0s 277us/step - loss: 0.2132 - acc: 0.9591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba6c4c35c0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainYO = to_categorical(trainY, num_classes=Yclasses)\n",
    "testYO = to_categorical(testY, num_classes=Yclasses)\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model.fit(trainX, trainYO, epochs=170)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 0s 540us/step\n",
      "[2.374495917503987, 0.7431192661917537]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testYO)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 0s 208us/step\n",
      "Test loss: 0.0053427535172721205\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(trainX, trainYO, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7431192661917537\n",
      "1.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Keras reported accuracy:\n",
    "score = model.evaluate(testX,testYO, verbose=0) \n",
    "print(score[1])\n",
    "# 0.98580000000000001\n",
    "\n",
    "# Actual accuracy calculated manually:\n",
    "predY= model.predict(trainX)\n",
    "acc = sum([numpy.argmax(trainYO[i])==numpy.argmax(predY[i]) for i in range(len(trainYO))])/len(trainYO)\n",
    "print(acc)\n",
    "# 0.98580000000000001\n",
    "\n",
    "print(score[1]==acc)\n",
    "# True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict1 = model.predict(trainX)\n",
    "testPredict1 = model.predict(testX)\n",
    "#trainPredict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax(trainPredict)\n",
    "trainPredict=numpy.argmax(trainPredict1,1)\n",
    "testPredict=numpy.argmax(testPredict1,1)\n",
    "diffY=trainPredict-trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(diffY)\n",
    "plt.plot(trainY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# invert predictions\n",
    "#trainPredict = scaler.inverse_transform(trainPredict)\n",
    "#trainY = scaler.inverse_transform([trainY])\n",
    "#testPredict = scaler.inverse_transform(testPredict)\n",
    "#testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.165 RMSE\n",
      "Test Score: 0.525 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "print('Train Score: %.3f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY, testPredict))\n",
    "print('Test Score: %.3f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (220) into shape (220,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2d6065407b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwws\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainPredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mwws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# shift test predictions for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (220) into shape (220,1)"
     ]
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "wws=2;\n",
    "trainPredictPlot[wws:len(trainPredict)+wws,0] = trainPredict\n",
    "wws=60\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+wws+1:len(dataset)-look_back-1, 0] = testPredict\n",
    "# plot baseline and predictions\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(trainY,label=\"set\")\n",
    "ax1.plot(trainPredictPlot,label=\"train\")\n",
    "ax2.plot(testPredictPlot,label=\"testm\")\n",
    "#plt.figure(figsize=(15,3))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
