{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import  GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers import Conv2D,Flatten\n",
    "from keras.backend import argmax\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "csv_url='mrec20190331sfft.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>date</th>\n",
       "      <th>training</th>\n",
       "      <th>step</th>\n",
       "      <th>up</th>\n",
       "      <th>slice</th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>...</th>\n",
       "      <th>F_119</th>\n",
       "      <th>F_120</th>\n",
       "      <th>F_121</th>\n",
       "      <th>F_122</th>\n",
       "      <th>F_123</th>\n",
       "      <th>F_124</th>\n",
       "      <th>F_125</th>\n",
       "      <th>F_126</th>\n",
       "      <th>F_127</th>\n",
       "      <th>F_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1554034538313</td>\n",
       "      <td>13:15:38.313</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20749</td>\n",
       "      <td>20.056</td>\n",
       "      <td>94.1213</td>\n",
       "      <td>56.6893</td>\n",
       "      <td>244.1506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.6455</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>1.0433</td>\n",
       "      <td>1.3137</td>\n",
       "      <td>2.4414</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>1.2927</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>1.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1554034538958</td>\n",
       "      <td>13:15:38.958</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11691</td>\n",
       "      <td>42.667</td>\n",
       "      <td>47.5442</td>\n",
       "      <td>72.8170</td>\n",
       "      <td>77.0697</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>1.2355</td>\n",
       "      <td>1.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1554034539517</td>\n",
       "      <td>13:15:39.517</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>16388</td>\n",
       "      <td>14.943</td>\n",
       "      <td>104.5249</td>\n",
       "      <td>123.5440</td>\n",
       "      <td>325.2235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>2.3395</td>\n",
       "      <td>1.9636</td>\n",
       "      <td>2.7001</td>\n",
       "      <td>1.6907</td>\n",
       "      <td>1.3859</td>\n",
       "      <td>1.2406</td>\n",
       "      <td>1.8272</td>\n",
       "      <td>2.2453</td>\n",
       "      <td>1.9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1554034540279</td>\n",
       "      <td>13:15:40.279</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7260</td>\n",
       "      <td>67.612</td>\n",
       "      <td>65.4221</td>\n",
       "      <td>27.6171</td>\n",
       "      <td>155.5133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1.1747</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.3874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1554034540774</td>\n",
       "      <td>13:15:40.774</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>10254</td>\n",
       "      <td>51.408</td>\n",
       "      <td>151.3434</td>\n",
       "      <td>105.4404</td>\n",
       "      <td>376.3469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4306</td>\n",
       "      <td>1.2238</td>\n",
       "      <td>1.7379</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.4861</td>\n",
       "      <td>1.4681</td>\n",
       "      <td>0.6975</td>\n",
       "      <td>1.1393</td>\n",
       "      <td>1.1507</td>\n",
       "      <td>0.8577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1554034541375</td>\n",
       "      <td>13:15:41.375</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12306</td>\n",
       "      <td>43.714</td>\n",
       "      <td>197.7310</td>\n",
       "      <td>287.9238</td>\n",
       "      <td>458.5304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1554</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.3534</td>\n",
       "      <td>0.2905</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.8668</td>\n",
       "      <td>1.0058</td>\n",
       "      <td>0.3264</td>\n",
       "      <td>0.7247</td>\n",
       "      <td>0.2743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1554034542523</td>\n",
       "      <td>13:15:42.523</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>12275</td>\n",
       "      <td>36.233</td>\n",
       "      <td>4.3915</td>\n",
       "      <td>16.2003</td>\n",
       "      <td>67.3880</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1263</td>\n",
       "      <td>3.0966</td>\n",
       "      <td>0.7637</td>\n",
       "      <td>4.8209</td>\n",
       "      <td>2.8054</td>\n",
       "      <td>2.8560</td>\n",
       "      <td>6.5081</td>\n",
       "      <td>4.1053</td>\n",
       "      <td>4.8375</td>\n",
       "      <td>6.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1554034542921</td>\n",
       "      <td>13:15:42.921</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>11018</td>\n",
       "      <td>8.626</td>\n",
       "      <td>32.5530</td>\n",
       "      <td>114.6319</td>\n",
       "      <td>209.5523</td>\n",
       "      <td>...</td>\n",
       "      <td>12.8564</td>\n",
       "      <td>10.8709</td>\n",
       "      <td>12.2103</td>\n",
       "      <td>5.6484</td>\n",
       "      <td>4.5759</td>\n",
       "      <td>3.3081</td>\n",
       "      <td>11.8470</td>\n",
       "      <td>11.5552</td>\n",
       "      <td>9.8679</td>\n",
       "      <td>11.1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1554034543324</td>\n",
       "      <td>13:15:43.324</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>48631</td>\n",
       "      <td>32.854</td>\n",
       "      <td>117.6710</td>\n",
       "      <td>414.5323</td>\n",
       "      <td>808.1890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7231</td>\n",
       "      <td>2.2230</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>1.8384</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>1.7533</td>\n",
       "      <td>0.5601</td>\n",
       "      <td>1.9654</td>\n",
       "      <td>1.1408</td>\n",
       "      <td>0.8986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1554034543872</td>\n",
       "      <td>13:15:43.872</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12990</td>\n",
       "      <td>95.478</td>\n",
       "      <td>328.9371</td>\n",
       "      <td>369.8189</td>\n",
       "      <td>122.3947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1821</td>\n",
       "      <td>0.3310</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>1.0916</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.7634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1554034552741</td>\n",
       "      <td>13:15:52.741</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7295</td>\n",
       "      <td>51.179</td>\n",
       "      <td>110.7519</td>\n",
       "      <td>121.1264</td>\n",
       "      <td>48.1045</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1168</td>\n",
       "      <td>1.2650</td>\n",
       "      <td>1.0660</td>\n",
       "      <td>0.5436</td>\n",
       "      <td>1.8927</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.0917</td>\n",
       "      <td>1.9604</td>\n",
       "      <td>2.4926</td>\n",
       "      <td>2.0846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1554034553464</td>\n",
       "      <td>13:15:53.464</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>15670</td>\n",
       "      <td>91.319</td>\n",
       "      <td>297.7418</td>\n",
       "      <td>238.2316</td>\n",
       "      <td>187.9039</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0638</td>\n",
       "      <td>2.1898</td>\n",
       "      <td>5.2268</td>\n",
       "      <td>3.8690</td>\n",
       "      <td>1.8847</td>\n",
       "      <td>4.5757</td>\n",
       "      <td>2.6038</td>\n",
       "      <td>3.7672</td>\n",
       "      <td>3.9683</td>\n",
       "      <td>2.8586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1554034554064</td>\n",
       "      <td>13:15:54.064</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7187</td>\n",
       "      <td>89.670</td>\n",
       "      <td>25.7131</td>\n",
       "      <td>114.5191</td>\n",
       "      <td>124.2920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6383</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>1.0036</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>1.8103</td>\n",
       "      <td>1.6553</td>\n",
       "      <td>0.9874</td>\n",
       "      <td>0.3588</td>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.5634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1554034555772</td>\n",
       "      <td>13:15:55.772</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>63054</td>\n",
       "      <td>176.856</td>\n",
       "      <td>283.4218</td>\n",
       "      <td>629.3238</td>\n",
       "      <td>135.3471</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4961</td>\n",
       "      <td>1.1693</td>\n",
       "      <td>1.2748</td>\n",
       "      <td>0.8044</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.7229</td>\n",
       "      <td>1.1219</td>\n",
       "      <td>1.4850</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.9767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1554034556313</td>\n",
       "      <td>13:15:56.313</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7979</td>\n",
       "      <td>92.689</td>\n",
       "      <td>82.8024</td>\n",
       "      <td>227.5237</td>\n",
       "      <td>131.2919</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2423</td>\n",
       "      <td>7.1212</td>\n",
       "      <td>8.3202</td>\n",
       "      <td>2.1716</td>\n",
       "      <td>5.9391</td>\n",
       "      <td>6.3946</td>\n",
       "      <td>2.9856</td>\n",
       "      <td>4.9899</td>\n",
       "      <td>4.1904</td>\n",
       "      <td>3.5582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1554034556749</td>\n",
       "      <td>13:15:56.749</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9095</td>\n",
       "      <td>60.448</td>\n",
       "      <td>116.9732</td>\n",
       "      <td>83.3200</td>\n",
       "      <td>21.3689</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5014</td>\n",
       "      <td>2.7870</td>\n",
       "      <td>2.7309</td>\n",
       "      <td>1.7733</td>\n",
       "      <td>2.5238</td>\n",
       "      <td>1.4445</td>\n",
       "      <td>3.5992</td>\n",
       "      <td>1.5622</td>\n",
       "      <td>1.9059</td>\n",
       "      <td>1.0923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1554034557167</td>\n",
       "      <td>13:15:57.167</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10473</td>\n",
       "      <td>5.745</td>\n",
       "      <td>86.2626</td>\n",
       "      <td>214.0896</td>\n",
       "      <td>92.3728</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3324</td>\n",
       "      <td>5.7336</td>\n",
       "      <td>7.9026</td>\n",
       "      <td>9.3345</td>\n",
       "      <td>12.5422</td>\n",
       "      <td>9.1397</td>\n",
       "      <td>12.9683</td>\n",
       "      <td>10.5161</td>\n",
       "      <td>12.1184</td>\n",
       "      <td>8.5896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1554034557652</td>\n",
       "      <td>13:15:57.652</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9539</td>\n",
       "      <td>105.918</td>\n",
       "      <td>222.3163</td>\n",
       "      <td>146.9655</td>\n",
       "      <td>107.1227</td>\n",
       "      <td>...</td>\n",
       "      <td>23.9141</td>\n",
       "      <td>20.8094</td>\n",
       "      <td>6.7922</td>\n",
       "      <td>16.0169</td>\n",
       "      <td>20.7039</td>\n",
       "      <td>16.0992</td>\n",
       "      <td>14.9737</td>\n",
       "      <td>23.0030</td>\n",
       "      <td>20.7990</td>\n",
       "      <td>13.1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1554034558096</td>\n",
       "      <td>13:15:58.096</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>8128</td>\n",
       "      <td>113.464</td>\n",
       "      <td>202.6727</td>\n",
       "      <td>179.4488</td>\n",
       "      <td>142.2376</td>\n",
       "      <td>...</td>\n",
       "      <td>4.3024</td>\n",
       "      <td>5.0787</td>\n",
       "      <td>5.5172</td>\n",
       "      <td>4.9711</td>\n",
       "      <td>3.8353</td>\n",
       "      <td>3.0314</td>\n",
       "      <td>3.4189</td>\n",
       "      <td>3.2492</td>\n",
       "      <td>1.4511</td>\n",
       "      <td>0.9276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1554034558486</td>\n",
       "      <td>13:15:58.486</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>8780</td>\n",
       "      <td>72.946</td>\n",
       "      <td>35.3417</td>\n",
       "      <td>163.8728</td>\n",
       "      <td>139.9921</td>\n",
       "      <td>...</td>\n",
       "      <td>19.7552</td>\n",
       "      <td>7.4400</td>\n",
       "      <td>12.9592</td>\n",
       "      <td>8.1229</td>\n",
       "      <td>18.1363</td>\n",
       "      <td>8.3398</td>\n",
       "      <td>11.5341</td>\n",
       "      <td>16.8605</td>\n",
       "      <td>8.9856</td>\n",
       "      <td>15.4814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1554034568646</td>\n",
       "      <td>13:16:08.646</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8535</td>\n",
       "      <td>39.649</td>\n",
       "      <td>54.2175</td>\n",
       "      <td>21.5935</td>\n",
       "      <td>108.3749</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8681</td>\n",
       "      <td>1.4058</td>\n",
       "      <td>1.1511</td>\n",
       "      <td>0.6821</td>\n",
       "      <td>0.3394</td>\n",
       "      <td>1.3633</td>\n",
       "      <td>1.1481</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>1.3334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1554034569220</td>\n",
       "      <td>13:16:09.220</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>13869</td>\n",
       "      <td>33.400</td>\n",
       "      <td>36.6111</td>\n",
       "      <td>34.0357</td>\n",
       "      <td>119.4974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>0.7304</td>\n",
       "      <td>0.9182</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>0.3038</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.1936</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>0.6368</td>\n",
       "      <td>0.6733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1554034569985</td>\n",
       "      <td>13:16:09.985</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12336</td>\n",
       "      <td>53.202</td>\n",
       "      <td>15.5826</td>\n",
       "      <td>76.3712</td>\n",
       "      <td>129.6231</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1067</td>\n",
       "      <td>5.6749</td>\n",
       "      <td>1.3833</td>\n",
       "      <td>3.7632</td>\n",
       "      <td>4.2902</td>\n",
       "      <td>3.4170</td>\n",
       "      <td>2.1096</td>\n",
       "      <td>5.3670</td>\n",
       "      <td>2.9436</td>\n",
       "      <td>5.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1554034570413</td>\n",
       "      <td>13:16:10.413</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9037</td>\n",
       "      <td>78.870</td>\n",
       "      <td>53.4001</td>\n",
       "      <td>116.1074</td>\n",
       "      <td>411.5939</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9045</td>\n",
       "      <td>1.8140</td>\n",
       "      <td>1.5852</td>\n",
       "      <td>1.9278</td>\n",
       "      <td>1.9029</td>\n",
       "      <td>0.5600</td>\n",
       "      <td>0.8157</td>\n",
       "      <td>0.7449</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>1.2878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             time          date  training  step  up  slice      F_1       F_2  \\\n",
       "0   1554034538313  13:15:38.313         0     3   1  20749   20.056   94.1213   \n",
       "1   1554034538958  13:15:38.958         0     4   1  11691   42.667   47.5442   \n",
       "2   1554034539517  13:15:39.517         0     5   1  16388   14.943  104.5249   \n",
       "3   1554034540279  13:15:40.279         0     7   1   7260   67.612   65.4221   \n",
       "4   1554034540774  13:15:40.774         0     8   1  10254   51.408  151.3434   \n",
       "5   1554034541375  13:15:41.375         0     9   1  12306   43.714  197.7310   \n",
       "6   1554034542523  13:15:42.523         0    11   1  12275   36.233    4.3915   \n",
       "7   1554034542921  13:15:42.921         0    12   1  11018    8.626   32.5530   \n",
       "8   1554034543324  13:15:43.324         0    13   1  48631   32.854  117.6710   \n",
       "9   1554034543872  13:15:43.872         0    14   1  12990   95.478  328.9371   \n",
       "10  1554034552741  13:15:52.741         0     3   2   7295   51.179  110.7519   \n",
       "11  1554034553464  13:15:53.464         0     4   2  15670   91.319  297.7418   \n",
       "12  1554034554064  13:15:54.064         0     5   2   7187   89.670   25.7131   \n",
       "13  1554034555772  13:15:55.772         0     7   2  63054  176.856  283.4218   \n",
       "14  1554034556313  13:15:56.313         0     8   2   7979   92.689   82.8024   \n",
       "15  1554034556749  13:15:56.749         0     9   2   9095   60.448  116.9732   \n",
       "16  1554034557167  13:15:57.167         0    10   2  10473    5.745   86.2626   \n",
       "17  1554034557652  13:15:57.652         0    12   2   9539  105.918  222.3163   \n",
       "18  1554034558096  13:15:58.096         0    13   2   8128  113.464  202.6727   \n",
       "19  1554034558486  13:15:58.486         0    14   2   8780   72.946   35.3417   \n",
       "20  1554034568646  13:16:08.646         0     3   1   8535   39.649   54.2175   \n",
       "21  1554034569220  13:16:09.220         0     4   1  13869   33.400   36.6111   \n",
       "22  1554034569985  13:16:09.985         0     6   1  12336   53.202   15.5826   \n",
       "23  1554034570413  13:16:10.413         0     7   1   9037   78.870   53.4001   \n",
       "\n",
       "         F_3       F_4   ...       F_119    F_120    F_121    F_122    F_123  \\\n",
       "0    56.6893  244.1506   ...      0.9821   1.6455   1.2656   1.0433   1.3137   \n",
       "1    72.8170   77.0697   ...      1.0073   0.9375   0.8401   0.9308   0.8793   \n",
       "2   123.5440  325.2235   ...      0.4323   2.3395   1.9636   2.7001   1.6907   \n",
       "3    27.6171  155.5133   ...      0.7560   1.1747   0.5885   0.3979   0.7220   \n",
       "4   105.4404  376.3469   ...      0.4306   1.2238   1.7379   0.3559   0.4861   \n",
       "5   287.9238  458.5304   ...      0.1554   0.9977   0.3534   0.2905   0.0378   \n",
       "6    16.2003   67.3880   ...      1.1263   3.0966   0.7637   4.8209   2.8054   \n",
       "7   114.6319  209.5523   ...     12.8564  10.8709  12.2103   5.6484   4.5759   \n",
       "8   414.5323  808.1890   ...      0.7231   2.2230   0.1032   1.8384   0.9183   \n",
       "9   369.8189  122.3947   ...      0.1821   0.3310   0.1429   0.9825   0.4588   \n",
       "10  121.1264   48.1045   ...      1.1168   1.2650   1.0660   0.5436   1.8927   \n",
       "11  238.2316  187.9039   ...      6.0638   2.1898   5.2268   3.8690   1.8847   \n",
       "12  114.5191  124.2920   ...      0.6383   0.2362   1.0036   1.5005   1.8103   \n",
       "13  629.3238  135.3471   ...      1.4961   1.1693   1.2748   0.8044   0.6667   \n",
       "14  227.5237  131.2919   ...      2.2423   7.1212   8.3202   2.1716   5.9391   \n",
       "15   83.3200   21.3689   ...      2.5014   2.7870   2.7309   1.7733   2.5238   \n",
       "16  214.0896   92.3728   ...      7.3324   5.7336   7.9026   9.3345  12.5422   \n",
       "17  146.9655  107.1227   ...     23.9141  20.8094   6.7922  16.0169  20.7039   \n",
       "18  179.4488  142.2376   ...      4.3024   5.0787   5.5172   4.9711   3.8353   \n",
       "19  163.8728  139.9921   ...     19.7552   7.4400  12.9592   8.1229  18.1363   \n",
       "20   21.5935  108.3749   ...      0.8681   1.4058   1.1511   0.6821   0.3394   \n",
       "21   34.0357  119.4974   ...      0.1142   0.7304   0.9182   0.3173   0.3038   \n",
       "22   76.3712  129.6231   ...      3.1067   5.6749   1.3833   3.7632   4.2902   \n",
       "23  116.1074  411.5939   ...      1.9045   1.8140   1.5852   1.9278   1.9029   \n",
       "\n",
       "      F_124    F_125    F_126    F_127    F_128  \n",
       "0    2.4414   1.0232   1.2927   0.7879   1.7651  \n",
       "1    1.0529   1.0475   1.0450   1.2355   1.4466  \n",
       "2    1.3859   1.2406   1.8272   2.2453   1.9073  \n",
       "3    0.7708   0.3064   0.8781   0.7001   0.3874  \n",
       "4    1.4681   0.6975   1.1393   1.1507   0.8577  \n",
       "5    0.8668   1.0058   0.3264   0.7247   0.2743  \n",
       "6    2.8560   6.5081   4.1053   4.8375   6.3500  \n",
       "7    3.3081  11.8470  11.5552   9.8679  11.1101  \n",
       "8    1.7533   0.5601   1.9654   1.1408   0.8986  \n",
       "9    1.0916   0.6122   0.9901   0.4200   0.7634  \n",
       "10   0.9676   0.0917   1.9604   2.4926   2.0846  \n",
       "11   4.5757   2.6038   3.7672   3.9683   2.8586  \n",
       "12   1.6553   0.9874   0.3588   0.1287   0.5634  \n",
       "13   0.7229   1.1219   1.4850   0.7797   0.9767  \n",
       "14   6.3946   2.9856   4.9899   4.1904   3.5582  \n",
       "15   1.4445   3.5992   1.5622   1.9059   1.0923  \n",
       "16   9.1397  12.9683  10.5161  12.1184   8.5896  \n",
       "17  16.0992  14.9737  23.0030  20.7990  13.1253  \n",
       "18   3.0314   3.4189   3.2492   1.4511   0.9276  \n",
       "19   8.3398  11.5341  16.8605   8.9856  15.4814  \n",
       "20   1.3633   1.1481   0.7551   0.1447   1.3334  \n",
       "21   0.1145   0.1936   0.7005   0.6368   0.6733  \n",
       "22   3.4170   2.1096   5.3670   2.9436   5.0052  \n",
       "23   0.5600   0.8157   0.7449   0.7691   1.2878  \n",
       "\n",
       "[24 rows x 134 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MM= pandas.read_csv(csv_url)\n",
    "#Mdataset=shuffle(MM)\n",
    "Mdataset=MM\n",
    "#Mdataset=MM.sample(frac=1)\n",
    "Mdataset.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F_1</th>\n",
       "      <th>F_2</th>\n",
       "      <th>F_3</th>\n",
       "      <th>F_4</th>\n",
       "      <th>F_5</th>\n",
       "      <th>F_6</th>\n",
       "      <th>F_7</th>\n",
       "      <th>F_8</th>\n",
       "      <th>F_9</th>\n",
       "      <th>F_10</th>\n",
       "      <th>...</th>\n",
       "      <th>F_119</th>\n",
       "      <th>F_120</th>\n",
       "      <th>F_121</th>\n",
       "      <th>F_122</th>\n",
       "      <th>F_123</th>\n",
       "      <th>F_124</th>\n",
       "      <th>F_125</th>\n",
       "      <th>F_126</th>\n",
       "      <th>F_127</th>\n",
       "      <th>F_128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.056</td>\n",
       "      <td>94.1213</td>\n",
       "      <td>56.6893</td>\n",
       "      <td>244.1506</td>\n",
       "      <td>297.1014</td>\n",
       "      <td>68.4912</td>\n",
       "      <td>73.1389</td>\n",
       "      <td>86.4008</td>\n",
       "      <td>46.9100</td>\n",
       "      <td>33.9853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>1.6455</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>1.0433</td>\n",
       "      <td>1.3137</td>\n",
       "      <td>2.4414</td>\n",
       "      <td>1.0232</td>\n",
       "      <td>1.2927</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>1.7651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.667</td>\n",
       "      <td>47.5442</td>\n",
       "      <td>72.8170</td>\n",
       "      <td>77.0697</td>\n",
       "      <td>35.0869</td>\n",
       "      <td>51.4889</td>\n",
       "      <td>103.3456</td>\n",
       "      <td>73.5672</td>\n",
       "      <td>6.5732</td>\n",
       "      <td>18.0737</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0073</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>1.0475</td>\n",
       "      <td>1.0450</td>\n",
       "      <td>1.2355</td>\n",
       "      <td>1.4466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.943</td>\n",
       "      <td>104.5249</td>\n",
       "      <td>123.5440</td>\n",
       "      <td>325.2235</td>\n",
       "      <td>216.0460</td>\n",
       "      <td>188.1778</td>\n",
       "      <td>216.8083</td>\n",
       "      <td>106.9825</td>\n",
       "      <td>14.6230</td>\n",
       "      <td>98.7340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>2.3395</td>\n",
       "      <td>1.9636</td>\n",
       "      <td>2.7001</td>\n",
       "      <td>1.6907</td>\n",
       "      <td>1.3859</td>\n",
       "      <td>1.2406</td>\n",
       "      <td>1.8272</td>\n",
       "      <td>2.2453</td>\n",
       "      <td>1.9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67.612</td>\n",
       "      <td>65.4221</td>\n",
       "      <td>27.6171</td>\n",
       "      <td>155.5133</td>\n",
       "      <td>82.3221</td>\n",
       "      <td>36.4214</td>\n",
       "      <td>46.7972</td>\n",
       "      <td>11.0348</td>\n",
       "      <td>7.9684</td>\n",
       "      <td>6.5255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>1.1747</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.7220</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.7001</td>\n",
       "      <td>0.3874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      F_1       F_2       F_3       F_4       F_5       F_6       F_7  \\\n",
       "0  20.056   94.1213   56.6893  244.1506  297.1014   68.4912   73.1389   \n",
       "1  42.667   47.5442   72.8170   77.0697   35.0869   51.4889  103.3456   \n",
       "2  14.943  104.5249  123.5440  325.2235  216.0460  188.1778  216.8083   \n",
       "3  67.612   65.4221   27.6171  155.5133   82.3221   36.4214   46.7972   \n",
       "\n",
       "        F_8      F_9     F_10   ...     F_119   F_120   F_121   F_122   F_123  \\\n",
       "0   86.4008  46.9100  33.9853   ...    0.9821  1.6455  1.2656  1.0433  1.3137   \n",
       "1   73.5672   6.5732  18.0737   ...    1.0073  0.9375  0.8401  0.9308  0.8793   \n",
       "2  106.9825  14.6230  98.7340   ...    0.4323  2.3395  1.9636  2.7001  1.6907   \n",
       "3   11.0348   7.9684   6.5255   ...    0.7560  1.1747  0.5885  0.3979  0.7220   \n",
       "\n",
       "    F_124   F_125   F_126   F_127   F_128  \n",
       "0  2.4414  1.0232  1.2927  0.7879  1.7651  \n",
       "1  1.0529  1.0475  1.0450  1.2355  1.4466  \n",
       "2  1.3859  1.2406  1.8272  2.2453  1.9073  \n",
       "3  0.7708  0.3064  0.8781  0.7001  0.3874  \n",
       "\n",
       "[4 rows x 128 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdataset=Mdataset.iloc[:,6:]\n",
    "Xdataset.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "5     1\n",
       "6     1\n",
       "7     1\n",
       "8     1\n",
       "9     1\n",
       "10    2\n",
       "11    2\n",
       "12    2\n",
       "13    2\n",
       "Name: up, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ydataset = Mdataset['up']\n",
    "Ydataset.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05107705, 0.13282973, 0.0315936 , 0.11904173, 0.41328094,\n",
       "       0.13013288, 0.06080097, 0.17240561, 0.11593018, 0.13377925,\n",
       "       0.16578296, 0.22347617, 0.18588993, 0.24958625, 0.24999181,\n",
       "       0.21088881, 0.25110176, 0.23986342, 0.18482355, 0.17883252,\n",
       "       0.08571672, 0.04426925, 0.07225791, 0.04398994, 0.04239361,\n",
       "       0.09207876, 0.02792083, 0.05240673, 0.03630493, 0.03536118,\n",
       "       0.03906149, 0.05114374, 0.31190407, 0.07596091, 0.0062227 ,\n",
       "       0.11509733, 0.05813498, 0.06469943, 0.03883907, 0.06532665,\n",
       "       0.08104072, 0.15104461, 0.02481355, 0.04821453, 0.03800076,\n",
       "       0.06716089, 0.03636204, 0.04252229, 0.02039525, 0.05664509,\n",
       "       0.08343108, 0.0506808 , 0.05086898, 0.10339523, 0.06999565,\n",
       "       0.10077718, 0.07215901, 0.05073296, 0.03928588, 0.08224652,\n",
       "       0.10731719, 0.07810754, 0.09611584, 0.06576441, 0.04820583,\n",
       "       0.01855257, 0.01783409, 0.0669564 , 0.05401583, 0.02569278,\n",
       "       0.0042775 , 0.02914104, 0.00444942, 0.01840004, 0.03252842,\n",
       "       0.01247216, 0.02284181, 0.01525286, 0.01313781, 0.00801906,\n",
       "       0.01558545, 0.01095846, 0.01185073, 0.00987019, 0.01022947,\n",
       "       0.00759526, 0.00351516, 0.00380238, 0.01086448, 0.00781609,\n",
       "       0.01839407, 0.02454824, 0.01837983, 0.01915168, 0.01719985,\n",
       "       0.01444853, 0.07601717, 0.00664145, 0.00400803, 0.02043834,\n",
       "       0.01854563, 0.00908576, 0.00544105, 0.01766119, 0.02045896,\n",
       "       0.01995918, 0.00327982, 0.00412347, 0.01148738, 0.00868088,\n",
       "       0.00191083, 0.00615714, 0.01725313, 0.01454628, 0.01727621,\n",
       "       0.01270132, 0.00898884, 0.02366211, 0.01558594, 0.02758063,\n",
       "       0.01705962, 0.01649737, 0.02054278, 0.04137462, 0.01304682,\n",
       "       0.01925503, 0.01511903, 0.02898126])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(Xdataset)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(329, 4, 32, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05107705],\n",
       "       [0.13282973],\n",
       "       [0.0315936 ],\n",
       "       [0.11904173],\n",
       "       [0.41328094],\n",
       "       [0.13013288],\n",
       "       [0.06080097],\n",
       "       [0.17240561],\n",
       "       [0.11593018],\n",
       "       [0.13377925],\n",
       "       [0.16578296],\n",
       "       [0.22347617],\n",
       "       [0.18588993],\n",
       "       [0.24958625],\n",
       "       [0.24999181],\n",
       "       [0.21088881],\n",
       "       [0.25110176],\n",
       "       [0.23986342],\n",
       "       [0.18482355],\n",
       "       [0.17883252],\n",
       "       [0.08571672],\n",
       "       [0.04426925],\n",
       "       [0.07225791],\n",
       "       [0.04398994],\n",
       "       [0.04239361],\n",
       "       [0.09207876],\n",
       "       [0.02792083],\n",
       "       [0.05240673],\n",
       "       [0.03630493],\n",
       "       [0.03536118],\n",
       "       [0.03906149],\n",
       "       [0.05114374]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset=dataset.reshape((dataset.shape[0],1,-1))\n",
    "dataset=dataset.reshape(dataset.shape[0],4,32,1)\n",
    "print(dataset.shape)\n",
    "dataset[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 43\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.87)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(trainX), len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "look_width = 48\n",
    "look_height=32\n",
    "from sklearn import preprocessing \n",
    "from sklearn import utils\n",
    "from io import StringIO\n",
    "\n",
    "#trainX, trainY = create_dataset(train, look_back)\n",
    "#testX, testY = create_dataset(test, look_back)\n",
    "le = preprocessing.LabelEncoder()\n",
    "YN=utils.column_or_1d(Ydataset, warn=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "YO=le.fit_transform(YN)\n",
    "YO\n",
    "len(le.classes_)\n",
    "Yclasses=len(le.classes_)\n",
    "print(Yclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 286 43\n"
     ]
    }
   ],
   "source": [
    "trainY = YO[0:train_size]\n",
    "testY=YO[train_size:len(YO)]\n",
    "print('data:',len(trainY), len(testY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286, 4, 32, 1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features]\n",
    "#trainX = numpy.reshape(trainX, (trainX.shape[0],  trainX.shape[2],2))\n",
    "#testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 4, 32, 1)\n",
      "(286,)\n",
      "trainyo (286, 2)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "#dataset.shape\n",
    "print(trainY.shape)\n",
    "print('trainyo',trainYO.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_81 (Conv2D)           (None, 32, 30, 1)         416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 30, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 8, 28, 1)          776       \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 8, 28, 1)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 224)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 450       \n",
      "=================================================================\n",
      "Total params: 1,642\n",
      "Trainable params: 1,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,1) , activation='relu', input_shape=(4,32,1)))\n",
    "model.add(MaxPooling2D(1))\n",
    "model.add(Conv2D(8, (3,1), activation='relu'))\n",
    "#model.add(MaxPooling1D(1))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dense(128))\n",
    "#model.add(Conv1D(128,1 ))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Conv1D(128,1 ))\n",
    "#model.add(MaxPooling1D())\n",
    "#model.add(Conv1D(128, 1, activation='relu'))\n",
    "#model.add(GlobalAveragePooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(Yclasses, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/170\n",
      "286/286 [==============================] - 0s 1ms/step - loss: 0.6959 - acc: 0.5175\n",
      "Epoch 2/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.6897 - acc: 0.5385\n",
      "Epoch 3/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.6694 - acc: 0.5804\n",
      "Epoch 4/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.6346 - acc: 0.6608\n",
      "Epoch 5/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.6056 - acc: 0.6503\n",
      "Epoch 6/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.5510 - acc: 0.7413\n",
      "Epoch 7/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.5203 - acc: 0.7483\n",
      "Epoch 8/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.4950 - acc: 0.7727\n",
      "Epoch 9/170\n",
      "286/286 [==============================] - 0s 127us/step - loss: 0.4874 - acc: 0.7797\n",
      "Epoch 10/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.5048 - acc: 0.7832\n",
      "Epoch 11/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.4612 - acc: 0.7727\n",
      "Epoch 12/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.4771 - acc: 0.7552\n",
      "Epoch 13/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.4234 - acc: 0.8322\n",
      "Epoch 14/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.4785 - acc: 0.7762\n",
      "Epoch 15/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.4555 - acc: 0.7902\n",
      "Epoch 16/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.4221 - acc: 0.7832\n",
      "Epoch 17/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.4430 - acc: 0.8007\n",
      "Epoch 18/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3946 - acc: 0.7832\n",
      "Epoch 19/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.4027 - acc: 0.8287\n",
      "Epoch 20/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.4277 - acc: 0.7902\n",
      "Epoch 21/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.3852 - acc: 0.8182\n",
      "Epoch 22/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.3831 - acc: 0.8217\n",
      "Epoch 23/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.3930 - acc: 0.8147\n",
      "Epoch 24/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.3714 - acc: 0.8357\n",
      "Epoch 25/170\n",
      "286/286 [==============================] - 0s 128us/step - loss: 0.3595 - acc: 0.8287\n",
      "Epoch 26/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3689 - acc: 0.8566\n",
      "Epoch 27/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.3419 - acc: 0.8427\n",
      "Epoch 28/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.3587 - acc: 0.8462\n",
      "Epoch 29/170\n",
      "286/286 [==============================] - 0s 118us/step - loss: 0.3913 - acc: 0.7972\n",
      "Epoch 30/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3159 - acc: 0.8462\n",
      "Epoch 31/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3352 - acc: 0.8357\n",
      "Epoch 32/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3305 - acc: 0.8636\n",
      "Epoch 33/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.3397 - acc: 0.8601\n",
      "Epoch 34/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3543 - acc: 0.8392\n",
      "Epoch 35/170\n",
      "286/286 [==============================] - 0s 115us/step - loss: 0.2986 - acc: 0.8811\n",
      "Epoch 36/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.2812 - acc: 0.8776\n",
      "Epoch 37/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3367 - acc: 0.8811\n",
      "Epoch 38/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.3055 - acc: 0.8601\n",
      "Epoch 39/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.3096 - acc: 0.8566\n",
      "Epoch 40/170\n",
      "286/286 [==============================] - 0s 119us/step - loss: 0.3106 - acc: 0.8531\n",
      "Epoch 41/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.3065 - acc: 0.8706\n",
      "Epoch 42/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.2675 - acc: 0.8776\n",
      "Epoch 43/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.3021 - acc: 0.8497\n",
      "Epoch 44/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.2453 - acc: 0.9091\n",
      "Epoch 45/170\n",
      "286/286 [==============================] - 0s 130us/step - loss: 0.2506 - acc: 0.8916\n",
      "Epoch 46/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2905 - acc: 0.8531\n",
      "Epoch 47/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.2439 - acc: 0.8986\n",
      "Epoch 48/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.2923 - acc: 0.8497\n",
      "Epoch 49/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2650 - acc: 0.8986\n",
      "Epoch 50/170\n",
      "286/286 [==============================] - 0s 128us/step - loss: 0.2908 - acc: 0.8671\n",
      "Epoch 51/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.2676 - acc: 0.8916\n",
      "Epoch 52/170\n",
      "286/286 [==============================] - 0s 127us/step - loss: 0.2337 - acc: 0.9161\n",
      "Epoch 53/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2847 - acc: 0.8881\n",
      "Epoch 54/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.2604 - acc: 0.8951\n",
      "Epoch 55/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.2428 - acc: 0.8986\n",
      "Epoch 56/170\n",
      "286/286 [==============================] - 0s 118us/step - loss: 0.2403 - acc: 0.9091\n",
      "Epoch 57/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.2201 - acc: 0.9161\n",
      "Epoch 58/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.2459 - acc: 0.8986\n",
      "Epoch 59/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2505 - acc: 0.8741\n",
      "Epoch 60/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.2284 - acc: 0.8951\n",
      "Epoch 61/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.2603 - acc: 0.8951\n",
      "Epoch 62/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1747 - acc: 0.9266\n",
      "Epoch 63/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2380 - acc: 0.8951\n",
      "Epoch 64/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.2261 - acc: 0.9056\n",
      "Epoch 65/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.2682 - acc: 0.8741\n",
      "Epoch 66/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.2454 - acc: 0.8811\n",
      "Epoch 67/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.2265 - acc: 0.8951\n",
      "Epoch 68/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.2180 - acc: 0.9056\n",
      "Epoch 69/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1834 - acc: 0.9161\n",
      "Epoch 70/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2766 - acc: 0.8636\n",
      "Epoch 71/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.2067 - acc: 0.9196\n",
      "Epoch 72/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.1977 - acc: 0.9056\n",
      "Epoch 73/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1883 - acc: 0.9161\n",
      "Epoch 74/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1828 - acc: 0.9266\n",
      "Epoch 75/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1991 - acc: 0.9371\n",
      "Epoch 76/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.2379 - acc: 0.8881\n",
      "Epoch 77/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1788 - acc: 0.9161\n",
      "Epoch 78/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2028 - acc: 0.9161\n",
      "Epoch 79/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.2061 - acc: 0.9091\n",
      "Epoch 80/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.2287 - acc: 0.9161\n",
      "Epoch 81/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1594 - acc: 0.9161\n",
      "Epoch 82/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.2035 - acc: 0.9126\n",
      "Epoch 83/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 0s 120us/step - loss: 0.1935 - acc: 0.9126\n",
      "Epoch 84/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1953 - acc: 0.9161\n",
      "Epoch 85/170\n",
      "286/286 [==============================] - 0s 119us/step - loss: 0.2447 - acc: 0.9056\n",
      "Epoch 86/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1955 - acc: 0.9266\n",
      "Epoch 87/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1684 - acc: 0.9266\n",
      "Epoch 88/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.1759 - acc: 0.9196\n",
      "Epoch 89/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.1629 - acc: 0.9266\n",
      "Epoch 90/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.2050 - acc: 0.9091\n",
      "Epoch 91/170\n",
      "286/286 [==============================] - 0s 127us/step - loss: 0.1726 - acc: 0.9301\n",
      "Epoch 92/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1695 - acc: 0.9231\n",
      "Epoch 93/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1664 - acc: 0.9371\n",
      "Epoch 94/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1855 - acc: 0.9161\n",
      "Epoch 95/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1933 - acc: 0.9336\n",
      "Epoch 96/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1797 - acc: 0.9056\n",
      "Epoch 97/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1785 - acc: 0.8916\n",
      "Epoch 98/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.1732 - acc: 0.9406\n",
      "Epoch 99/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1475 - acc: 0.9301\n",
      "Epoch 100/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1664 - acc: 0.9266\n",
      "Epoch 101/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1699 - acc: 0.9301\n",
      "Epoch 102/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1985 - acc: 0.9021\n",
      "Epoch 103/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1971 - acc: 0.9196\n",
      "Epoch 104/170\n",
      "286/286 [==============================] - 0s 127us/step - loss: 0.1455 - acc: 0.9476\n",
      "Epoch 105/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.2017 - acc: 0.9056\n",
      "Epoch 106/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1643 - acc: 0.9441\n",
      "Epoch 107/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1618 - acc: 0.9231\n",
      "Epoch 108/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1482 - acc: 0.9441\n",
      "Epoch 109/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1426 - acc: 0.9476\n",
      "Epoch 110/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1911 - acc: 0.9266\n",
      "Epoch 111/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1389 - acc: 0.9510\n",
      "Epoch 112/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1772 - acc: 0.9336\n",
      "Epoch 113/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1207 - acc: 0.9615\n",
      "Epoch 114/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1900 - acc: 0.9196\n",
      "Epoch 115/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1470 - acc: 0.9510\n",
      "Epoch 116/170\n",
      "286/286 [==============================] - 0s 119us/step - loss: 0.1809 - acc: 0.9231\n",
      "Epoch 117/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1457 - acc: 0.9441\n",
      "Epoch 118/170\n",
      "286/286 [==============================] - 0s 127us/step - loss: 0.1432 - acc: 0.9406\n",
      "Epoch 119/170\n",
      "286/286 [==============================] - 0s 127us/step - loss: 0.1374 - acc: 0.9406\n",
      "Epoch 120/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1782 - acc: 0.9126\n",
      "Epoch 121/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1916 - acc: 0.9091\n",
      "Epoch 122/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1549 - acc: 0.9301\n",
      "Epoch 123/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1580 - acc: 0.9301\n",
      "Epoch 124/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.1561 - acc: 0.9161\n",
      "Epoch 125/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1558 - acc: 0.9406\n",
      "Epoch 126/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1615 - acc: 0.9371\n",
      "Epoch 127/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1472 - acc: 0.9371\n",
      "Epoch 128/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1482 - acc: 0.9476\n",
      "Epoch 129/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1439 - acc: 0.9580\n",
      "Epoch 130/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1480 - acc: 0.9441\n",
      "Epoch 131/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.1298 - acc: 0.9510\n",
      "Epoch 132/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1233 - acc: 0.9406\n",
      "Epoch 133/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.1052 - acc: 0.9580\n",
      "Epoch 134/170\n",
      "286/286 [==============================] - 0s 127us/step - loss: 0.1377 - acc: 0.9406\n",
      "Epoch 135/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1756 - acc: 0.9266\n",
      "Epoch 136/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.1118 - acc: 0.9685\n",
      "Epoch 137/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1258 - acc: 0.9441\n",
      "Epoch 138/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1464 - acc: 0.9441\n",
      "Epoch 139/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1315 - acc: 0.9510\n",
      "Epoch 140/170\n",
      "286/286 [==============================] - 0s 118us/step - loss: 0.1402 - acc: 0.9406\n",
      "Epoch 141/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1014 - acc: 0.9650\n",
      "Epoch 142/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1405 - acc: 0.9371\n",
      "Epoch 143/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1111 - acc: 0.9510\n",
      "Epoch 144/170\n",
      "286/286 [==============================] - 0s 128us/step - loss: 0.0972 - acc: 0.9685\n",
      "Epoch 145/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1483 - acc: 0.9231\n",
      "Epoch 146/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.1395 - acc: 0.9406\n",
      "Epoch 147/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1080 - acc: 0.9510\n",
      "Epoch 148/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1108 - acc: 0.9441\n",
      "Epoch 149/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1215 - acc: 0.9476\n",
      "Epoch 150/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1704 - acc: 0.9231\n",
      "Epoch 151/170\n",
      "286/286 [==============================] - 0s 125us/step - loss: 0.1107 - acc: 0.9580\n",
      "Epoch 152/170\n",
      "286/286 [==============================] - 0s 117us/step - loss: 0.1367 - acc: 0.9406\n",
      "Epoch 153/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1058 - acc: 0.9615\n",
      "Epoch 154/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1370 - acc: 0.9441\n",
      "Epoch 155/170\n",
      "286/286 [==============================] - 0s 130us/step - loss: 0.1433 - acc: 0.9371\n",
      "Epoch 156/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1668 - acc: 0.9371\n",
      "Epoch 157/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1406 - acc: 0.9406\n",
      "Epoch 158/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1415 - acc: 0.9476\n",
      "Epoch 159/170\n",
      "286/286 [==============================] - 0s 119us/step - loss: 0.1314 - acc: 0.9441\n",
      "Epoch 160/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.1093 - acc: 0.9545\n",
      "Epoch 161/170\n",
      "286/286 [==============================] - 0s 124us/step - loss: 0.1120 - acc: 0.9545\n",
      "Epoch 162/170\n",
      "286/286 [==============================] - 0s 128us/step - loss: 0.0872 - acc: 0.9650\n",
      "Epoch 163/170\n",
      "286/286 [==============================] - 0s 122us/step - loss: 0.1312 - acc: 0.9545\n",
      "Epoch 164/170\n",
      "286/286 [==============================] - 0s 123us/step - loss: 0.1588 - acc: 0.9406\n",
      "Epoch 165/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 0s 121us/step - loss: 0.1248 - acc: 0.9476\n",
      "Epoch 166/170\n",
      "286/286 [==============================] - 0s 119us/step - loss: 0.0927 - acc: 0.9720\n",
      "Epoch 167/170\n",
      "286/286 [==============================] - 0s 126us/step - loss: 0.1469 - acc: 0.9441\n",
      "Epoch 168/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1238 - acc: 0.9545\n",
      "Epoch 169/170\n",
      "286/286 [==============================] - 0s 120us/step - loss: 0.1531 - acc: 0.9371\n",
      "Epoch 170/170\n",
      "286/286 [==============================] - 0s 121us/step - loss: 0.1540 - acc: 0.9441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ae9ceb400>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainYO = to_categorical(trainY, num_classes=Yclasses)\n",
    "testYO = to_categorical(testY, num_classes=Yclasses)\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "model.fit(trainX, trainYO, epochs=170)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trainYO = to_categorical(trainY, num_classes=Yclasses)\n",
    "testYO = to_categorical(testY, num_classes=Yclasses)\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(trainX, trainYO, epochs=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 0s 3ms/step\n",
      "[3.3953992377880007, 0.4883720930232558]\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testX, testYO)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 0s 67us/step\n",
      "Test loss: 0.052586617690699916\n",
      "Test accuracy: 0.986013986013986\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(trainX, trainYO, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4883720930232558\n",
      "0.986013986013986\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Keras reported accuracy:\n",
    "score = model.evaluate(testX,testYO, verbose=0) \n",
    "print(score[1])\n",
    "# 0.98580000000000001\n",
    "\n",
    "# Actual accuracy calculated manually:\n",
    "predY= model.predict(trainX)\n",
    "acc = sum([numpy.argmax(trainYO[i])==numpy.argmax(predY[i]) for i in range(len(trainYO))])/len(trainYO)\n",
    "print(acc)\n",
    "# 0.98580000000000001\n",
    "\n",
    "print(score[1]==acc)\n",
    "# True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict1 = model.predict(trainX)\n",
    "testPredict1 = model.predict(testX)\n",
    "#trainPredict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax(trainPredict)\n",
    "trainPredict=numpy.argmax(trainPredict1,1)\n",
    "testPredict=numpy.argmax(testPredict1,1)\n",
    "diffY=testPredict-testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(diffY)\n",
    "plt.plot(testY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# invert predictions\n",
    "#trainPredict = scaler.inverse_transform(trainPredict)\n",
    "#trainY = scaler.inverse_transform([trainY])\n",
    "#testPredict = scaler.inverse_transform(testPredict)\n",
    "#testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 5.460 RMSE\n",
      "Test Score: 5.301 RMSE\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY, trainPredict))\n",
    "print('Train Score: %.3f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY, testPredict))\n",
    "print('Test Score: %.3f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578, 256, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'look_back' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-44853d77e2ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlook_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainPredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# shift test predictions for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtestPredictPlot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'look_back' is not defined"
     ]
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "print(trainPredictPlot.shape)\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, 0] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)-7:len(dataset)-9, 0] = testPredict\n",
    "# plot baseline and predictions\n",
    "#plt.plot(scaler.inverse_transform(dataset),label=\"set\")\n",
    "plt.plot(trainPredictPlot,label=\"train\")\n",
    "plt.plot(testPredictPlot,label=\"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (387) into shape (387,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-2d6065407b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainPredictPlot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwws\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainPredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwws\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainPredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mwws\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# shift test predictions for plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (387) into shape (387,1)"
     ]
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "wws=2;\n",
    "trainPredictPlot[wws:len(trainPredict)+wws,0] = trainPredict\n",
    "wws=60\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+wws+1:len(dataset)-look_back-1, 0] = testPredict\n",
    "# plot baseline and predictions\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(trainY,label=\"set\")\n",
    "ax1.plot(trainPredictPlot,label=\"train\")\n",
    "ax2.plot(testPredictPlot,label=\"testm\")\n",
    "#plt.figure(figsize=(15,3))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
