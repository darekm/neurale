{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from time import time\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.layers import Layer, InputSpec,Input\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Conv1D,  Dense, Flatten, Reshape\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from datasets import load_mnist, load_usps, load_mrec\n",
    "\n",
    "#import tensorflow.eager as tfe\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0716 18:36:18.565547 140591974237952 cross_device_ops.py:1111] Not all devices in `tf.distribute.Strategy` are visible to TensorFlow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.0.0-alpha0\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GLOBAL_BATCH_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-296e8f8722f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Number of devices: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_replicas_in_sync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOBAL_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'GLOBAL_BATCH_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "print(tf.executing_eagerly())\n",
    "print(tf.__version__)\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "print(GLOBAL_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics\n",
    "\n",
    "def printMetrics(aName,yy,_loss):\n",
    "        if yHid is not None:\n",
    "                    acc = np.round(metrics.acc(yHid, yy), 5)\n",
    "                    nmi = np.round(metrics.nmi(yHid, yy), 5)\n",
    "                    ari = np.round(metrics.ari(yHid, yy), 5)\n",
    "                    loss = np.round(_loss, 7)\n",
    "                    #logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, L=_loss[0], Lc=_loss[1], Lr=_loss[2])\n",
    "                    #logwriter.writerow(logdict)\n",
    "                    # print('Iter', ite, ': Acc', acc, ', nmi', nmi, ', ari', ari, '; loss=', loss,'  delta=',delta_label)\n",
    "\n",
    "                    print(aName,'acc = %.4f, nmi = %.4f, ari = %.4f' % (acc,nmi,ari),';  loss=',_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "Ndataset='mnist-test'\n",
    "Nclusters=16\n",
    "Nsave_dir='temp'\n",
    "Ngamma=0.1\n",
    "Ntol=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST: (70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_mnist, load_usps, load_mrec\n",
    "if Ndataset == 'mnist':\n",
    "    xData, yHid = load_mnist()\n",
    "elif Ndataset == 'usps':\n",
    "    xData, yHid = load_usps('data/usps')\n",
    "elif Ndataset == 'mrec':\n",
    "    xData, YY = load_mrec()\n",
    "elif Ndataset == 'mnist-test':\n",
    "    xData, yHid = load_mnist()\n",
    "    xData, yHid = xData[60000:], yHid[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oneHot(y):\n",
    "        e = np.zeros(10)\n",
    "        e[y] = 1\n",
    "        return e\n",
    "train_Y10 = np.array([oneHot(y) for y in yHid])\n",
    "train_Y10.shape\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdataset = tf.data.Dataset.from_tensor_slices((xData, train_Y10))\n",
    "#test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        #print(\"__init__\")\n",
    "        # the data, shuffled and split between train and test sets\n",
    "        from tensorflow.keras.datasets import mnist\n",
    "        (train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #x = x_train.reshape(-1, 784).astype('float32')\n",
    "        train_x = train_x/255.\n",
    "        test_x=test_x/255.\n",
    "\n",
    "        #with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
    "        #          train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "        # with open(\"../input/mnist.pkl/mnist.pkl\",mode=\"rb\") as f:\n",
    "        #          tr_d, va_d, te_d = pickle.load(f,encoding=\"bytes\")\n",
    "        #train_x = tr_d[0]\n",
    "        #train_y = tr_d[1]\n",
    "        self.train_x = np.array([np.reshape(X,(784)) for X in train_x]) \n",
    "        self.train_y = np.array([self.oneHot(y) for y in train_y])\n",
    "\n",
    "            \n",
    "        #    validation_x = va_d[0]\n",
    "        #    validation_y = va_d[1]\n",
    "        #    self.validation_x = np.array([np.reshape(X,(784)) for X in validation_x])\n",
    "        #    self.validation_y = np.array([self.oneHot(y) for y in validation_y])\n",
    "            \n",
    "        #test_x = te_d[0]\n",
    "        #test_y = te_d[1]\n",
    "        self.test_x = np.array([np.reshape(X,(784)) for X in test_x])\n",
    "        self.test_y = np.array([self.oneHot(y) for y in test_y])\n",
    "        print('trainx',self.train_x.shape)\n",
    "        print('trainy',self.train_y.shape)\n",
    "\n",
    "    def oneHot(self,y):\n",
    "        e = np.zeros(10)\n",
    "        e[y] = 1\n",
    "        return e\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        index = np.random.randint(0,len(self.train_x),batch_size)\n",
    "        #print(index)\n",
    "        print(type(self.train_x))\n",
    "        return self.train_x[index,:],self.train_y[index]\n",
    "    \n",
    "    def getTestData(self):\n",
    "        return self.test_x,self.test_y\n",
    "  \n",
    "\n",
    "    def get_batch_test(self, batch_size):\n",
    "        index = np.random.randint(0,len(self.test_x),batch_size)\n",
    "        #print(index)\n",
    "        #print(type(self.train_x))\n",
    "        return self.test_x[index],self.test_y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xdataset = x.shuffle(1000).batch(32)\n",
    "#dataset = DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,     input_shape=(28, 28, 1), filters=[32, 64, 128, 10]):\n",
    "        super(Encoder, self).__init__()\n",
    "        #self.w='name'\n",
    "        #self.ishape=input_shape\n",
    "        self.filters=filters\n",
    "        #self.pad3\n",
    "        # model = Sequential()\n",
    "        if input_shape[0] % 8 == 0:\n",
    "            pad3 = 'same'\n",
    "        else:\n",
    "            pad3 = 'valid'\n",
    "            \n",
    "        self.conv0= Conv2D(self.filters[0], 5, strides=(2,2), padding='same', activation='relu', name='conv0', input_shape=input_shape)\n",
    "        self.conv1= Conv2D(self.filters[1], 5, strides=(2,2), padding='same', activation='relu', name='conv1')\n",
    "        self.conv2= Conv2D(self.filters[2], 3, strides=2, padding=pad3, activation='relu', name='conv2')\n",
    "        self.flat= Flatten()\n",
    "        self.emb=Dense(units=self.filters[3], name='embedding')\n",
    "         \n",
    "               \n",
    "    def call(self, x, training=True):\n",
    "        \n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flat(x)\n",
    "        return self.emb(x)\n",
    "    \n",
    "         \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "     def __init__(self,   flatten=(10,) ,  output_shape=(28,28,1), filters=[32, 64, 128, 10]):\n",
    "        super(Decoder, self).__init__()\n",
    "        if output_shape[0] % 8 == 0:\n",
    "            pad3 = 'same'\n",
    "        else:\n",
    "            pad3 = 'valid'\n",
    "        #self.ishape=input_shape\n",
    "        self.filters=filters\n",
    "      \n",
    "        self.dense1=Dense(units=self.filters[2]*int(output_shape[0]/8)*int(output_shape[1]/8),input_shape=flatten , activation='relu')\n",
    "\n",
    "        self.resh1=Reshape((int(output_shape[0]/8), int(output_shape[1]/8), filters[2]))\n",
    "        self.deconv2=Conv2DTranspose(self.filters[1], 3, strides=2, padding=pad3, activation='relu', name='deconv2')\n",
    "\n",
    "        self.deconv1=Conv2DTranspose(self.filters[0], 5, strides=(2,2), padding='same', activation='relu', name='deconv1')\n",
    "        self.deconv0=Conv2DTranspose(output_shape[2], 5, strides=(2,2), padding='same', name='deconv0')\n",
    "    \n",
    "     def call(self, x, training=True):\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        x = self.resh1(x)\n",
    "     \n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv0(x)       \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters,n_dim, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_dim=n_dim\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        #self.input_spec = InputSpec(ndim=2)\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, n_dim))\n",
    "       \n",
    "      \n",
    "        print('initLC')\n",
    "\n",
    " \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print('buildlc', input_shape)\n",
    "        assert len(input_shape) == 2\n",
    "        \n",
    "        self.clusters = self.add_weight('clusters',(self.n_clusters, self.n_dim),trainable=True, initializer='glorot_uniform')\n",
    "        #input_dim = input_shape[1]\n",
    "        #self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "        super(ClusteringLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/56951218/how-to-calculate-the-gradient-of-the-kullback-leibler-divergence-of-two-tensorfl\n",
    "class KLDLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(KLDLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mean_W = self.add_weight('mean_W',trainable=True)\n",
    "        super(KLDLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        self.kernel_dist = tfp.distributions.MultivariateNormalDiag(\n",
    "            loc=self.mean_W,\n",
    "            scale_diag=(1.,)\n",
    "        )\n",
    "        return tfp.distributions.kl_divergence(\n",
    "            self.kernel_dist,\n",
    "            tfp.distributions.MultivariateNormalDiag(\n",
    "                loc=self.mean_W*0.,\n",
    "                scale_diag=(1.,)\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringModel(tf.keras.Model):\n",
    "   \n",
    "    def __init__(self,     aClusters=10,aDim=10):\n",
    "        super(ClusteringModel, self).__init__()\n",
    "        #self.w='name'\n",
    "        self.emb=ClusteringLayer(n_clusters=aClusters,n_dim=aDim, name='clustering')\n",
    "         \n",
    "               \n",
    "    def call(self, x, training=True):\n",
    "        \n",
    "        return self.emb(x)\n",
    "    \n",
    "    def setWeights(self,aWeights):\n",
    "        self.emb.set_weights(aWeights)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 100\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "nGamma=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "#checkpoint = tf.train.Checkpoint(myModel=model,optimizer=optimizer)\n",
    "g_encoder = Encoder()\n",
    "g_decoder = Decoder()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "xclusters=10\n",
    "#clustering_layer = ClusteringLayer(n_clusters, name='clustering')(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tdataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 28, 28, 1) (256, 10)\n",
      "(256, 28, 28, 1) (256, 10)\n"
     ]
    }
   ],
   "source": [
    "for images,labels in dataset.take(2):\n",
    "\n",
    "    #images,labels = dataset.get_batch(3)\n",
    "    print(images.shape,labels.shape)\n",
    "    #print(\"Logits: \", g_encoder(images[0:1]).numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function to be optimized\n",
    "def loss_mse( inputs, targets):\n",
    "    #error = inputs - targets\n",
    "    #sq=tf.square(error)\n",
    "    sq=tf.keras.losses.MSE(inputs,targets)\n",
    "\n",
    "    #return tf.compute_average_loss(   sq,        global_batch_size=GLOBAL_BATCH_SIZE)\n",
    "    return tf.reduce_mean(sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_kld(inputs,pred):\n",
    "    _nn=tf.keras.losses.KLD( inputs,pred)\n",
    "    return tf.reduce_mean(_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_sm( inputs, y):\n",
    "    return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          logits=y, labels=inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "def train(model,X_train, Y_train, epochs):\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "  \n",
    "    for i in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            curr_loss = loss(model,X_train, Y_train)\n",
    "        grads = tape.gradient( curr_loss, model.variables )\n",
    "        optimizer.apply_gradients(zip(grads, model.variables),        global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    print(\"Loss at step {:d}: {:.3f}\".format(i, loss(model, X_train , Y_train)))\n",
    "    \n",
    "print('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_train():\n",
    "    #start = time.time()\n",
    "    loss = 0\n",
    "    xoptimizer = tf.optimizers.Adam()\n",
    "\n",
    "    for (batch, (x, labels)) in enumerate(dataset.take(-1)):\n",
    "        if batch % 10 == 0:\n",
    "             print('.',end='')\n",
    "        #    print('Batch {} loss{:.3f}'.format(batch,loss) )\n",
    "    \n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_encoder.trainable = True\n",
    "            z = g_encoder(x, training=True)\n",
    "            #x_hat = g_decoder(z, training=True)\n",
    "            loss = loss_sm(labels, z)\n",
    "                       \n",
    "        variables = g_encoder.trainable_variables\n",
    "        #for  v in variables:\n",
    "        #    print (\"batch: \", v.name,'  ',v.shape)\n",
    "        #print('nam:',g_encoder.trainable_variables.shape)\n",
    "        #print('var:',variables)\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        xoptimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....Epoch 1  Loss 0.0180\n",
      "pre:  acc = 0.9525, nmi = 0.8870, ari = 0.8979 ;  loss= tf.Tensor(0.018010827, shape=(), dtype=float32)\n",
      "....Epoch 2  Loss 0.0008\n",
      "pre:  acc = 0.9791, nmi = 0.9445, ari = 0.9542 ;  loss= tf.Tensor(0.00084503414, shape=(), dtype=float32)\n",
      "....Epoch 3  Loss 0.0003\n",
      "pre:  acc = 0.9869, nmi = 0.9637, ari = 0.9712 ;  loss= tf.Tensor(0.00034566384, shape=(), dtype=float32)\n",
      "....Epoch 4  Loss 0.0002\n",
      "pre:  acc = 0.9899, nmi = 0.9717, ari = 0.9778 ;  loss= tf.Tensor(0.00015162281, shape=(), dtype=float32)\n",
      "....Epoch 5  Loss 0.0001\n",
      "pre:  acc = 0.9907, nmi = 0.9735, ari = 0.9795 ;  loss= tf.Tensor(7.212526e-05, shape=(), dtype=float32)\n",
      "....Epoch 6  Loss 0.0000\n",
      "pre:  acc = 0.9914, nmi = 0.9757, ari = 0.9806 ;  loss= tf.Tensor(2.4859657e-05, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 6\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    ls=classification_train()\n",
    "    print('Epoch {}  Loss {:.4f}'.format(epoch + 1,ls.numpy()))\n",
    "    nn=g_encoder.predict(xData)\n",
    "    nnp=nn.argmax(1)\n",
    "    \n",
    "    printMetrics('pre: ',nnp,ls)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -9.047852   -2.4791203  16.472572   -6.0599923 -20.862007  -13.150835\n",
      "  -3.2002225 -18.769917   -2.4611874 -25.569584 ] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "nn=g_encoder.predict(xData)\n",
    "print(nn[1],train_Y10[1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train():\n",
    "    #start = time.time()\n",
    "    loss = 0\n",
    "    xoptimizer = tf.optimizers.Adam()\n",
    "    \n",
    "    for (batch, (x, labels)) in enumerate(dataset.take(-1)):\n",
    "        #if batch % 10 == 0:\n",
    "        #print('mse',x.shape)\n",
    "                 \n",
    "        #     print('.',end='')\n",
    "        #    print('Batch {} loss{:.3f}'.format(batch,loss) )\n",
    "    \n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_encoder.trainable = True\n",
    "            g_decoder.trainable = True\n",
    "            z = g_encoder(x, training=True)\n",
    "            x_hat = g_decoder(z, training=True)\n",
    "            loss = loss_mse(x, x_hat)\n",
    "            #print('mse',x.shape, loss.numpy())\n",
    "                  \n",
    "        #for  v in variables:\n",
    "        #    print (\"batch: \", v.name,'  ',v.shape)\n",
    "        #print('nam:',g_encoder.trainable_variables.shape)\n",
    "        #print('var:',variables)\n",
    "        variables = g_encoder.trainable_variables + g_decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        xoptimizer.apply_gradients(zip(gradients, variables))\n",
    "        #xoptimizer.minimize(loss,variables)\n",
    "    return loss\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse (256, 28, 28, 1) 0.009252087\n",
      "mse (256, 28, 28, 1) 0.014913804\n",
      "mse (256, 28, 28, 1) 0.011826583\n",
      "mse (256, 28, 28, 1) 0.013954412\n",
      "mse (256, 28, 28, 1) 0.012982361\n",
      "mse (256, 28, 28, 1) 0.013176286\n",
      "mse (256, 28, 28, 1) 0.012904429\n",
      "mse (256, 28, 28, 1) 0.012787782\n",
      "mse (256, 28, 28, 1) 0.012925306\n",
      "mse (256, 28, 28, 1) 0.012314297\n",
      "mse (256, 28, 28, 1) 0.012462001\n",
      "mse (256, 28, 28, 1) 0.012033183\n",
      "mse (256, 28, 28, 1) 0.012018196\n",
      "mse (256, 28, 28, 1) 0.012141665\n",
      "mse (256, 28, 28, 1) 0.013362712\n",
      "mse (256, 28, 28, 1) 0.012706832\n",
      "mse (256, 28, 28, 1) 0.01245947\n",
      "mse (256, 28, 28, 1) 0.012339722\n",
      "mse (256, 28, 28, 1) 0.0119373165\n",
      "mse (256, 28, 28, 1) 0.011489263\n",
      "mse (256, 28, 28, 1) 0.011398189\n",
      "mse (256, 28, 28, 1) 0.011284992\n",
      "mse (256, 28, 28, 1) 0.014093808\n",
      "mse (256, 28, 28, 1) 0.012978867\n",
      "mse (256, 28, 28, 1) 0.012013935\n",
      "mse (256, 28, 28, 1) 0.012767707\n",
      "mse (256, 28, 28, 1) 0.01219884\n",
      "mse (256, 28, 28, 1) 0.010725406\n",
      "mse (256, 28, 28, 1) 0.011551959\n",
      "mse (256, 28, 28, 1) 0.012148609\n",
      "mse (256, 28, 28, 1) 0.013929276\n",
      "mse (256, 28, 28, 1) 0.013190051\n",
      "mse (256, 28, 28, 1) 0.015215543\n",
      "mse (256, 28, 28, 1) 0.013710184\n",
      "mse (256, 28, 28, 1) 0.010599966\n",
      "mse (256, 28, 28, 1) 0.01470008\n",
      "mse (256, 28, 28, 1) 0.012063443\n",
      "mse (256, 28, 28, 1) 0.016516054\n",
      "mse (256, 28, 28, 1) 0.014715156\n",
      "mse (16, 28, 28, 1) 0.01225963\n",
      "Epoch 1  Loss 0.0123\n",
      "pre:  acc = 0.4234, nmi = 0.3474, ari = 0.2044 ;  loss= tf.Tensor(0.01225963, shape=(), dtype=float32)\n",
      "mse (256, 28, 28, 1) 0.009213878\n",
      "mse (256, 28, 28, 1) 0.0138719985\n",
      "mse (256, 28, 28, 1) 0.011974393\n",
      "mse (256, 28, 28, 1) 0.012572601\n",
      "mse (256, 28, 28, 1) 0.012841511\n",
      "mse (256, 28, 28, 1) 0.012701666\n",
      "mse (256, 28, 28, 1) 0.012145718\n",
      "mse (256, 28, 28, 1) 0.012319339\n",
      "mse (256, 28, 28, 1) 0.012647867\n",
      "mse (256, 28, 28, 1) 0.012019835\n",
      "mse (256, 28, 28, 1) 0.0119450735\n",
      "mse (256, 28, 28, 1) 0.011660588\n",
      "mse (256, 28, 28, 1) 0.011684334\n",
      "mse (256, 28, 28, 1) 0.011763188\n",
      "mse (256, 28, 28, 1) 0.013049368\n",
      "mse (256, 28, 28, 1) 0.012349506\n",
      "mse (256, 28, 28, 1) 0.012136055\n",
      "mse (256, 28, 28, 1) 0.012103803\n",
      "mse (256, 28, 28, 1) 0.011733917\n",
      "mse (256, 28, 28, 1) 0.011302887\n",
      "mse (256, 28, 28, 1) 0.011179725\n",
      "mse (256, 28, 28, 1) 0.011087847\n",
      "mse (256, 28, 28, 1) 0.013856684\n",
      "mse (256, 28, 28, 1) 0.012756015\n",
      "mse (256, 28, 28, 1) 0.011784537\n",
      "mse (256, 28, 28, 1) 0.012712157\n",
      "mse (256, 28, 28, 1) 0.012069945\n",
      "mse (256, 28, 28, 1) 0.010544727\n",
      "mse (256, 28, 28, 1) 0.011471189\n",
      "mse (256, 28, 28, 1) 0.012059979\n",
      "mse (256, 28, 28, 1) 0.013825537\n",
      "mse (256, 28, 28, 1) 0.013148163\n",
      "mse (256, 28, 28, 1) 0.015223829\n",
      "mse (256, 28, 28, 1) 0.013797689\n",
      "mse (256, 28, 28, 1) 0.010489999\n",
      "mse (256, 28, 28, 1) 0.014491591\n",
      "mse (256, 28, 28, 1) 0.012008266\n",
      "mse (256, 28, 28, 1) 0.016370697\n",
      "mse (256, 28, 28, 1) 0.014717767\n",
      "mse (16, 28, 28, 1) 0.012095706\n",
      "Epoch 2  Loss 0.0121\n",
      "pre:  acc = 0.4241, nmi = 0.3487, ari = 0.2052 ;  loss= tf.Tensor(0.012095706, shape=(), dtype=float32)\n",
      "mse (256, 28, 28, 1) 0.00911748\n",
      "mse (256, 28, 28, 1) 0.015310982\n",
      "mse (256, 28, 28, 1) 0.011566882\n",
      "mse (256, 28, 28, 1) 0.015058\n",
      "mse (256, 28, 28, 1) 0.01303936\n",
      "mse (256, 28, 28, 1) 0.013079888\n",
      "mse (256, 28, 28, 1) 0.013416155\n",
      "mse (256, 28, 28, 1) 0.013499643\n",
      "mse (256, 28, 28, 1) 0.01312441\n",
      "mse (256, 28, 28, 1) 0.012137544\n",
      "mse (256, 28, 28, 1) 0.012788265\n",
      "mse (256, 28, 28, 1) 0.012719201\n",
      "mse (256, 28, 28, 1) 0.012045602\n",
      "mse (256, 28, 28, 1) 0.011896013\n",
      "mse (256, 28, 28, 1) 0.013539689\n",
      "mse (256, 28, 28, 1) 0.01312957\n",
      "mse (256, 28, 28, 1) 0.012707333\n",
      "mse (256, 28, 28, 1) 0.012224022\n",
      "mse (256, 28, 28, 1) 0.011868295\n",
      "mse (256, 28, 28, 1) 0.011683973\n",
      "mse (256, 28, 28, 1) 0.01153324\n",
      "mse (256, 28, 28, 1) 0.011232617\n",
      "mse (256, 28, 28, 1) 0.013909921\n",
      "mse (256, 28, 28, 1) 0.0129103465\n",
      "mse (256, 28, 28, 1) 0.0122173475\n",
      "mse (256, 28, 28, 1) 0.012989035\n",
      "mse (256, 28, 28, 1) 0.012291003\n",
      "mse (256, 28, 28, 1) 0.010623075\n",
      "mse (256, 28, 28, 1) 0.011498831\n",
      "mse (256, 28, 28, 1) 0.012211\n",
      "mse (256, 28, 28, 1) 0.014222045\n",
      "mse (256, 28, 28, 1) 0.013189912\n",
      "mse (256, 28, 28, 1) 0.0152315665\n",
      "mse (256, 28, 28, 1) 0.013880404\n",
      "mse (256, 28, 28, 1) 0.010654354\n",
      "mse (256, 28, 28, 1) 0.014787194\n",
      "mse (256, 28, 28, 1) 0.012156394\n",
      "mse (256, 28, 28, 1) 0.016610282\n",
      "mse (256, 28, 28, 1) 0.014701566\n",
      "mse (16, 28, 28, 1) 0.012516847\n",
      "Epoch 3  Loss 0.0125\n",
      "pre:  acc = 0.4300, nmi = 0.3506, ari = 0.2091 ;  loss= tf.Tensor(0.012516847, shape=(), dtype=float32)\n",
      "mse (256, 28, 28, 1) 0.0091484105\n",
      "mse (256, 28, 28, 1) 0.014663697\n",
      "mse (256, 28, 28, 1) 0.011951963\n",
      "mse (256, 28, 28, 1) 0.0130426325\n",
      "mse (256, 28, 28, 1) 0.012753511\n",
      "mse (256, 28, 28, 1) 0.01272669\n",
      "mse (256, 28, 28, 1) 0.012251068\n",
      "mse (256, 28, 28, 1) 0.012548942\n",
      "mse (256, 28, 28, 1) 0.01293257\n",
      "mse (256, 28, 28, 1) 0.012028608\n",
      "mse (256, 28, 28, 1) 0.011865142\n",
      "mse (256, 28, 28, 1) 0.011741368\n",
      "mse (256, 28, 28, 1) 0.01191537\n",
      "mse (256, 28, 28, 1) 0.011882727\n",
      "mse (256, 28, 28, 1) 0.013008067\n",
      "mse (256, 28, 28, 1) 0.01247331\n",
      "mse (256, 28, 28, 1) 0.012290085\n",
      "mse (256, 28, 28, 1) 0.012243827\n",
      "mse (256, 28, 28, 1) 0.011765499\n",
      "mse (256, 28, 28, 1) 0.011311006\n",
      "mse (256, 28, 28, 1) 0.011283108\n",
      "mse (256, 28, 28, 1) 0.011215444\n",
      "mse (256, 28, 28, 1) 0.01399418\n",
      "mse (256, 28, 28, 1) 0.012790755\n",
      "mse (256, 28, 28, 1) 0.0118064815\n",
      "mse (256, 28, 28, 1) 0.012774884\n",
      "mse (256, 28, 28, 1) 0.012158582\n",
      "mse (256, 28, 28, 1) 0.010599232\n",
      "mse (256, 28, 28, 1) 0.011431802\n",
      "mse (256, 28, 28, 1) 0.012046573\n",
      "mse (256, 28, 28, 1) 0.013857\n",
      "mse (256, 28, 28, 1) 0.01315018\n",
      "mse (256, 28, 28, 1) 0.01518222\n",
      "mse (256, 28, 28, 1) 0.013647925\n",
      "mse (256, 28, 28, 1) 0.01047635\n",
      "mse (256, 28, 28, 1) 0.014682911\n",
      "mse (256, 28, 28, 1) 0.012107667\n",
      "mse (256, 28, 28, 1) 0.016500471\n",
      "mse (256, 28, 28, 1) 0.014689516\n",
      "mse (16, 28, 28, 1) 0.012010932\n",
      "Epoch 4  Loss 0.0120\n",
      "pre:  acc = 0.4285, nmi = 0.3514, ari = 0.2078 ;  loss= tf.Tensor(0.012010932, shape=(), dtype=float32)\n",
      "mse (256, 28, 28, 1) 0.00905525\n",
      "mse (256, 28, 28, 1) 0.012621311\n",
      "mse (256, 28, 28, 1) 0.012609621\n",
      "mse (256, 28, 28, 1) 0.013392301\n",
      "mse (256, 28, 28, 1) 0.012619687\n",
      "mse (256, 28, 28, 1) 0.012318957\n",
      "mse (256, 28, 28, 1) 0.01278369\n",
      "mse (256, 28, 28, 1) 0.012798783\n",
      "mse (256, 28, 28, 1) 0.012549094\n",
      "mse (256, 28, 28, 1) 0.01200185\n",
      "mse (256, 28, 28, 1) 0.012111955\n",
      "mse (256, 28, 28, 1) 0.01167789\n",
      "mse (256, 28, 28, 1) 0.011749086\n",
      "mse (256, 28, 28, 1) 0.011823287\n",
      "mse (256, 28, 28, 1) 0.012844141\n",
      "mse (256, 28, 28, 1) 0.012369723\n",
      "mse (256, 28, 28, 1) 0.012272851\n",
      "mse (256, 28, 28, 1) 0.012148436\n",
      "mse (256, 28, 28, 1) 0.011561143\n",
      "mse (256, 28, 28, 1) 0.011183008\n",
      "mse (256, 28, 28, 1) 0.011201917\n",
      "mse (256, 28, 28, 1) 0.011133379\n",
      "mse (256, 28, 28, 1) 0.01375752\n",
      "mse (256, 28, 28, 1) 0.012586098\n",
      "mse (256, 28, 28, 1) 0.011634517\n",
      "mse (256, 28, 28, 1) 0.012570194\n",
      "mse (256, 28, 28, 1) 0.012052965\n",
      "mse (256, 28, 28, 1) 0.010418547\n",
      "mse (256, 28, 28, 1) 0.011257661\n",
      "mse (256, 28, 28, 1) 0.011946689\n",
      "mse (256, 28, 28, 1) 0.013745637\n",
      "mse (256, 28, 28, 1) 0.013019236\n",
      "mse (256, 28, 28, 1) 0.014954501\n",
      "mse (256, 28, 28, 1) 0.013514567\n",
      "mse (256, 28, 28, 1) 0.010438613\n",
      "mse (256, 28, 28, 1) 0.014581759\n",
      "mse (256, 28, 28, 1) 0.011973695\n",
      "mse (256, 28, 28, 1) 0.016423153\n",
      "mse (256, 28, 28, 1) 0.01453706\n",
      "mse (16, 28, 28, 1) 0.011972813\n",
      "Epoch 5  Loss 0.0120\n",
      "pre:  acc = 0.4266, nmi = 0.3499, ari = 0.2081 ;  loss= tf.Tensor(0.011972813, shape=(), dtype=float32)\n",
      "mse (256, 28, 28, 1) 0.008850813\n",
      "mse (256, 28, 28, 1) 0.013011749\n",
      "mse (256, 28, 28, 1) 0.012246044\n",
      "mse (256, 28, 28, 1) 0.011712142\n",
      "mse (256, 28, 28, 1) 0.012249805\n",
      "mse (256, 28, 28, 1) 0.012160428\n",
      "mse (256, 28, 28, 1) 0.011706423\n",
      "mse (256, 28, 28, 1) 0.012003161\n",
      "mse (256, 28, 28, 1) 0.012297041\n",
      "mse (256, 28, 28, 1) 0.011545893\n",
      "mse (256, 28, 28, 1) 0.011512693\n",
      "mse (256, 28, 28, 1) 0.011418772\n",
      "mse (256, 28, 28, 1) 0.011451003\n",
      "mse (256, 28, 28, 1) 0.011383767\n",
      "mse (256, 28, 28, 1) 0.01260973\n",
      "mse (256, 28, 28, 1) 0.012112613\n",
      "mse (256, 28, 28, 1) 0.011958575\n",
      "mse (256, 28, 28, 1) 0.011828189\n",
      "mse (256, 28, 28, 1) 0.011359572\n",
      "mse (256, 28, 28, 1) 0.010971595\n",
      "mse (256, 28, 28, 1) 0.010998018\n",
      "mse (256, 28, 28, 1) 0.010981632\n",
      "mse (256, 28, 28, 1) 0.013588182\n",
      "mse (256, 28, 28, 1) 0.012359631\n",
      "mse (256, 28, 28, 1) 0.01146456\n",
      "mse (256, 28, 28, 1) 0.012404077\n",
      "mse (256, 28, 28, 1) 0.0118854735\n",
      "mse (256, 28, 28, 1) 0.010247579\n",
      "mse (256, 28, 28, 1) 0.01112522\n",
      "mse (256, 28, 28, 1) 0.011791183\n",
      "mse (256, 28, 28, 1) 0.013545584\n",
      "mse (256, 28, 28, 1) 0.012846274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse (256, 28, 28, 1) 0.014885792\n",
      "mse (256, 28, 28, 1) 0.013504935\n",
      "mse (256, 28, 28, 1) 0.010394651\n",
      "mse (256, 28, 28, 1) 0.0143493535\n",
      "mse (256, 28, 28, 1) 0.01186455\n",
      "mse (256, 28, 28, 1) 0.016283672\n",
      "mse (256, 28, 28, 1) 0.014485015\n",
      "mse (16, 28, 28, 1) 0.011880574\n",
      "Epoch 6  Loss 0.0119\n",
      "pre:  acc = 0.4264, nmi = 0.3507, ari = 0.2076 ;  loss= tf.Tensor(0.011880574, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 6\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    ls=pre_train()\n",
    "    print('Epoch {}  Loss {:.4f}'.format(epoch + 1,ls.numpy()))\n",
    "    nn=g_encoder.predict(xData)\n",
    "    nnp=nn.argmax(1)\n",
    "    \n",
    "    printMetrics('pre: ',nnp,ls)\n",
    "  \n",
    "    #print('nam:',g_encoder.trainable_variables)\n",
    "    \n",
    "        \n",
    "    ##variablesx = [v.name for v in g_encoder.trainable_variables]\n",
    "    #print('names',variable_names)\n",
    "    #values = sess.run(variables_names)\n",
    "    #for  v in g_encoder.trainable_variables:\n",
    "    #    print (\"Variable: \", v.name,'  ',v.shape)\n",
    "        #print (\"Shape: \", v.shape)\n",
    "    #    print v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yData = autoencoder.predict(xData)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "zzz = g_encoder.predict(xData)\n",
    "yHat=g_decoder.predict(zzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 18 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    vv=random.randint(1,10000)\n",
    "    plt.imshow(xData[vv].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    #z = g_encoder.predict(xData[i])\n",
    "    #yHat = g_decoder(z, training=True)\n",
    "          \n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(yHat[vv].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing cluster centers with k-means.\n",
      "pred: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Initializing cluster centers with k-means.')\n",
    "kmeans = KMeans(n_clusters=xclusters, n_init=20)\n",
    "y_pr = kmeans.fit_predict(g_encoder.predict(xData))\n",
    "print('pred:',y_pr.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.7153497    0.91084284  -0.21371841   5.1841664   -9.367039\n",
      "   -3.3409793    0.13481784  -2.2584596    4.947014     2.7245884 ]\n",
      " [  5.52756      1.4020389    0.48630834   1.1042687   -3.5142682\n",
      "    4.057763    -0.9080545    4.352916    -6.2820125    6.113641  ]\n",
      " [  2.26693      4.3559337   -1.6558882    3.1227243    2.0488894\n",
      "   -3.8205929   -2.8306785    9.098958     1.8562992    0.29442883]\n",
      " [  3.1102843    2.138294    -8.169355     0.04561639  -1.2647371\n",
      "    4.168779    -0.34173584  -4.2144003    8.476395     3.389319  ]\n",
      " [ 10.486285    -2.5292006   -1.9186387   -1.7270837   -0.7843191\n",
      "   -1.787342    -2.9040074  -11.108117    -1.6634436    1.233525  ]\n",
      " [  5.654563    -1.1140316   -0.53909767  -3.0508392   -1.6572595\n",
      "   -2.3467493   -3.014371     3.1685386    7.554686     5.034936  ]\n",
      " [  2.0027947   -2.6664755   -5.518729    -5.3685327   -1.4950584\n",
      "   -5.38985      0.26576495   3.102964    -4.7899175    5.529003  ]\n",
      " [  7.230487    -3.0378077   -3.9186041    2.5775247    6.10007\n",
      "    1.0313191   -0.67197514   2.8449302    0.88115907   4.4440293 ]\n",
      " [  7.2052355    3.9297497   -2.2641659   -5.958963     4.4254484\n",
      "   -7.3903146    2.49918     -3.382721     0.96597344  -2.847039  ]\n",
      " [  5.5790806   -3.9314744    7.270211     2.0599427   -0.20686826\n",
      "   -8.262188    -4.949712     0.9172108    2.1207228   -0.65728855]]\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initLC\n"
     ]
    }
   ],
   "source": [
    "g_clustering=ClusteringModel(xclusters,xclusters)\n",
    "g_clustering.setWeights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initLC\n",
      "buildlc (None, 10)\n"
     ]
    }
   ],
   "source": [
    "gx = Input(shape=(10,))\n",
    "gfx = ClusteringLayer(10,10)(gx)\n",
    "CLmodel = Model(name='test_Clustering', inputs=[gx], outputs=[gfx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-235-fd5074f6eb0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKLDLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mKLmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test_random'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkfx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-231-8dd537589e6a>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         self.kernel_dist = tfp.distributions.MultivariateNormalDiag(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mscale_diag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfp' is not defined"
     ]
    }
   ],
   "source": [
    "kx = Input(shape=(10,))\n",
    "kfx = KLDLayer()(x)\n",
    "KLmodel = Model(name='test_random', inputs=[kx], outputs=[kfx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000,)\n",
      "Iter0 acc = 0.4264, nmi = 0.3507, ari = 0.2076 ;  loss= 0.0\n"
     ]
    }
   ],
   "source": [
    "nn=g_encoder.predict(xData)\n",
    "#nn.shape\n",
    "nnp = nn.argmax(1)\n",
    "#na=zip(nnp,yHid)\n",
    "print(nnp.shape)\n",
    "print(yHid.shape)\n",
    "printMetrics('Iter0',nnp,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TakeDataset shapes: ((None, 28, 28, 1), (None, 10)), types: (tf.float32, tf.float32)>\n",
      "lab tf.Tensor(\n",
      "[-0.3545655   0.45527846  0.06518746  0.8057229   2.8872395   1.8693975\n",
      "  0.04940075 -1.3665642  -3.2324367  -0.17865996], shape=(10,), dtype=float32)\n",
      "zz tf.Tensor(\n",
      "[[  5.685621     0.80314344   1.414061   ...   3.9176912   -7.3858223\n",
      "    2.8651094 ]\n",
      " [ -2.6615272    7.0134077    7.343408   ...  -3.0310454   10.459287\n",
      "   -3.202344  ]\n",
      " [  1.4785663    1.7134796   -1.8005834  ...   9.335108     1.6361098\n",
      "   -2.4643269 ]\n",
      " ...\n",
      " [  2.947751     1.1345807  -11.101541   ...  -3.5898283    5.4802294\n",
      "    0.5202478 ]\n",
      " [  2.7691753   -0.83559805   0.19727191 ...   4.4953995   -9.073815\n",
      "    5.337539  ]\n",
      " [  2.4589562    2.6170359    1.1064471  ...  10.437503    -4.8946624\n",
      "    3.3066757 ]], shape=(256, 10), dtype=float32)\n",
      "buildlc (256, 10)\n",
      "g2 tf.Tensor(\n",
      "[0.07786503 0.13703214 0.10850964 0.08540079 0.082936   0.08479349\n",
      " 0.10641936 0.14377011 0.09256916 0.08070423], shape=(10,), dtype=float32)\n",
      "kld tf.Tensor(2.725943, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "nn=g_encoder.predict(xData)\n",
    "nnp=nn.argmax(1)\n",
    "    \n",
    "p = target_distribution(nn)  # update the auxiliary target distribution p\n",
    "ndataset = tf.data.Dataset.from_tensor_slices((xData, p)).batch(batch_size)\n",
    "print(ndataset.take(1))\n",
    "for images,labels in ndataset.take(1):\n",
    "\n",
    "    #images,labels = dataset.get_batch(3)\n",
    "    print('lab',labels[0])\n",
    "    zz=g_encoder(images)\n",
    "    print('zz',zz)\n",
    "    z2=g_clustering(labels)\n",
    "    print('g2',z2[0])\n",
    "    print('kld',loss_kld(labels,z2))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_train():\n",
    "    nn2=g_encoder.predict(xData)\n",
    "    nn=g_clustering.predict(nn2)\n",
    "    nnp=nn.argmax(1)\n",
    "    \n",
    "    p = target_distribution(nn)  # update the auxiliary target distribution p\n",
    "    ndataset = tf.data.Dataset.from_tensor_slices((xData, p)).batch(batch_size)\n",
    "    printMetrics('Iter ',nnp,0.1)\n",
    "    #print('x',xData[0])\n",
    "    #print('q',nn[0])\n",
    "    #print('p',p.shape, p[0])\n",
    "    loss = 0\n",
    "    variables = g_encoder.variables + g_decoder.variables+ g_clustering.variables\n",
    "    #variables = g_encoder.variables + g_decoder.variables+ CLmodel.variables\n",
    "         \n",
    "    for (batch, (x, labels)) in enumerate(ndataset.take(-1)):\n",
    "        if batch % 10 == 0:\n",
    "            nn=g_encoder.predict(xData)\n",
    "    \n",
    "            p = target_distribution(nn)  # update the auxiliary target distribution p\n",
    "            yn=CLmodel(nn)\n",
    "            _nn=tf.keras.losses.KLD( p,yn)\n",
    "            print('nn',_nn)\n",
    "        #     print('.',end='')\n",
    "        #    print('Batch {} loss{:.3f}'.format(batch,loss) )\n",
    "    \n",
    "        \n",
    "        #q, _ = self.model.predict(x, verbose=0)\n",
    "        #p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "        #        self.y_pred = q.argmax(1)\n",
    "   \n",
    "        with tf.GradientTape() as tape:\n",
    "            g_encoder.trainable = True\n",
    "            g_decoder.trainable = True\n",
    "            g_clustering.trainable =True\n",
    "            z = g_encoder(x, training=True)\n",
    "            #y_pred = tf.argmax(z,1)\n",
    "            x_hat = g_decoder(z, training=True)\n",
    "            y_hat=CLmodel(z)\n",
    "            y_hat=g_clustering(z,training=True)\n",
    "            #print('yhat',y_hat.numpy(),y_pred.shape, labels.shape)\n",
    "            #id=batch * batch_size\n",
    "            #pll=p[id:(id + batch_size)]\n",
    "            lossmse = loss_mse(x, x_hat)\n",
    "            losskld = loss_kld(labels,y_hat)   \n",
    "            loss=lossmse+losskld*0.2\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            print('lossmse',lossmse.numpy(),':kld',losskld.numpy())\n",
    "            #print('labe=',labels[0])\n",
    "            #print('yhat',y_hat[0])\n",
    "            #print('pll=',pll[0])\n",
    "            #print('kld=',tf.keras.losses.KLD( pll[0],y_hat[0]))\n",
    "            \n",
    "                #print(labels)\n",
    "      \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  acc = 0.6756, nmi = 0.6241, ari = 0.5151 ;  loss= 0.1\n",
      "nn tf.Tensor([1.1618898 2.192871  1.989682  ... 1.609273  1.7562814 2.1360443], shape=(10000,), dtype=float32)\n",
      "lossmse 0.01492223 :kld 0.19502407\n",
      "nn tf.Tensor([10.54277    3.3808966  4.61855   ...  4.7920356  2.29875    2.1230388], shape=(10000,), dtype=float32)\n",
      "lossmse 0.018878127 :kld 0.11098859\n",
      "nn tf.Tensor([4.166155  2.272351  5.1356554 ... 3.8521295 8.705306  2.4652345], shape=(10000,), dtype=float32)\n",
      "lossmse 0.018723648 :kld 0.09062822\n",
      "nn tf.Tensor([2.248446  2.230135  1.7939208 ... 1.9252322 1.7949194 1.4408478], shape=(10000,), dtype=float32)\n",
      "lossmse 0.020329613 :kld 0.050022505\n",
      "Epoch 1  Loss 0.0212\n",
      "Iter  acc = 0.7333, nmi = 0.6743, ari = 0.5820 ;  loss= 0.1\n",
      "nn tf.Tensor([4.4221563 1.7271726 2.7153773 ... 2.9972315 1.5338426 1.0850186], shape=(10000,), dtype=float32)\n",
      "lossmse 0.018276151 :kld 0.2294249\n",
      "nn tf.Tensor([3.2003303 1.6080921 2.43248   ... 7.2629175 2.6519578 1.6636533], shape=(10000,), dtype=float32)\n",
      "lossmse 0.022765229 :kld 0.14293736\n",
      "nn tf.Tensor([6.175175  1.6214242 9.058061  ... 7.836611  2.1780274 1.3362571], shape=(10000,), dtype=float32)\n",
      "lossmse 0.021703808 :kld 0.14966346\n",
      "nn tf.Tensor([ 2.7991366  1.3105017  3.274643  ... 10.069309   3.678624   1.263436 ], shape=(10000,), dtype=float32)\n",
      "lossmse 0.025438484 :kld 0.114981815\n",
      "Epoch 2  Loss 0.0387\n",
      "Iter  acc = 0.7743, nmi = 0.7223, ari = 0.6474 ;  loss= 0.1\n",
      "nn tf.Tensor([2.6556745 1.3849657 2.5176756 ... 4.2043996 4.7620163 1.0591365], shape=(10000,), dtype=float32)\n",
      "lossmse 0.022102049 :kld 0.2104063\n",
      "nn tf.Tensor([2.2421024 1.3051114 2.3794045 ... 7.251189  5.2512827 1.625638 ], shape=(10000,), dtype=float32)\n",
      "lossmse 0.02465845 :kld 0.15277424\n",
      "nn tf.Tensor([4.056944  1.298841  2.5242646 ... 7.4969254 4.3176503 1.3316374], shape=(10000,), dtype=float32)\n",
      "lossmse 0.023391658 :kld 0.16999988\n",
      "nn tf.Tensor([2.54284   1.2971082 2.8614407 ... 6.5404696 5.721656  1.3284246], shape=(10000,), dtype=float32)\n",
      "lossmse 0.029245542 :kld 0.1618492\n",
      "Epoch 3  Loss 0.0474\n",
      "Iter  acc = 0.7962, nmi = 0.7527, ari = 0.6855 ;  loss= 0.1\n",
      "nn tf.Tensor([2.5706646 1.418183  2.2850246 ... 3.354484  9.89541   1.2380838], shape=(10000,), dtype=float32)\n",
      "lossmse 0.024409844 :kld 0.18987806\n",
      "nn tf.Tensor([2.1645494 1.1833514 2.383092  ... 6.254866  6.864741  1.5965374], shape=(10000,), dtype=float32)\n",
      "lossmse 0.025583187 :kld 0.14867288\n",
      "nn tf.Tensor([3.158893  1.1944051 2.3727958 ... 4.737273  9.441519  1.3504809], shape=(10000,), dtype=float32)\n",
      "lossmse 0.025366656 :kld 0.16526708\n",
      "nn tf.Tensor([2.1338768 1.3539101 2.391983  ... 3.849297  9.204786  1.4571321], shape=(10000,), dtype=float32)\n",
      "lossmse 0.028504705 :kld 0.17543617\n",
      "Epoch 4  Loss 0.0485\n",
      "Iter  acc = 0.8115, nmi = 0.7740, ari = 0.7133 ;  loss= 0.1\n",
      "nn tf.Tensor([2.5782833 1.3826946 2.145307  ... 2.1194475 8.880463  1.5427381], shape=(10000,), dtype=float32)\n",
      "lossmse 0.026180085 :kld 0.17380941\n",
      "nn tf.Tensor([1.9499291 1.198719  2.3879967 ... 3.0977304 5.361959  1.5765157], shape=(10000,), dtype=float32)\n",
      "lossmse 0.026163587 :kld 0.14426415\n",
      "nn tf.Tensor([2.6326447 1.1634139 2.3793674 ... 2.836611  3.9785707 1.403067 ], shape=(10000,), dtype=float32)\n",
      "lossmse 0.025461324 :kld 0.15724988\n",
      "nn tf.Tensor([1.9835795 1.3047019 2.3888946 ... 2.4445257 5.1948647 1.3903165], shape=(10000,), dtype=float32)\n",
      "lossmse 0.028331064 :kld 0.18652287\n",
      "Epoch 5  Loss 0.0484\n",
      "Iter  acc = 0.8216, nmi = 0.7898, ari = 0.7315 ;  loss= 0.1\n",
      "nn tf.Tensor([1.8787477 1.3930967 4.867537  ... 2.154738  3.9925683 1.4601914], shape=(10000,), dtype=float32)\n",
      "lossmse 0.02692435 :kld 0.16410375\n",
      "nn tf.Tensor([1.8038161 1.1824359 2.3864908 ... 3.2092316 4.531699  1.5173352], shape=(10000,), dtype=float32)\n",
      "lossmse 0.02665967 :kld 0.14083067\n",
      "nn tf.Tensor([2.2565315 1.1968409 2.383216  ... 2.6833656 4.104368  1.4750766], shape=(10000,), dtype=float32)\n",
      "lossmse 0.024660533 :kld 0.14962192\n",
      "nn tf.Tensor([1.8181174 1.263     2.3897536 ... 2.4984996 5.354599  1.4256427], shape=(10000,), dtype=float32)\n",
      "lossmse 0.028023575 :kld 0.18721947\n",
      "Epoch 6  Loss 0.0443\n",
      "Iter  acc = 0.8275, nmi = 0.7985, ari = 0.7429 ;  loss= 0.1\n",
      "nn tf.Tensor([1.6863277 1.4002379 2.3845596 ... 2.674211  3.8346705 1.5587438], shape=(10000,), dtype=float32)\n",
      "lossmse 0.026907444 :kld 0.160667\n",
      "nn tf.Tensor([1.811448  1.0971848 2.3790848 ... 3.6324418 4.150629  1.4658341], shape=(10000,), dtype=float32)\n",
      "lossmse 0.026120843 :kld 0.1394493\n",
      "nn tf.Tensor([2.0730798 1.2125636 2.393507  ... 2.8122108 4.0982976 1.5331318], shape=(10000,), dtype=float32)\n",
      "lossmse 0.025175994 :kld 0.13519335\n",
      "nn tf.Tensor([1.817414  1.2210943 2.3899753 ... 2.8623602 4.755807  1.4621694], shape=(10000,), dtype=float32)\n",
      "lossmse 0.02791103 :kld 0.17682281\n",
      "Epoch 7  Loss 0.0392\n",
      "Iter  acc = 0.8316, nmi = 0.8050, ari = 0.7513 ;  loss= 0.1\n",
      "nn tf.Tensor([1.7331128 1.3720806 2.3838937 ... 2.7542462 4.364347  1.5066955], shape=(10000,), dtype=float32)\n",
      "lossmse 0.028197402 :kld 0.15881354\n",
      "nn tf.Tensor([1.8801701 1.0303253 2.379361  ... 3.2201138 3.791831  1.4359303], shape=(10000,), dtype=float32)\n",
      "lossmse 0.026446681 :kld 0.1379376\n",
      "nn tf.Tensor([2.0348535 1.1430873 2.4004679 ... 3.0418153 3.7849169 1.6484187], shape=(10000,), dtype=float32)\n",
      "lossmse 0.025647305 :kld 0.12909831\n",
      "nn tf.Tensor([1.8459795 1.1531558 2.3905053 ... 3.3852034 4.463414  1.5706489], shape=(10000,), dtype=float32)\n",
      "lossmse 0.028356459 :kld 0.16137177\n",
      "Epoch 8  Loss 0.0363\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    ls=cluster_train()\n",
    "    print('Epoch {}  Loss {:.4f}'.format(epoch + 1,ls.numpy()))\n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train():\n",
    "    #start = time.time()\n",
    "    loss = 0\n",
    "    for (batch, (x, labels)) in enumerate(dataset.take(40)):\n",
    "        if batch % 10 == 0:\n",
    "             print('.',end='')\n",
    "        #    print('Batch {} loss{:.3f}'.format(batch,loss) )\n",
    "    \n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_encoder.trainable = True\n",
    "            g_decoder.trainable = True\n",
    "            z = g_encoder(x, training=True)\n",
    "            x_hat = g_decoder(z, training=True)\n",
    "            loss = loss_mse(x, x_hat)\n",
    "                       \n",
    "        variables = g_encoder.variables + g_decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Generator Training\n",
    "    #https://github.com/tensorflow/tensorflow/issues/23407\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                # err_g_bce\n",
    "                g_encoder.trainable = True\n",
    "                g_decoder.trainable = True\n",
    "                z = g_encoder(x, training=True)\n",
    "                x_hat = g_decoder(z, training=True)\n",
    "                bce_loss = tf.losses.sigmoid_cross_entropy(\n",
    "                    multi_class_labels=tf.ones_like(disc_x_hat),\n",
    "                    logits=disc_x_hat,  # G wants to generate reals so ones_like\n",
    "                )\n",
    "\n",
    "                l1_loss = tf.losses.absolute_difference(x, x_hat)\n",
    "                # err_g_enc\n",
    "                z_hat = encoder(x_hat, training=True)\n",
    "                l2_loss = tf.losses.mean_squared_error(z, z_hat)\n",
    "\n",
    "                gen_loss = 1* bce_loss + 50 * l1_loss + 1 * l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cae = CAE(input_shape, filters)\n",
    "hidden = self.cae.get_layer(name='embedding').output\n",
    "encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "# Define DCEC model\n",
    "#    clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(hidden)\n",
    "model = Model(inputs=cae.input,outputs=[cae.output])\n",
    "\n",
    "self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "#from keras.callbacks import CSVLogger\n",
    "#        csv_logger = CSVLogger(args.save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "t0 = time()\n",
    "#self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.cae = CAE(input_shape, filters)\n",
    "        hidden = self.cae.get_layer(name='embedding').output\n",
    "        self.encoder = Model(inputs=self.cae.input, outputs=hidden)\n",
    "\n",
    "        # Define DCEC model\n",
    "        print('nn',self.n_clusters)\n",
    "        clustering_layer = ClusteringLayer(n_clusters=self.n_clusters, name='clustering')(hidden)\n",
    "        self.model = Model(inputs=self.cae.input,\n",
    "                           outputs=[clustering_layer, self.cae.output])\n",
    "\n",
    "    def pretrain(self, x, batch_size=256, epochs=50, optimizer='adam'):\n",
    "        print('...Pretraining...')\n",
    "        self.cae.compile(optimizer=optimizer, loss='mse')\n",
    "        from tensorflow.keras.callbacks import CSVLogger\n",
    "        csv_logger = CSVLogger(self.save_dir + '/pretrain_log.csv')\n",
    "\n",
    "        # begin training\n",
    "        t0 = time()\n",
    "        self.cae.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=[csv_logger])\n",
    "        print('Pretraining time: ', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    optimizer= tf.train.AdamOptimizer()\n",
    "    for epoch in range(3):\n",
    "        for (batch, (images, labels)) in enumerate(dataset):\n",
    "            train_step(images, labels)\n",
    "        print ('Epoch {} finished'.format(epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
